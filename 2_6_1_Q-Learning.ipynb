{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/EscUpmPolit_p.gif \"UPM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Notes for Learning Intelligent Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Department of Telematic Engineering Systems, Universidad Politécnica de Madrid, © 2018 Carlos A. Iglesias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Introduction to Machine Learning V](2_6_0_Intro_RL.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Introduction](#Introduction)\n",
    "* [Getting started with OpenAI Gym](#Getting-started-with-OpenAI-Gym)\n",
    "* [The Frozen Lake scenario](#The-Frozen-Lake-scenario)\n",
    "* [Q-Learning with the Frozen Lake scenario](#Q-Learning-with-the-Frozen-Lake-scenario)\n",
    "* [Exercises](#Exercises)\n",
    "* [Optional exercises](#Optional-exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The purpose of this practice is to understand better Reinforcement Learning (RL) and, in particular, Q-Learning.\n",
    "\n",
    "We are going to use [OpenAI Gym](https://gym.openai.com/). OpenAI is a toolkit for developing and comparing RL algorithms.Take a loot at ther [website](https://gym.openai.com/).\n",
    "\n",
    "It implements [algorithm imitation](http://gym.openai.com/envs/#algorithmic), [classic control problems](http://gym.openai.com/envs/#classic_control), [Atari games](http://gym.openai.com/envs/#atari), [Box2D continuous control](http://gym.openai.com/envs/#box2d), [robotics with MuJoCo, Multi-Joint dynamics with Contact](http://gym.openai.com/envs/#mujoco),  and [simple text based environments](http://gym.openai.com/envs/#toy_text).\n",
    "\n",
    "This notebook is based on * [Diving deeper into Reinforcement Learning with Q-Learning](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe).\n",
    "\n",
    "First of all, install the OpenAI Gym  library:\n",
    "\n",
    "```console\n",
    "foo@bar:~$ pip install gym\n",
    "```\n",
    "\n",
    "\n",
    "If you get the error message 'NotImplementedError: abstract', [execute](https://github.com/openai/gym/issues/775) \n",
    "```console\n",
    "foo@bar:~$ pip install pyglet==1.2.4\n",
    "```\n",
    "\n",
    "If you want to try the Atari environment, it is better that you opt for the full installation from the source. Follow the instructions at [https://github.com/openai/gym#id15](OpenGym).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with OpenAI Gym\n",
    "\n",
    "First of all, read the [introduction](http://gym.openai.com/docs/#getting-started-with-gym) of OpenAI Gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "OpenGym provides a number of problems called *environments*. \n",
    "\n",
    "Try the 'CartPole-v0' (or 'MountainCar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "#env = gym.make('MountainCar-v0')\n",
    "#env = gym.make('Taxi-v2')\n",
    "\n",
    "#env = gym.make('Jamesbond-ram-v0')\n",
    "\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will launch an external window with the game. If you cannot close that window, just execute in a code cell:\n",
    "\n",
    "```python\n",
    "env.close()\n",
    "```\n",
    "\n",
    "The full list of available environments can be found printing the environment registry as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([EnvSpec(Copy-v0), EnvSpec(RepeatCopy-v0), EnvSpec(ReversedAddition-v0), EnvSpec(ReversedAddition3-v0), EnvSpec(DuplicatedInput-v0), EnvSpec(Reverse-v0), EnvSpec(CartPole-v0), EnvSpec(CartPole-v1), EnvSpec(MountainCar-v0), EnvSpec(MountainCarContinuous-v0), EnvSpec(Pendulum-v0), EnvSpec(Acrobot-v1), EnvSpec(LunarLander-v2), EnvSpec(LunarLanderContinuous-v2), EnvSpec(BipedalWalker-v2), EnvSpec(BipedalWalkerHardcore-v2), EnvSpec(CarRacing-v0), EnvSpec(Blackjack-v0), EnvSpec(KellyCoinflip-v0), EnvSpec(KellyCoinflipGeneralized-v0), EnvSpec(FrozenLake-v0), EnvSpec(FrozenLake8x8-v0), EnvSpec(CliffWalking-v0), EnvSpec(NChain-v0), EnvSpec(Roulette-v0), EnvSpec(Taxi-v2), EnvSpec(GuessingGame-v0), EnvSpec(HotterColder-v0), EnvSpec(Reacher-v2), EnvSpec(Pusher-v2), EnvSpec(Thrower-v2), EnvSpec(Striker-v2), EnvSpec(InvertedPendulum-v2), EnvSpec(InvertedDoublePendulum-v2), EnvSpec(HalfCheetah-v2), EnvSpec(Hopper-v2), EnvSpec(Swimmer-v2), EnvSpec(Walker2d-v2), EnvSpec(Ant-v2), EnvSpec(Humanoid-v2), EnvSpec(HumanoidStandup-v2), EnvSpec(FetchSlide-v1), EnvSpec(FetchPickAndPlace-v1), EnvSpec(FetchReach-v1), EnvSpec(FetchPush-v1), EnvSpec(HandReach-v0), EnvSpec(HandManipulateBlockRotateZ-v0), EnvSpec(HandManipulateBlockRotateParallel-v0), EnvSpec(HandManipulateBlockRotateXYZ-v0), EnvSpec(HandManipulateBlockFull-v0), EnvSpec(HandManipulateBlock-v0), EnvSpec(HandManipulateEggRotate-v0), EnvSpec(HandManipulateEggFull-v0), EnvSpec(HandManipulateEgg-v0), EnvSpec(HandManipulatePenRotate-v0), EnvSpec(HandManipulatePenFull-v0), EnvSpec(HandManipulatePen-v0), EnvSpec(FetchSlideDense-v1), EnvSpec(FetchPickAndPlaceDense-v1), EnvSpec(FetchReachDense-v1), EnvSpec(FetchPushDense-v1), EnvSpec(HandReachDense-v0), EnvSpec(HandManipulateBlockRotateZDense-v0), EnvSpec(HandManipulateBlockRotateParallelDense-v0), EnvSpec(HandManipulateBlockRotateXYZDense-v0), EnvSpec(HandManipulateBlockFullDense-v0), EnvSpec(HandManipulateBlockDense-v0), EnvSpec(HandManipulateEggRotateDense-v0), EnvSpec(HandManipulateEggFullDense-v0), EnvSpec(HandManipulateEggDense-v0), EnvSpec(HandManipulatePenRotateDense-v0), EnvSpec(HandManipulatePenFullDense-v0), EnvSpec(HandManipulatePenDense-v0), EnvSpec(AirRaid-v0), EnvSpec(AirRaid-v4), EnvSpec(AirRaidDeterministic-v0), EnvSpec(AirRaidDeterministic-v4), EnvSpec(AirRaidNoFrameskip-v0), EnvSpec(AirRaidNoFrameskip-v4), EnvSpec(AirRaid-ram-v0), EnvSpec(AirRaid-ram-v4), EnvSpec(AirRaid-ramDeterministic-v0), EnvSpec(AirRaid-ramDeterministic-v4), EnvSpec(AirRaid-ramNoFrameskip-v0), EnvSpec(AirRaid-ramNoFrameskip-v4), EnvSpec(Alien-v0), EnvSpec(Alien-v4), EnvSpec(AlienDeterministic-v0), EnvSpec(AlienDeterministic-v4), EnvSpec(AlienNoFrameskip-v0), EnvSpec(AlienNoFrameskip-v4), EnvSpec(Alien-ram-v0), EnvSpec(Alien-ram-v4), EnvSpec(Alien-ramDeterministic-v0), EnvSpec(Alien-ramDeterministic-v4), EnvSpec(Alien-ramNoFrameskip-v0), EnvSpec(Alien-ramNoFrameskip-v4), EnvSpec(Amidar-v0), EnvSpec(Amidar-v4), EnvSpec(AmidarDeterministic-v0), EnvSpec(AmidarDeterministic-v4), EnvSpec(AmidarNoFrameskip-v0), EnvSpec(AmidarNoFrameskip-v4), EnvSpec(Amidar-ram-v0), EnvSpec(Amidar-ram-v4), EnvSpec(Amidar-ramDeterministic-v0), EnvSpec(Amidar-ramDeterministic-v4), EnvSpec(Amidar-ramNoFrameskip-v0), EnvSpec(Amidar-ramNoFrameskip-v4), EnvSpec(Assault-v0), EnvSpec(Assault-v4), EnvSpec(AssaultDeterministic-v0), EnvSpec(AssaultDeterministic-v4), EnvSpec(AssaultNoFrameskip-v0), EnvSpec(AssaultNoFrameskip-v4), EnvSpec(Assault-ram-v0), EnvSpec(Assault-ram-v4), EnvSpec(Assault-ramDeterministic-v0), EnvSpec(Assault-ramDeterministic-v4), EnvSpec(Assault-ramNoFrameskip-v0), EnvSpec(Assault-ramNoFrameskip-v4), EnvSpec(Asterix-v0), EnvSpec(Asterix-v4), EnvSpec(AsterixDeterministic-v0), EnvSpec(AsterixDeterministic-v4), EnvSpec(AsterixNoFrameskip-v0), EnvSpec(AsterixNoFrameskip-v4), EnvSpec(Asterix-ram-v0), EnvSpec(Asterix-ram-v4), EnvSpec(Asterix-ramDeterministic-v0), EnvSpec(Asterix-ramDeterministic-v4), EnvSpec(Asterix-ramNoFrameskip-v0), EnvSpec(Asterix-ramNoFrameskip-v4), EnvSpec(Asteroids-v0), EnvSpec(Asteroids-v4), EnvSpec(AsteroidsDeterministic-v0), EnvSpec(AsteroidsDeterministic-v4), EnvSpec(AsteroidsNoFrameskip-v0), EnvSpec(AsteroidsNoFrameskip-v4), EnvSpec(Asteroids-ram-v0), EnvSpec(Asteroids-ram-v4), EnvSpec(Asteroids-ramDeterministic-v0), EnvSpec(Asteroids-ramDeterministic-v4), EnvSpec(Asteroids-ramNoFrameskip-v0), EnvSpec(Asteroids-ramNoFrameskip-v4), EnvSpec(Atlantis-v0), EnvSpec(Atlantis-v4), EnvSpec(AtlantisDeterministic-v0), EnvSpec(AtlantisDeterministic-v4), EnvSpec(AtlantisNoFrameskip-v0), EnvSpec(AtlantisNoFrameskip-v4), EnvSpec(Atlantis-ram-v0), EnvSpec(Atlantis-ram-v4), EnvSpec(Atlantis-ramDeterministic-v0), EnvSpec(Atlantis-ramDeterministic-v4), EnvSpec(Atlantis-ramNoFrameskip-v0), EnvSpec(Atlantis-ramNoFrameskip-v4), EnvSpec(BankHeist-v0), EnvSpec(BankHeist-v4), EnvSpec(BankHeistDeterministic-v0), EnvSpec(BankHeistDeterministic-v4), EnvSpec(BankHeistNoFrameskip-v0), EnvSpec(BankHeistNoFrameskip-v4), EnvSpec(BankHeist-ram-v0), EnvSpec(BankHeist-ram-v4), EnvSpec(BankHeist-ramDeterministic-v0), EnvSpec(BankHeist-ramDeterministic-v4), EnvSpec(BankHeist-ramNoFrameskip-v0), EnvSpec(BankHeist-ramNoFrameskip-v4), EnvSpec(BattleZone-v0), EnvSpec(BattleZone-v4), EnvSpec(BattleZoneDeterministic-v0), EnvSpec(BattleZoneDeterministic-v4), EnvSpec(BattleZoneNoFrameskip-v0), EnvSpec(BattleZoneNoFrameskip-v4), EnvSpec(BattleZone-ram-v0), EnvSpec(BattleZone-ram-v4), EnvSpec(BattleZone-ramDeterministic-v0), EnvSpec(BattleZone-ramDeterministic-v4), EnvSpec(BattleZone-ramNoFrameskip-v0), EnvSpec(BattleZone-ramNoFrameskip-v4), EnvSpec(BeamRider-v0), EnvSpec(BeamRider-v4), EnvSpec(BeamRiderDeterministic-v0), EnvSpec(BeamRiderDeterministic-v4), EnvSpec(BeamRiderNoFrameskip-v0), EnvSpec(BeamRiderNoFrameskip-v4), EnvSpec(BeamRider-ram-v0), EnvSpec(BeamRider-ram-v4), EnvSpec(BeamRider-ramDeterministic-v0), EnvSpec(BeamRider-ramDeterministic-v4), EnvSpec(BeamRider-ramNoFrameskip-v0), EnvSpec(BeamRider-ramNoFrameskip-v4), EnvSpec(Berzerk-v0), EnvSpec(Berzerk-v4), EnvSpec(BerzerkDeterministic-v0), EnvSpec(BerzerkDeterministic-v4), EnvSpec(BerzerkNoFrameskip-v0), EnvSpec(BerzerkNoFrameskip-v4), EnvSpec(Berzerk-ram-v0), EnvSpec(Berzerk-ram-v4), EnvSpec(Berzerk-ramDeterministic-v0), EnvSpec(Berzerk-ramDeterministic-v4), EnvSpec(Berzerk-ramNoFrameskip-v0), EnvSpec(Berzerk-ramNoFrameskip-v4), EnvSpec(Bowling-v0), EnvSpec(Bowling-v4), EnvSpec(BowlingDeterministic-v0), EnvSpec(BowlingDeterministic-v4), EnvSpec(BowlingNoFrameskip-v0), EnvSpec(BowlingNoFrameskip-v4), EnvSpec(Bowling-ram-v0), EnvSpec(Bowling-ram-v4), EnvSpec(Bowling-ramDeterministic-v0), EnvSpec(Bowling-ramDeterministic-v4), EnvSpec(Bowling-ramNoFrameskip-v0), EnvSpec(Bowling-ramNoFrameskip-v4), EnvSpec(Boxing-v0), EnvSpec(Boxing-v4), EnvSpec(BoxingDeterministic-v0), EnvSpec(BoxingDeterministic-v4), EnvSpec(BoxingNoFrameskip-v0), EnvSpec(BoxingNoFrameskip-v4), EnvSpec(Boxing-ram-v0), EnvSpec(Boxing-ram-v4), EnvSpec(Boxing-ramDeterministic-v0), EnvSpec(Boxing-ramDeterministic-v4), EnvSpec(Boxing-ramNoFrameskip-v0), EnvSpec(Boxing-ramNoFrameskip-v4), EnvSpec(Breakout-v0), EnvSpec(Breakout-v4), EnvSpec(BreakoutDeterministic-v0), EnvSpec(BreakoutDeterministic-v4), EnvSpec(BreakoutNoFrameskip-v0), EnvSpec(BreakoutNoFrameskip-v4), EnvSpec(Breakout-ram-v0), EnvSpec(Breakout-ram-v4), EnvSpec(Breakout-ramDeterministic-v0), EnvSpec(Breakout-ramDeterministic-v4), EnvSpec(Breakout-ramNoFrameskip-v0), EnvSpec(Breakout-ramNoFrameskip-v4), EnvSpec(Carnival-v0), EnvSpec(Carnival-v4), EnvSpec(CarnivalDeterministic-v0), EnvSpec(CarnivalDeterministic-v4), EnvSpec(CarnivalNoFrameskip-v0), EnvSpec(CarnivalNoFrameskip-v4), EnvSpec(Carnival-ram-v0), EnvSpec(Carnival-ram-v4), EnvSpec(Carnival-ramDeterministic-v0), EnvSpec(Carnival-ramDeterministic-v4), EnvSpec(Carnival-ramNoFrameskip-v0), EnvSpec(Carnival-ramNoFrameskip-v4), EnvSpec(Centipede-v0), EnvSpec(Centipede-v4), EnvSpec(CentipedeDeterministic-v0), EnvSpec(CentipedeDeterministic-v4), EnvSpec(CentipedeNoFrameskip-v0), EnvSpec(CentipedeNoFrameskip-v4), EnvSpec(Centipede-ram-v0), EnvSpec(Centipede-ram-v4), EnvSpec(Centipede-ramDeterministic-v0), EnvSpec(Centipede-ramDeterministic-v4), EnvSpec(Centipede-ramNoFrameskip-v0), EnvSpec(Centipede-ramNoFrameskip-v4), EnvSpec(ChopperCommand-v0), EnvSpec(ChopperCommand-v4), EnvSpec(ChopperCommandDeterministic-v0), EnvSpec(ChopperCommandDeterministic-v4), EnvSpec(ChopperCommandNoFrameskip-v0), EnvSpec(ChopperCommandNoFrameskip-v4), EnvSpec(ChopperCommand-ram-v0), EnvSpec(ChopperCommand-ram-v4), EnvSpec(ChopperCommand-ramDeterministic-v0), EnvSpec(ChopperCommand-ramDeterministic-v4), EnvSpec(ChopperCommand-ramNoFrameskip-v0), EnvSpec(ChopperCommand-ramNoFrameskip-v4), EnvSpec(CrazyClimber-v0), EnvSpec(CrazyClimber-v4), EnvSpec(CrazyClimberDeterministic-v0), EnvSpec(CrazyClimberDeterministic-v4), EnvSpec(CrazyClimberNoFrameskip-v0), EnvSpec(CrazyClimberNoFrameskip-v4), EnvSpec(CrazyClimber-ram-v0), EnvSpec(CrazyClimber-ram-v4), EnvSpec(CrazyClimber-ramDeterministic-v0), EnvSpec(CrazyClimber-ramDeterministic-v4), EnvSpec(CrazyClimber-ramNoFrameskip-v0), EnvSpec(CrazyClimber-ramNoFrameskip-v4), EnvSpec(DemonAttack-v0), EnvSpec(DemonAttack-v4), EnvSpec(DemonAttackDeterministic-v0), EnvSpec(DemonAttackDeterministic-v4), EnvSpec(DemonAttackNoFrameskip-v0), EnvSpec(DemonAttackNoFrameskip-v4), EnvSpec(DemonAttack-ram-v0), EnvSpec(DemonAttack-ram-v4), EnvSpec(DemonAttack-ramDeterministic-v0), EnvSpec(DemonAttack-ramDeterministic-v4), EnvSpec(DemonAttack-ramNoFrameskip-v0), EnvSpec(DemonAttack-ramNoFrameskip-v4), EnvSpec(DoubleDunk-v0), EnvSpec(DoubleDunk-v4), EnvSpec(DoubleDunkDeterministic-v0), EnvSpec(DoubleDunkDeterministic-v4), EnvSpec(DoubleDunkNoFrameskip-v0), EnvSpec(DoubleDunkNoFrameskip-v4), EnvSpec(DoubleDunk-ram-v0), EnvSpec(DoubleDunk-ram-v4), EnvSpec(DoubleDunk-ramDeterministic-v0), EnvSpec(DoubleDunk-ramDeterministic-v4), EnvSpec(DoubleDunk-ramNoFrameskip-v0), EnvSpec(DoubleDunk-ramNoFrameskip-v4), EnvSpec(ElevatorAction-v0), EnvSpec(ElevatorAction-v4), EnvSpec(ElevatorActionDeterministic-v0), EnvSpec(ElevatorActionDeterministic-v4), EnvSpec(ElevatorActionNoFrameskip-v0), EnvSpec(ElevatorActionNoFrameskip-v4), EnvSpec(ElevatorAction-ram-v0), EnvSpec(ElevatorAction-ram-v4), EnvSpec(ElevatorAction-ramDeterministic-v0), EnvSpec(ElevatorAction-ramDeterministic-v4), EnvSpec(ElevatorAction-ramNoFrameskip-v0), EnvSpec(ElevatorAction-ramNoFrameskip-v4), EnvSpec(Enduro-v0), EnvSpec(Enduro-v4), EnvSpec(EnduroDeterministic-v0), EnvSpec(EnduroDeterministic-v4), EnvSpec(EnduroNoFrameskip-v0), EnvSpec(EnduroNoFrameskip-v4), EnvSpec(Enduro-ram-v0), EnvSpec(Enduro-ram-v4), EnvSpec(Enduro-ramDeterministic-v0), EnvSpec(Enduro-ramDeterministic-v4), EnvSpec(Enduro-ramNoFrameskip-v0), EnvSpec(Enduro-ramNoFrameskip-v4), EnvSpec(FishingDerby-v0), EnvSpec(FishingDerby-v4), EnvSpec(FishingDerbyDeterministic-v0), EnvSpec(FishingDerbyDeterministic-v4), EnvSpec(FishingDerbyNoFrameskip-v0), EnvSpec(FishingDerbyNoFrameskip-v4), EnvSpec(FishingDerby-ram-v0), EnvSpec(FishingDerby-ram-v4), EnvSpec(FishingDerby-ramDeterministic-v0), EnvSpec(FishingDerby-ramDeterministic-v4), EnvSpec(FishingDerby-ramNoFrameskip-v0), EnvSpec(FishingDerby-ramNoFrameskip-v4), EnvSpec(Freeway-v0), EnvSpec(Freeway-v4), EnvSpec(FreewayDeterministic-v0), EnvSpec(FreewayDeterministic-v4), EnvSpec(FreewayNoFrameskip-v0), EnvSpec(FreewayNoFrameskip-v4), EnvSpec(Freeway-ram-v0), EnvSpec(Freeway-ram-v4), EnvSpec(Freeway-ramDeterministic-v0), EnvSpec(Freeway-ramDeterministic-v4), EnvSpec(Freeway-ramNoFrameskip-v0), EnvSpec(Freeway-ramNoFrameskip-v4), EnvSpec(Frostbite-v0), EnvSpec(Frostbite-v4), EnvSpec(FrostbiteDeterministic-v0), EnvSpec(FrostbiteDeterministic-v4), EnvSpec(FrostbiteNoFrameskip-v0), EnvSpec(FrostbiteNoFrameskip-v4), EnvSpec(Frostbite-ram-v0), EnvSpec(Frostbite-ram-v4), EnvSpec(Frostbite-ramDeterministic-v0), EnvSpec(Frostbite-ramDeterministic-v4), EnvSpec(Frostbite-ramNoFrameskip-v0), EnvSpec(Frostbite-ramNoFrameskip-v4), EnvSpec(Gopher-v0), EnvSpec(Gopher-v4), EnvSpec(GopherDeterministic-v0), EnvSpec(GopherDeterministic-v4), EnvSpec(GopherNoFrameskip-v0), EnvSpec(GopherNoFrameskip-v4), EnvSpec(Gopher-ram-v0), EnvSpec(Gopher-ram-v4), EnvSpec(Gopher-ramDeterministic-v0), EnvSpec(Gopher-ramDeterministic-v4), EnvSpec(Gopher-ramNoFrameskip-v0), EnvSpec(Gopher-ramNoFrameskip-v4), EnvSpec(Gravitar-v0), EnvSpec(Gravitar-v4), EnvSpec(GravitarDeterministic-v0), EnvSpec(GravitarDeterministic-v4), EnvSpec(GravitarNoFrameskip-v0), EnvSpec(GravitarNoFrameskip-v4), EnvSpec(Gravitar-ram-v0), EnvSpec(Gravitar-ram-v4), EnvSpec(Gravitar-ramDeterministic-v0), EnvSpec(Gravitar-ramDeterministic-v4), EnvSpec(Gravitar-ramNoFrameskip-v0), EnvSpec(Gravitar-ramNoFrameskip-v4), EnvSpec(Hero-v0), EnvSpec(Hero-v4), EnvSpec(HeroDeterministic-v0), EnvSpec(HeroDeterministic-v4), EnvSpec(HeroNoFrameskip-v0), EnvSpec(HeroNoFrameskip-v4), EnvSpec(Hero-ram-v0), EnvSpec(Hero-ram-v4), EnvSpec(Hero-ramDeterministic-v0), EnvSpec(Hero-ramDeterministic-v4), EnvSpec(Hero-ramNoFrameskip-v0), EnvSpec(Hero-ramNoFrameskip-v4), EnvSpec(IceHockey-v0), EnvSpec(IceHockey-v4), EnvSpec(IceHockeyDeterministic-v0), EnvSpec(IceHockeyDeterministic-v4), EnvSpec(IceHockeyNoFrameskip-v0), EnvSpec(IceHockeyNoFrameskip-v4), EnvSpec(IceHockey-ram-v0), EnvSpec(IceHockey-ram-v4), EnvSpec(IceHockey-ramDeterministic-v0), EnvSpec(IceHockey-ramDeterministic-v4), EnvSpec(IceHockey-ramNoFrameskip-v0), EnvSpec(IceHockey-ramNoFrameskip-v4), EnvSpec(Jamesbond-v0), EnvSpec(Jamesbond-v4), EnvSpec(JamesbondDeterministic-v0), EnvSpec(JamesbondDeterministic-v4), EnvSpec(JamesbondNoFrameskip-v0), EnvSpec(JamesbondNoFrameskip-v4), EnvSpec(Jamesbond-ram-v0), EnvSpec(Jamesbond-ram-v4), EnvSpec(Jamesbond-ramDeterministic-v0), EnvSpec(Jamesbond-ramDeterministic-v4), EnvSpec(Jamesbond-ramNoFrameskip-v0), EnvSpec(Jamesbond-ramNoFrameskip-v4), EnvSpec(JourneyEscape-v0), EnvSpec(JourneyEscape-v4), EnvSpec(JourneyEscapeDeterministic-v0), EnvSpec(JourneyEscapeDeterministic-v4), EnvSpec(JourneyEscapeNoFrameskip-v0), EnvSpec(JourneyEscapeNoFrameskip-v4), EnvSpec(JourneyEscape-ram-v0), EnvSpec(JourneyEscape-ram-v4), EnvSpec(JourneyEscape-ramDeterministic-v0), EnvSpec(JourneyEscape-ramDeterministic-v4), EnvSpec(JourneyEscape-ramNoFrameskip-v0), EnvSpec(JourneyEscape-ramNoFrameskip-v4), EnvSpec(Kangaroo-v0), EnvSpec(Kangaroo-v4), EnvSpec(KangarooDeterministic-v0), EnvSpec(KangarooDeterministic-v4), EnvSpec(KangarooNoFrameskip-v0), EnvSpec(KangarooNoFrameskip-v4), EnvSpec(Kangaroo-ram-v0), EnvSpec(Kangaroo-ram-v4), EnvSpec(Kangaroo-ramDeterministic-v0), EnvSpec(Kangaroo-ramDeterministic-v4), EnvSpec(Kangaroo-ramNoFrameskip-v0), EnvSpec(Kangaroo-ramNoFrameskip-v4), EnvSpec(Krull-v0), EnvSpec(Krull-v4), EnvSpec(KrullDeterministic-v0), EnvSpec(KrullDeterministic-v4), EnvSpec(KrullNoFrameskip-v0), EnvSpec(KrullNoFrameskip-v4), EnvSpec(Krull-ram-v0), EnvSpec(Krull-ram-v4), EnvSpec(Krull-ramDeterministic-v0), EnvSpec(Krull-ramDeterministic-v4), EnvSpec(Krull-ramNoFrameskip-v0), EnvSpec(Krull-ramNoFrameskip-v4), EnvSpec(KungFuMaster-v0), EnvSpec(KungFuMaster-v4), EnvSpec(KungFuMasterDeterministic-v0), EnvSpec(KungFuMasterDeterministic-v4), EnvSpec(KungFuMasterNoFrameskip-v0), EnvSpec(KungFuMasterNoFrameskip-v4), EnvSpec(KungFuMaster-ram-v0), EnvSpec(KungFuMaster-ram-v4), EnvSpec(KungFuMaster-ramDeterministic-v0), EnvSpec(KungFuMaster-ramDeterministic-v4), EnvSpec(KungFuMaster-ramNoFrameskip-v0), EnvSpec(KungFuMaster-ramNoFrameskip-v4), EnvSpec(MontezumaRevenge-v0), EnvSpec(MontezumaRevenge-v4), EnvSpec(MontezumaRevengeDeterministic-v0), EnvSpec(MontezumaRevengeDeterministic-v4), EnvSpec(MontezumaRevengeNoFrameskip-v0), EnvSpec(MontezumaRevengeNoFrameskip-v4), EnvSpec(MontezumaRevenge-ram-v0), EnvSpec(MontezumaRevenge-ram-v4), EnvSpec(MontezumaRevenge-ramDeterministic-v0), EnvSpec(MontezumaRevenge-ramDeterministic-v4), EnvSpec(MontezumaRevenge-ramNoFrameskip-v0), EnvSpec(MontezumaRevenge-ramNoFrameskip-v4), EnvSpec(MsPacman-v0), EnvSpec(MsPacman-v4), EnvSpec(MsPacmanDeterministic-v0), EnvSpec(MsPacmanDeterministic-v4), EnvSpec(MsPacmanNoFrameskip-v0), EnvSpec(MsPacmanNoFrameskip-v4), EnvSpec(MsPacman-ram-v0), EnvSpec(MsPacman-ram-v4), EnvSpec(MsPacman-ramDeterministic-v0), EnvSpec(MsPacman-ramDeterministic-v4), EnvSpec(MsPacman-ramNoFrameskip-v0), EnvSpec(MsPacman-ramNoFrameskip-v4), EnvSpec(NameThisGame-v0), EnvSpec(NameThisGame-v4), EnvSpec(NameThisGameDeterministic-v0), EnvSpec(NameThisGameDeterministic-v4), EnvSpec(NameThisGameNoFrameskip-v0), EnvSpec(NameThisGameNoFrameskip-v4), EnvSpec(NameThisGame-ram-v0), EnvSpec(NameThisGame-ram-v4), EnvSpec(NameThisGame-ramDeterministic-v0), EnvSpec(NameThisGame-ramDeterministic-v4), EnvSpec(NameThisGame-ramNoFrameskip-v0), EnvSpec(NameThisGame-ramNoFrameskip-v4), EnvSpec(Phoenix-v0), EnvSpec(Phoenix-v4), EnvSpec(PhoenixDeterministic-v0), EnvSpec(PhoenixDeterministic-v4), EnvSpec(PhoenixNoFrameskip-v0), EnvSpec(PhoenixNoFrameskip-v4), EnvSpec(Phoenix-ram-v0), EnvSpec(Phoenix-ram-v4), EnvSpec(Phoenix-ramDeterministic-v0), EnvSpec(Phoenix-ramDeterministic-v4), EnvSpec(Phoenix-ramNoFrameskip-v0), EnvSpec(Phoenix-ramNoFrameskip-v4), EnvSpec(Pitfall-v0), EnvSpec(Pitfall-v4), EnvSpec(PitfallDeterministic-v0), EnvSpec(PitfallDeterministic-v4), EnvSpec(PitfallNoFrameskip-v0), EnvSpec(PitfallNoFrameskip-v4), EnvSpec(Pitfall-ram-v0), EnvSpec(Pitfall-ram-v4), EnvSpec(Pitfall-ramDeterministic-v0), EnvSpec(Pitfall-ramDeterministic-v4), EnvSpec(Pitfall-ramNoFrameskip-v0), EnvSpec(Pitfall-ramNoFrameskip-v4), EnvSpec(Pong-v0), EnvSpec(Pong-v4), EnvSpec(PongDeterministic-v0), EnvSpec(PongDeterministic-v4), EnvSpec(PongNoFrameskip-v0), EnvSpec(PongNoFrameskip-v4), EnvSpec(Pong-ram-v0), EnvSpec(Pong-ram-v4), EnvSpec(Pong-ramDeterministic-v0), EnvSpec(Pong-ramDeterministic-v4), EnvSpec(Pong-ramNoFrameskip-v0), EnvSpec(Pong-ramNoFrameskip-v4), EnvSpec(Pooyan-v0), EnvSpec(Pooyan-v4), EnvSpec(PooyanDeterministic-v0), EnvSpec(PooyanDeterministic-v4), EnvSpec(PooyanNoFrameskip-v0), EnvSpec(PooyanNoFrameskip-v4), EnvSpec(Pooyan-ram-v0), EnvSpec(Pooyan-ram-v4), EnvSpec(Pooyan-ramDeterministic-v0), EnvSpec(Pooyan-ramDeterministic-v4), EnvSpec(Pooyan-ramNoFrameskip-v0), EnvSpec(Pooyan-ramNoFrameskip-v4), EnvSpec(PrivateEye-v0), EnvSpec(PrivateEye-v4), EnvSpec(PrivateEyeDeterministic-v0), EnvSpec(PrivateEyeDeterministic-v4), EnvSpec(PrivateEyeNoFrameskip-v0), EnvSpec(PrivateEyeNoFrameskip-v4), EnvSpec(PrivateEye-ram-v0), EnvSpec(PrivateEye-ram-v4), EnvSpec(PrivateEye-ramDeterministic-v0), EnvSpec(PrivateEye-ramDeterministic-v4), EnvSpec(PrivateEye-ramNoFrameskip-v0), EnvSpec(PrivateEye-ramNoFrameskip-v4), EnvSpec(Qbert-v0), EnvSpec(Qbert-v4), EnvSpec(QbertDeterministic-v0), EnvSpec(QbertDeterministic-v4), EnvSpec(QbertNoFrameskip-v0), EnvSpec(QbertNoFrameskip-v4), EnvSpec(Qbert-ram-v0), EnvSpec(Qbert-ram-v4), EnvSpec(Qbert-ramDeterministic-v0), EnvSpec(Qbert-ramDeterministic-v4), EnvSpec(Qbert-ramNoFrameskip-v0), EnvSpec(Qbert-ramNoFrameskip-v4), EnvSpec(Riverraid-v0), EnvSpec(Riverraid-v4), EnvSpec(RiverraidDeterministic-v0), EnvSpec(RiverraidDeterministic-v4), EnvSpec(RiverraidNoFrameskip-v0), EnvSpec(RiverraidNoFrameskip-v4), EnvSpec(Riverraid-ram-v0), EnvSpec(Riverraid-ram-v4), EnvSpec(Riverraid-ramDeterministic-v0), EnvSpec(Riverraid-ramDeterministic-v4), EnvSpec(Riverraid-ramNoFrameskip-v0), EnvSpec(Riverraid-ramNoFrameskip-v4), EnvSpec(RoadRunner-v0), EnvSpec(RoadRunner-v4), EnvSpec(RoadRunnerDeterministic-v0), EnvSpec(RoadRunnerDeterministic-v4), EnvSpec(RoadRunnerNoFrameskip-v0), EnvSpec(RoadRunnerNoFrameskip-v4), EnvSpec(RoadRunner-ram-v0), EnvSpec(RoadRunner-ram-v4), EnvSpec(RoadRunner-ramDeterministic-v0), EnvSpec(RoadRunner-ramDeterministic-v4), EnvSpec(RoadRunner-ramNoFrameskip-v0), EnvSpec(RoadRunner-ramNoFrameskip-v4), EnvSpec(Robotank-v0), EnvSpec(Robotank-v4), EnvSpec(RobotankDeterministic-v0), EnvSpec(RobotankDeterministic-v4), EnvSpec(RobotankNoFrameskip-v0), EnvSpec(RobotankNoFrameskip-v4), EnvSpec(Robotank-ram-v0), EnvSpec(Robotank-ram-v4), EnvSpec(Robotank-ramDeterministic-v0), EnvSpec(Robotank-ramDeterministic-v4), EnvSpec(Robotank-ramNoFrameskip-v0), EnvSpec(Robotank-ramNoFrameskip-v4), EnvSpec(Seaquest-v0), EnvSpec(Seaquest-v4), EnvSpec(SeaquestDeterministic-v0), EnvSpec(SeaquestDeterministic-v4), EnvSpec(SeaquestNoFrameskip-v0), EnvSpec(SeaquestNoFrameskip-v4), EnvSpec(Seaquest-ram-v0), EnvSpec(Seaquest-ram-v4), EnvSpec(Seaquest-ramDeterministic-v0), EnvSpec(Seaquest-ramDeterministic-v4), EnvSpec(Seaquest-ramNoFrameskip-v0), EnvSpec(Seaquest-ramNoFrameskip-v4), EnvSpec(Skiing-v0), EnvSpec(Skiing-v4), EnvSpec(SkiingDeterministic-v0), EnvSpec(SkiingDeterministic-v4), EnvSpec(SkiingNoFrameskip-v0), EnvSpec(SkiingNoFrameskip-v4), EnvSpec(Skiing-ram-v0), EnvSpec(Skiing-ram-v4), EnvSpec(Skiing-ramDeterministic-v0), EnvSpec(Skiing-ramDeterministic-v4), EnvSpec(Skiing-ramNoFrameskip-v0), EnvSpec(Skiing-ramNoFrameskip-v4), EnvSpec(Solaris-v0), EnvSpec(Solaris-v4), EnvSpec(SolarisDeterministic-v0), EnvSpec(SolarisDeterministic-v4), EnvSpec(SolarisNoFrameskip-v0), EnvSpec(SolarisNoFrameskip-v4), EnvSpec(Solaris-ram-v0), EnvSpec(Solaris-ram-v4), EnvSpec(Solaris-ramDeterministic-v0), EnvSpec(Solaris-ramDeterministic-v4), EnvSpec(Solaris-ramNoFrameskip-v0), EnvSpec(Solaris-ramNoFrameskip-v4), EnvSpec(SpaceInvaders-v0), EnvSpec(SpaceInvaders-v4), EnvSpec(SpaceInvadersDeterministic-v0), EnvSpec(SpaceInvadersDeterministic-v4), EnvSpec(SpaceInvadersNoFrameskip-v0), EnvSpec(SpaceInvadersNoFrameskip-v4), EnvSpec(SpaceInvaders-ram-v0), EnvSpec(SpaceInvaders-ram-v4), EnvSpec(SpaceInvaders-ramDeterministic-v0), EnvSpec(SpaceInvaders-ramDeterministic-v4), EnvSpec(SpaceInvaders-ramNoFrameskip-v0), EnvSpec(SpaceInvaders-ramNoFrameskip-v4), EnvSpec(StarGunner-v0), EnvSpec(StarGunner-v4), EnvSpec(StarGunnerDeterministic-v0), EnvSpec(StarGunnerDeterministic-v4), EnvSpec(StarGunnerNoFrameskip-v0), EnvSpec(StarGunnerNoFrameskip-v4), EnvSpec(StarGunner-ram-v0), EnvSpec(StarGunner-ram-v4), EnvSpec(StarGunner-ramDeterministic-v0), EnvSpec(StarGunner-ramDeterministic-v4), EnvSpec(StarGunner-ramNoFrameskip-v0), EnvSpec(StarGunner-ramNoFrameskip-v4), EnvSpec(Tennis-v0), EnvSpec(Tennis-v4), EnvSpec(TennisDeterministic-v0), EnvSpec(TennisDeterministic-v4), EnvSpec(TennisNoFrameskip-v0), EnvSpec(TennisNoFrameskip-v4), EnvSpec(Tennis-ram-v0), EnvSpec(Tennis-ram-v4), EnvSpec(Tennis-ramDeterministic-v0), EnvSpec(Tennis-ramDeterministic-v4), EnvSpec(Tennis-ramNoFrameskip-v0), EnvSpec(Tennis-ramNoFrameskip-v4), EnvSpec(TimePilot-v0), EnvSpec(TimePilot-v4), EnvSpec(TimePilotDeterministic-v0), EnvSpec(TimePilotDeterministic-v4), EnvSpec(TimePilotNoFrameskip-v0), EnvSpec(TimePilotNoFrameskip-v4), EnvSpec(TimePilot-ram-v0), EnvSpec(TimePilot-ram-v4), EnvSpec(TimePilot-ramDeterministic-v0), EnvSpec(TimePilot-ramDeterministic-v4), EnvSpec(TimePilot-ramNoFrameskip-v0), EnvSpec(TimePilot-ramNoFrameskip-v4), EnvSpec(Tutankham-v0), EnvSpec(Tutankham-v4), EnvSpec(TutankhamDeterministic-v0), EnvSpec(TutankhamDeterministic-v4), EnvSpec(TutankhamNoFrameskip-v0), EnvSpec(TutankhamNoFrameskip-v4), EnvSpec(Tutankham-ram-v0), EnvSpec(Tutankham-ram-v4), EnvSpec(Tutankham-ramDeterministic-v0), EnvSpec(Tutankham-ramDeterministic-v4), EnvSpec(Tutankham-ramNoFrameskip-v0), EnvSpec(Tutankham-ramNoFrameskip-v4), EnvSpec(UpNDown-v0), EnvSpec(UpNDown-v4), EnvSpec(UpNDownDeterministic-v0), EnvSpec(UpNDownDeterministic-v4), EnvSpec(UpNDownNoFrameskip-v0), EnvSpec(UpNDownNoFrameskip-v4), EnvSpec(UpNDown-ram-v0), EnvSpec(UpNDown-ram-v4), EnvSpec(UpNDown-ramDeterministic-v0), EnvSpec(UpNDown-ramDeterministic-v4), EnvSpec(UpNDown-ramNoFrameskip-v0), EnvSpec(UpNDown-ramNoFrameskip-v4), EnvSpec(Venture-v0), EnvSpec(Venture-v4), EnvSpec(VentureDeterministic-v0), EnvSpec(VentureDeterministic-v4), EnvSpec(VentureNoFrameskip-v0), EnvSpec(VentureNoFrameskip-v4), EnvSpec(Venture-ram-v0), EnvSpec(Venture-ram-v4), EnvSpec(Venture-ramDeterministic-v0), EnvSpec(Venture-ramDeterministic-v4), EnvSpec(Venture-ramNoFrameskip-v0), EnvSpec(Venture-ramNoFrameskip-v4), EnvSpec(VideoPinball-v0), EnvSpec(VideoPinball-v4), EnvSpec(VideoPinballDeterministic-v0), EnvSpec(VideoPinballDeterministic-v4), EnvSpec(VideoPinballNoFrameskip-v0), EnvSpec(VideoPinballNoFrameskip-v4), EnvSpec(VideoPinball-ram-v0), EnvSpec(VideoPinball-ram-v4), EnvSpec(VideoPinball-ramDeterministic-v0), EnvSpec(VideoPinball-ramDeterministic-v4), EnvSpec(VideoPinball-ramNoFrameskip-v0), EnvSpec(VideoPinball-ramNoFrameskip-v4), EnvSpec(WizardOfWor-v0), EnvSpec(WizardOfWor-v4), EnvSpec(WizardOfWorDeterministic-v0), EnvSpec(WizardOfWorDeterministic-v4), EnvSpec(WizardOfWorNoFrameskip-v0), EnvSpec(WizardOfWorNoFrameskip-v4), EnvSpec(WizardOfWor-ram-v0), EnvSpec(WizardOfWor-ram-v4), EnvSpec(WizardOfWor-ramDeterministic-v0), EnvSpec(WizardOfWor-ramDeterministic-v4), EnvSpec(WizardOfWor-ramNoFrameskip-v0), EnvSpec(WizardOfWor-ramNoFrameskip-v4), EnvSpec(YarsRevenge-v0), EnvSpec(YarsRevenge-v4), EnvSpec(YarsRevengeDeterministic-v0), EnvSpec(YarsRevengeDeterministic-v4), EnvSpec(YarsRevengeNoFrameskip-v0), EnvSpec(YarsRevengeNoFrameskip-v4), EnvSpec(YarsRevenge-ram-v0), EnvSpec(YarsRevenge-ram-v4), EnvSpec(YarsRevenge-ramDeterministic-v0), EnvSpec(YarsRevenge-ramDeterministic-v4), EnvSpec(YarsRevenge-ramNoFrameskip-v0), EnvSpec(YarsRevenge-ramNoFrameskip-v4), EnvSpec(Zaxxon-v0), EnvSpec(Zaxxon-v4), EnvSpec(ZaxxonDeterministic-v0), EnvSpec(ZaxxonDeterministic-v4), EnvSpec(ZaxxonNoFrameskip-v0), EnvSpec(ZaxxonNoFrameskip-v4), EnvSpec(Zaxxon-ram-v0), EnvSpec(Zaxxon-ram-v4), EnvSpec(Zaxxon-ramDeterministic-v0), EnvSpec(Zaxxon-ramDeterministic-v4), EnvSpec(Zaxxon-ramNoFrameskip-v0), EnvSpec(Zaxxon-ramNoFrameskip-v4), EnvSpec(CubeCrash-v0), EnvSpec(CubeCrashSparse-v0), EnvSpec(CubeCrashScreenBecomesBlack-v0), EnvSpec(MemorizeDigits-v0)])\n"
     ]
    }
   ],
   "source": [
    "from gym import envs\n",
    "print(envs.registry.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment’s **step** function returns  four values. These are:\n",
    "\n",
    "* **observation (object):** an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "* **reward (float):** amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "* **done (boolean):** whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.).\n",
    "* **info (dict):** diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.\n",
    "\n",
    "The typical agent loop consists in first calling the method *reset* which provides an initial observation. Then the agent executes an action, and receives the reward, the new observation, and if the episode has finished (done is true). \n",
    "\n",
    "For example, analyze this sample of agent loop for 100 ms. The details of the previous variables for this game as described [here](https://github.com/openai/gym/wiki/CartPole-v0) are:\n",
    "* **observation**: Cart Position, Cart Velocity, Pole Angle, Pole Velocity.\n",
    "* **action**: 0\t(Push cart to the left), 1\t(Push cart to the right).\n",
    "* **reward**: 1  for every step taken, including the termination step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[-0.03841348  0.04244381 -0.00503248 -0.01166684]\n",
      "Action  1\n",
      "Observation  [-0.0375646   0.23763757 -0.00526582 -0.30593331] , reward  1.0 , done  False , info  {}\n",
      "[-0.0375646   0.23763757 -0.00526582 -0.30593331]\n",
      "Action  1\n",
      "Observation  [-0.03281185  0.43283416 -0.01138448 -0.60027229] , reward  1.0 , done  False , info  {}\n",
      "[-0.03281185  0.43283416 -0.01138448 -0.60027229]\n",
      "Action  0\n",
      "Observation  [-0.02415517  0.23787331 -0.02338993 -0.31119693] , reward  1.0 , done  False , info  {}\n",
      "[-0.02415517  0.23787331 -0.02338993 -0.31119693]\n",
      "Action  0\n",
      "Observation  [-0.0193977   0.04309227 -0.02961387 -0.0259813 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.0193977   0.04309227 -0.02961387 -0.0259813 ]\n",
      "Action  0\n",
      "Observation  [-0.01853586 -0.15159275 -0.03013349  0.25721299] , reward  1.0 , done  False , info  {}\n",
      "[-0.01853586 -0.15159275 -0.03013349  0.25721299]\n",
      "Action  1\n",
      "Observation  [-0.02156771  0.04394616 -0.02498923 -0.04482013] , reward  1.0 , done  False , info  {}\n",
      "[-0.02156771  0.04394616 -0.02498923 -0.04482013]\n",
      "Action  0\n",
      "Observation  [-0.02068879 -0.15080871 -0.02588564  0.23987489] , reward  1.0 , done  False , info  {}\n",
      "[-0.02068879 -0.15080871 -0.02588564  0.23987489]\n",
      "Action  0\n",
      "Observation  [-0.02370496 -0.3455515  -0.02108814  0.52428168] , reward  1.0 , done  False , info  {}\n",
      "[-0.02370496 -0.3455515  -0.02108814  0.52428168]\n",
      "Action  0\n",
      "Observation  [-0.03061599 -0.54037042 -0.0106025   0.81024564] , reward  1.0 , done  False , info  {}\n",
      "[-0.03061599 -0.54037042 -0.0106025   0.81024564]\n",
      "Action  1\n",
      "Observation  [-0.0414234  -0.34510482  0.00560241  0.51424663] , reward  1.0 , done  False , info  {}\n",
      "[-0.0414234  -0.34510482  0.00560241  0.51424663]\n",
      "Action  0\n",
      "Observation  [-0.0483255  -0.54030522  0.01588734  0.80868973] , reward  1.0 , done  False , info  {}\n",
      "[-0.0483255  -0.54030522  0.01588734  0.80868973]\n",
      "Action  0\n",
      "Observation  [-0.0591316  -0.73564124  0.03206114  1.10632746] , reward  1.0 , done  False , info  {}\n",
      "[-0.0591316  -0.73564124  0.03206114  1.10632746]\n",
      "Action  0\n",
      "Observation  [-0.07384443 -0.9311697   0.05418768  1.40889379] , reward  1.0 , done  False , info  {}\n",
      "[-0.07384443 -0.9311697   0.05418768  1.40889379]\n",
      "Action  1\n",
      "Observation  [-0.09246782 -0.7367602   0.08236556  1.13363095] , reward  1.0 , done  False , info  {}\n",
      "[-0.09246782 -0.7367602   0.08236556  1.13363095]\n",
      "Action  0\n",
      "Observation  [-0.10720302 -0.93285779  0.10503818  1.45096824] , reward  1.0 , done  False , info  {}\n",
      "[-0.10720302 -0.93285779  0.10503818  1.45096824]\n",
      "Action  1\n",
      "Observation  [-0.12586018 -0.73917162  0.13405754  1.1928647 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.12586018 -0.73917162  0.13405754  1.1928647 ]\n",
      "Action  1\n",
      "Observation  [-0.14064361 -0.54601623  0.15791484  0.94502615] , reward  1.0 , done  False , info  {}\n",
      "[-0.14064361 -0.54601623  0.15791484  0.94502615]\n",
      "Action  0\n",
      "Observation  [-0.15156394 -0.74287221  0.17681536  1.28287024] , reward  1.0 , done  False , info  {}\n",
      "[-0.15156394 -0.74287221  0.17681536  1.28287024]\n",
      "Action  1\n",
      "Observation  [-0.16642138 -0.55038686  0.20247277  1.0503571 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.16642138 -0.55038686  0.20247277  1.0503571 ]\n",
      "Action  0\n",
      "Observation  [-0.17742912 -0.74753408  0.22347991  1.39915814] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 20 timesteps\n",
      "[-0.00229379  0.00407557  0.01207787  0.03082686]\n",
      "Action  1\n",
      "Observation  [-0.00221228  0.19902226  0.01269441 -0.25802103] , reward  1.0 , done  False , info  {}\n",
      "[-0.00221228  0.19902226  0.01269441 -0.25802103]\n",
      "Action  0\n",
      "Observation  [0.00176817 0.0037214  0.00753399 0.03863872] , reward  1.0 , done  False , info  {}\n",
      "[0.00176817 0.0037214  0.00753399 0.03863872]\n",
      "Action  0\n",
      "Observation  [ 0.0018426  -0.19150778  0.00830676  0.33368914] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0018426  -0.19150778  0.00830676  0.33368914]\n",
      "Action  1\n",
      "Observation  [-0.00198756  0.00349496  0.01498054  0.04363727] , reward  1.0 , done  False , info  {}\n",
      "[-0.00198756  0.00349496  0.01498054  0.04363727]\n",
      "Action  0\n",
      "Observation  [-0.00191766 -0.19183856  0.01585329  0.3410088 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.00191766 -0.19183856  0.01585329  0.3410088 ]\n",
      "Action  1\n",
      "Observation  [-0.00575443  0.00305428  0.02267347  0.05336694] , reward  1.0 , done  False , info  {}\n",
      "[-0.00575443  0.00305428  0.02267347  0.05336694]\n",
      "Action  0\n",
      "Observation  [-0.00569334 -0.19238531  0.0237408   0.35311642] , reward  1.0 , done  False , info  {}\n",
      "[-0.00569334 -0.19238531  0.0237408   0.35311642]\n",
      "Action  1\n",
      "Observation  [-0.00954105  0.00239114  0.03080313  0.0680132 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.00954105  0.00239114  0.03080313  0.0680132 ]\n",
      "Action  1\n",
      "Observation  [-0.00949323  0.19705824  0.0321634  -0.21479423] , reward  1.0 , done  False , info  {}\n",
      "[-0.00949323  0.19705824  0.0321634  -0.21479423]\n",
      "Action  1\n",
      "Observation  [-0.00555206  0.39170596  0.02786751 -0.4971604 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.00555206  0.39170596  0.02786751 -0.4971604 ]\n",
      "Action  1\n",
      "Observation  [ 0.00228206  0.58642411  0.0179243  -0.78093223] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00228206  0.58642411  0.0179243  -0.78093223]\n",
      "Action  1\n",
      "Observation  [ 0.01401054  0.78129514  0.00230566 -1.06792236] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01401054  0.78129514  0.00230566 -1.06792236]\n",
      "Action  0\n",
      "Observation  [ 0.02963644  0.58614277 -0.01905279 -0.77451671] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02963644  0.58614277 -0.01905279 -0.77451671]\n",
      "Action  0\n",
      "Observation  [ 0.0413593   0.39128802 -0.03454312 -0.48788882] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0413593   0.39128802 -0.03454312 -0.48788882]\n",
      "Action  1\n",
      "Observation  [ 0.04918506  0.58687988 -0.0443009  -0.79125525] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04918506  0.58687988 -0.0443009  -0.79125525]\n",
      "Action  0\n",
      "Observation  [ 0.06092265  0.3923933  -0.060126   -0.5128318 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06092265  0.3923933  -0.060126   -0.5128318 ]\n",
      "Action  0\n",
      "Observation  [ 0.06877052  0.19816746 -0.07038264 -0.2396859 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06877052  0.19816746 -0.07038264 -0.2396859 ]\n",
      "Action  1\n",
      "Observation  [ 0.07273387  0.39422059 -0.07517636 -0.55371291] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07273387  0.39422059 -0.07517636 -0.55371291]\n",
      "Action  1\n",
      "Observation  [ 0.08061828  0.59031322 -0.08625062 -0.86910213] , reward  1.0 , done  False , info  {}\n",
      "[ 0.08061828  0.59031322 -0.08625062 -0.86910213]\n",
      "Action  1\n",
      "Observation  [ 0.09242455  0.786496   -0.10363266 -1.18760865] , reward  1.0 , done  False , info  {}\n",
      "[ 0.09242455  0.786496   -0.10363266 -1.18760865]\n",
      "Action  1\n",
      "Observation  [ 0.10815447  0.98279752 -0.12738483 -1.51089468] , reward  1.0 , done  False , info  {}\n",
      "[ 0.10815447  0.98279752 -0.12738483 -1.51089468]\n",
      "Action  0\n",
      "Observation  [ 0.12781042  0.78942818 -0.15760272 -1.26054075] , reward  1.0 , done  False , info  {}\n",
      "[ 0.12781042  0.78942818 -0.15760272 -1.26054075]\n",
      "Action  0\n",
      "Observation  [ 0.14359898  0.59663344 -0.18281354 -1.0210764 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.14359898  0.59663344 -0.18281354 -1.0210764 ]\n",
      "Action  1\n",
      "Observation  [ 0.15553165  0.79365779 -0.20323507 -1.36513644] , reward  1.0 , done  False , info  {}\n",
      "[ 0.15553165  0.79365779 -0.20323507 -1.36513644]\n",
      "Action  0\n",
      "Observation  [ 0.1714048   0.60157653 -0.2305378  -1.14228509] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 25 timesteps\n",
      "[-0.01319376  0.04407069  0.00589047 -0.03313298]\n",
      "Action  0\n",
      "Observation  [-0.01231234 -0.15113523  0.00522781  0.26140261] , reward  1.0 , done  False , info  {}\n",
      "[-0.01231234 -0.15113523  0.00522781  0.26140261]\n",
      "Action  1\n",
      "Observation  [-0.01533505  0.0439117   0.01045586 -0.02962683] , reward  1.0 , done  False , info  {}\n",
      "[-0.01533505  0.0439117   0.01045586 -0.02962683]\n",
      "Action  0\n",
      "Observation  [-0.01445681 -0.15135862  0.00986333  0.26633661] , reward  1.0 , done  False , info  {}\n",
      "[-0.01445681 -0.15135862  0.00986333  0.26633661]\n",
      "Action  0\n",
      "Observation  [-0.01748399 -0.34661994  0.01519006  0.56211412] , reward  1.0 , done  False , info  {}\n",
      "[-0.01748399 -0.34661994  0.01519006  0.56211412]\n",
      "Action  1\n",
      "Observation  [-0.02441638 -0.15171441  0.02643234  0.27425526] , reward  1.0 , done  False , info  {}\n",
      "[-0.02441638 -0.15171441  0.02643234  0.27425526]\n",
      "Action  0\n",
      "Observation  [-0.02745067 -0.34720334  0.03191745  0.57515642] , reward  1.0 , done  False , info  {}\n",
      "[-0.02745067 -0.34720334  0.03191745  0.57515642]\n",
      "Action  0\n",
      "Observation  [-0.03439474 -0.54275785  0.04342057  0.87772093] , reward  1.0 , done  False , info  {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03439474 -0.54275785  0.04342057  0.87772093]\n",
      "Action  0\n",
      "Observation  [-0.0452499  -0.73844213  0.06097499  1.18373233] , reward  1.0 , done  False , info  {}\n",
      "[-0.0452499  -0.73844213  0.06097499  1.18373233]\n",
      "Action  1\n",
      "Observation  [-0.06001874 -0.54416197  0.08464964  0.9107692 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.06001874 -0.54416197  0.08464964  0.9107692 ]\n",
      "Action  1\n",
      "Observation  [-0.07090198 -0.35028121  0.10286502  0.64584668] , reward  1.0 , done  False , info  {}\n",
      "[-0.07090198 -0.35028121  0.10286502  0.64584668]\n",
      "Action  0\n",
      "Observation  [-0.0779076  -0.54667471  0.11578196  0.96906875] , reward  1.0 , done  False , info  {}\n",
      "[-0.0779076  -0.54667471  0.11578196  0.96906875]\n",
      "Action  1\n",
      "Observation  [-0.0888411  -0.35328145  0.13516333  0.71488498] , reward  1.0 , done  False , info  {}\n",
      "[-0.0888411  -0.35328145  0.13516333  0.71488498]\n",
      "Action  0\n",
      "Observation  [-0.09590673 -0.54998977  0.14946103  1.04687343] , reward  1.0 , done  False , info  {}\n",
      "[-0.09590673 -0.54998977  0.14946103  1.04687343]\n",
      "Action  1\n",
      "Observation  [-0.10690652 -0.35713321  0.1703985   0.80459181] , reward  1.0 , done  False , info  {}\n",
      "[-0.10690652 -0.35713321  0.1703985   0.80459181]\n",
      "Action  0\n",
      "Observation  [-0.11404919 -0.5541303   0.18649034  1.14566294] , reward  1.0 , done  False , info  {}\n",
      "[-0.11404919 -0.5541303   0.18649034  1.14566294]\n",
      "Action  1\n",
      "Observation  [-0.12513179 -0.36186751  0.2094036   0.91678012] , reward  1.0 , done  False , info  {}\n",
      "[-0.12513179 -0.36186751  0.2094036   0.91678012]\n",
      "Action  0\n",
      "Observation  [-0.13236914 -0.55911147  0.2277392   1.26729859] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 17 timesteps\n",
      "[ 0.02849956 -0.04669834 -0.00087281  0.03600776]\n",
      "Action  0\n",
      "Observation  [ 2.75655959e-02 -2.41807763e-01 -1.52655396e-04  3.28415174e-01] , reward  1.0 , done  False , info  {}\n",
      "[ 2.75655959e-02 -2.41807763e-01 -1.52655396e-04  3.28415174e-01]\n",
      "Action  1\n",
      "Observation  [ 0.02272944 -0.04668364  0.00641565  0.03568411] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02272944 -0.04668364  0.00641565  0.03568411]\n",
      "Action  0\n",
      "Observation  [ 0.02179577 -0.241897    0.00712933  0.33038432] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02179577 -0.241897    0.00712933  0.33038432]\n",
      "Action  0\n",
      "Observation  [ 0.01695783 -0.43711971  0.01373702  0.62530694] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01695783 -0.43711971  0.01373702  0.62530694]\n",
      "Action  1\n",
      "Observation  [ 0.00821543 -0.24219219  0.02624316  0.33698181] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00821543 -0.24219219  0.02624316  0.33698181]\n",
      "Action  1\n",
      "Observation  [ 0.00337159 -0.04745334  0.03298279  0.05268871] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00337159 -0.04745334  0.03298279  0.05268871]\n",
      "Action  0\n",
      "Observation  [ 0.00242252 -0.24303232  0.03403657  0.3555928 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00242252 -0.24303232  0.03403657  0.3555928 ]\n",
      "Action  0\n",
      "Observation  [-0.00243812 -0.43862125  0.04114842  0.6588111 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.00243812 -0.43862125  0.04114842  0.6588111 ]\n",
      "Action  0\n",
      "Observation  [-0.01121055 -0.63429102  0.05432464  0.96416153] , reward  1.0 , done  False , info  {}\n",
      "[-0.01121055 -0.63429102  0.05432464  0.96416153]\n",
      "Action  1\n",
      "Observation  [-0.02389637 -0.43993931  0.07360787  0.68902763] , reward  1.0 , done  False , info  {}\n",
      "[-0.02389637 -0.43993931  0.07360787  0.68902763]\n",
      "Action  1\n",
      "Observation  [-0.03269515 -0.24591192  0.08738843  0.42039581] , reward  1.0 , done  False , info  {}\n",
      "[-0.03269515 -0.24591192  0.08738843  0.42039581]\n",
      "Action  0\n",
      "Observation  [-0.03761339 -0.44215635  0.09579634  0.73929868] , reward  1.0 , done  False , info  {}\n",
      "[-0.03761339 -0.44215635  0.09579634  0.73929868]\n",
      "Action  0\n",
      "Observation  [-0.04645652 -0.63846135  0.11058232  1.06052717] , reward  1.0 , done  False , info  {}\n",
      "[-0.04645652 -0.63846135  0.11058232  1.06052717]\n",
      "Action  0\n",
      "Observation  [-0.05922575 -0.83486021  0.13179286  1.38577104] , reward  1.0 , done  False , info  {}\n",
      "[-0.05922575 -0.83486021  0.13179286  1.38577104]\n",
      "Action  0\n",
      "Observation  [-0.07592295 -1.03135596  0.15950828  1.71659364] , reward  1.0 , done  False , info  {}\n",
      "[-0.07592295 -1.03135596  0.15950828  1.71659364]\n",
      "Action  1\n",
      "Observation  [-0.09655007 -0.83838259  0.19384015  1.47750496] , reward  1.0 , done  False , info  {}\n",
      "[-0.09655007 -0.83838259  0.19384015  1.47750496]\n",
      "Action  0\n",
      "Observation  [-0.11331772 -1.03527056  0.22339025  1.82393866] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 17 timesteps\n",
      "[ 0.02727926  0.04820152  0.00395783 -0.02631236]\n",
      "Action  0\n",
      "Observation  [ 0.02824329 -0.14697697  0.00343158  0.26761667] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02824329 -0.14697697  0.00343158  0.26761667]\n",
      "Action  1\n",
      "Observation  [ 0.02530375  0.04809584  0.00878392 -0.02398194] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02530375  0.04809584  0.00878392 -0.02398194]\n",
      "Action  1\n",
      "Observation  [ 0.02626567  0.24309073  0.00830428 -0.31388055] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02626567  0.24309073  0.00830428 -0.31388055]\n",
      "Action  1\n",
      "Observation  [ 0.03112748  0.4380934   0.00202667 -0.60393304] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03112748  0.4380934   0.00202667 -0.60393304]\n",
      "Action  1\n",
      "Observation  [ 0.03988935  0.63318695 -0.01005199 -0.89597693] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03988935  0.63318695 -0.01005199 -0.89597693]\n",
      "Action  0\n",
      "Observation  [ 0.05255309  0.43820272 -0.02797153 -0.60647059] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05255309  0.43820272 -0.02797153 -0.60647059]\n",
      "Action  0\n",
      "Observation  [ 0.06131714  0.24348282 -0.04010094 -0.32272755] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06131714  0.24348282 -0.04010094 -0.32272755]\n",
      "Action  1\n",
      "Observation  [ 0.0661868   0.43915218 -0.04655549 -0.62778215] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0661868   0.43915218 -0.04655549 -0.62778215]\n",
      "Action  1\n",
      "Observation  [ 0.07496984  0.63489193 -0.05911114 -0.93475601] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07496984  0.63489193 -0.05911114 -0.93475601]\n",
      "Action  0\n",
      "Observation  [ 0.08766768  0.44061499 -0.07780626 -0.66121814] , reward  1.0 , done  False , info  {}\n",
      "[ 0.08766768  0.44061499 -0.07780626 -0.66121814]\n",
      "Action  0\n",
      "Observation  [ 0.09647998  0.24665699 -0.09103062 -0.39401329] , reward  1.0 , done  False , info  {}\n",
      "[ 0.09647998  0.24665699 -0.09103062 -0.39401329]\n",
      "Action  0\n",
      "Observation  [ 0.10141312  0.05293672 -0.09891089 -0.13136207] , reward  1.0 , done  False , info  {}\n",
      "[ 0.10141312  0.05293672 -0.09891089 -0.13136207]\n",
      "Action  1\n",
      "Observation  [ 0.10247186  0.24932617 -0.10153813 -0.45353882] , reward  1.0 , done  False , info  {}\n",
      "[ 0.10247186  0.24932617 -0.10153813 -0.45353882]\n",
      "Action  1\n",
      "Observation  [ 0.10745838  0.44572634 -0.1106089  -0.77642266] , reward  1.0 , done  False , info  {}\n",
      "[ 0.10745838  0.44572634 -0.1106089  -0.77642266]\n",
      "Action  1\n",
      "Observation  [ 0.11637291  0.64218161 -0.12613736 -1.10175752] , reward  1.0 , done  False , info  {}\n",
      "[ 0.11637291  0.64218161 -0.12613736 -1.10175752]\n",
      "Action  0\n",
      "Observation  [ 0.12921654  0.44892434 -0.14817251 -0.85116082] , reward  1.0 , done  False , info  {}\n",
      "[ 0.12921654  0.44892434 -0.14817251 -0.85116082]\n",
      "Action  0\n",
      "Observation  [ 0.13819502  0.25609959 -0.16519572 -0.60849649] , reward  1.0 , done  False , info  {}\n",
      "[ 0.13819502  0.25609959 -0.16519572 -0.60849649]\n",
      "Action  0\n",
      "Observation  [ 0.14331702  0.06362537 -0.17736565 -0.37206257] , reward  1.0 , done  False , info  {}\n",
      "[ 0.14331702  0.06362537 -0.17736565 -0.37206257]\n",
      "Action  1\n",
      "Observation  [ 0.14458952  0.26076511 -0.1848069  -0.71500559] , reward  1.0 , done  False , info  {}\n",
      "[ 0.14458952  0.26076511 -0.1848069  -0.71500559]\n",
      "Action  1\n",
      "Observation  [ 0.14980483  0.45789882 -0.19910702 -1.05969536] , reward  1.0 , done  False , info  {}\n",
      "[ 0.14980483  0.45789882 -0.19910702 -1.05969536]\n",
      "Action  0\n",
      "Observation  [ 0.1589628   0.26589043 -0.22030092 -0.83552433] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 21 timesteps\n",
      "[-0.04592437  0.04545009  0.00089735 -0.00551942]\n",
      "Action  0\n",
      "Observation  [-0.04501537 -0.14968472  0.00078696  0.28744649] , reward  1.0 , done  False , info  {}\n",
      "[-0.04501537 -0.14968472  0.00078696  0.28744649]\n",
      "Action  1\n",
      "Observation  [-0.04800907  0.045426    0.00653589 -0.00498813] , reward  1.0 , done  False , info  {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04800907  0.045426    0.00653589 -0.00498813]\n",
      "Action  1\n",
      "Observation  [-0.04710055  0.24045361  0.00643613 -0.29560176] , reward  1.0 , done  False , info  {}\n",
      "[-0.04710055  0.24045361  0.00643613 -0.29560176]\n",
      "Action  1\n",
      "Observation  [-4.22914735e-02  4.35483215e-01  5.24090835e-04 -5.86247903e-01] , reward  1.0 , done  False , info  {}\n",
      "[-4.22914735e-02  4.35483215e-01  5.24090835e-04 -5.86247903e-01]\n",
      "Action  0\n",
      "Observation  [-0.03358181  0.24035393 -0.01120087 -0.29339993] , reward  1.0 , done  False , info  {}\n",
      "[-0.03358181  0.24035393 -0.01120087 -0.29339993]\n",
      "Action  0\n",
      "Observation  [-0.02877473  0.04539345 -0.01706887 -0.00427054] , reward  1.0 , done  False , info  {}\n",
      "[-0.02877473  0.04539345 -0.01706887 -0.00427054]\n",
      "Action  1\n",
      "Observation  [-0.02786686  0.24075598 -0.01715428 -0.30228965] , reward  1.0 , done  False , info  {}\n",
      "[-0.02786686  0.24075598 -0.01715428 -0.30228965]\n",
      "Action  0\n",
      "Observation  [-0.02305174  0.04588266 -0.02320007 -0.0150658 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.02305174  0.04588266 -0.02320007 -0.0150658 ]\n",
      "Action  1\n",
      "Observation  [-0.02213409  0.24132952 -0.02350139 -0.31497739] , reward  1.0 , done  False , info  {}\n",
      "[-0.02213409  0.24132952 -0.02350139 -0.31497739]\n",
      "Action  0\n",
      "Observation  [-0.0173075   0.04655008 -0.02980093 -0.02979769] , reward  1.0 , done  False , info  {}\n",
      "[-0.0173075   0.04655008 -0.02980093 -0.02979769]\n",
      "Action  0\n",
      "Observation  [-0.0163765  -0.14813211 -0.03039689  0.25333576] , reward  1.0 , done  False , info  {}\n",
      "[-0.0163765  -0.14813211 -0.03039689  0.25333576]\n",
      "Action  0\n",
      "Observation  [-0.01933914 -0.34280714 -0.02533017  0.5362781 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.01933914 -0.34280714 -0.02533017  0.5362781 ]\n",
      "Action  1\n",
      "Observation  [-0.02619528 -0.14733835 -0.01460461  0.2357227 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.02619528 -0.14733835 -0.01460461  0.2357227 ]\n",
      "Action  0\n",
      "Observation  [-0.02914205 -0.34224863 -0.00989016  0.52376334] , reward  1.0 , done  False , info  {}\n",
      "[-0.02914205 -0.34224863 -0.00989016  0.52376334]\n",
      "Action  1\n",
      "Observation  [-0.03598702 -0.1469889   0.00058511  0.2279804 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.03598702 -0.1469889   0.00058511  0.2279804 ]\n",
      "Action  0\n",
      "Observation  [-0.0389268  -0.3421192   0.00514472  0.52084783] , reward  1.0 , done  False , info  {}\n",
      "[-0.0389268  -0.3421192   0.00514472  0.52084783]\n",
      "Action  1\n",
      "Observation  [-0.04576918 -0.14707005  0.01556168  0.22979052] , reward  1.0 , done  False , info  {}\n",
      "[-0.04576918 -0.14707005  0.01556168  0.22979052]\n",
      "Action  1\n",
      "Observation  [-0.04871058  0.04782611  0.02015749 -0.05794338] , reward  1.0 , done  False , info  {}\n",
      "[-0.04871058  0.04782611  0.02015749 -0.05794338]\n",
      "Action  1\n",
      "Observation  [-0.04775406  0.24265332  0.01899862 -0.34419893] , reward  1.0 , done  False , info  {}\n",
      "[-0.04775406  0.24265332  0.01899862 -0.34419893]\n",
      "Action  1\n",
      "Observation  [-0.042901    0.43749992  0.01211464 -0.63083083] , reward  1.0 , done  False , info  {}\n",
      "[-0.042901    0.43749992  0.01211464 -0.63083083]\n",
      "Action  1\n",
      "Observation  [-3.41509975e-02  6.32450759e-01 -5.01977130e-04 -9.19674006e-01] , reward  1.0 , done  False , info  {}\n",
      "[-3.41509975e-02  6.32450759e-01 -5.01977130e-04 -9.19674006e-01]\n",
      "Action  0\n",
      "Observation  [-0.02150198  0.4373356  -0.01889546 -0.62714888] , reward  1.0 , done  False , info  {}\n",
      "[-0.02150198  0.4373356  -0.01889546 -0.62714888]\n",
      "Action  0\n",
      "Observation  [-0.01275527  0.24248241 -0.03143843 -0.34047621] , reward  1.0 , done  False , info  {}\n",
      "[-0.01275527  0.24248241 -0.03143843 -0.34047621]\n",
      "Action  1\n",
      "Observation  [-0.00790562  0.43803724 -0.03824796 -0.64290489] , reward  1.0 , done  False , info  {}\n",
      "[-0.00790562  0.43803724 -0.03824796 -0.64290489]\n",
      "Action  1\n",
      "Observation  [ 8.55122641e-04  6.33670841e-01 -5.11060568e-02 -9.47382824e-01] , reward  1.0 , done  False , info  {}\n",
      "[ 8.55122641e-04  6.33670841e-01 -5.11060568e-02 -9.47382824e-01]\n",
      "Action  1\n",
      "Observation  [ 0.01352854  0.82944234 -0.07005371 -1.25567531] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01352854  0.82944234 -0.07005371 -1.25567531]\n",
      "Action  0\n",
      "Observation  [ 0.03011739  0.63528371 -0.09516722 -0.98573065] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03011739  0.63528371 -0.09516722 -0.98573065]\n",
      "Action  1\n",
      "Observation  [ 0.04282306  0.83154253 -0.11488183 -1.30672372] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04282306  0.83154253 -0.11488183 -1.30672372]\n",
      "Action  0\n",
      "Observation  [ 0.05945391  0.63804877 -0.14101631 -1.05209726] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05945391  0.63804877 -0.14101631 -1.05209726]\n",
      "Action  1\n",
      "Observation  [ 0.07221489  0.83473033 -0.16205825 -1.38551264] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07221489  0.83473033 -0.16205825 -1.38551264]\n",
      "Action  0\n",
      "Observation  [ 0.08890949  0.64195708 -0.1897685  -1.1475784 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.08890949  0.64195708 -0.1897685  -1.1475784 ]\n",
      "Action  1\n",
      "Observation  [ 0.10174863  0.83898041 -0.21272007 -1.4932656 ] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 32 timesteps\n",
      "[ 0.00358108  0.03314799  0.04621539 -0.04765067]\n",
      "Action  0\n",
      "Observation  [ 0.00424404 -0.16260514  0.04526238  0.25924799] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00424404 -0.16260514  0.04526238  0.25924799]\n",
      "Action  0\n",
      "Observation  [ 0.00099194 -0.35834305  0.05044734  0.56585674] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00099194 -0.35834305  0.05044734  0.56585674]\n",
      "Action  1\n",
      "Observation  [-0.00617493 -0.16396377  0.06176448  0.28948399] , reward  1.0 , done  False , info  {}\n",
      "[-0.00617493 -0.16396377  0.06176448  0.28948399]\n",
      "Action  1\n",
      "Observation  [-0.0094542   0.03022555  0.06755416  0.01690265] , reward  1.0 , done  False , info  {}\n",
      "[-0.0094542   0.03022555  0.06755416  0.01690265]\n",
      "Action  1\n",
      "Observation  [-0.00884969  0.22431692  0.06789221 -0.25372454] , reward  1.0 , done  False , info  {}\n",
      "[-0.00884969  0.22431692  0.06789221 -0.25372454]\n",
      "Action  1\n",
      "Observation  [-0.00436335  0.41840709  0.06281772 -0.52424409] , reward  1.0 , done  False , info  {}\n",
      "[-0.00436335  0.41840709  0.06281772 -0.52424409]\n",
      "Action  0\n",
      "Observation  [ 0.00400479  0.22245994  0.05233284 -0.21244683] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00400479  0.22245994  0.05233284 -0.21244683]\n",
      "Action  0\n",
      "Observation  [0.00845399 0.02663034 0.0480839  0.09627425] , reward  1.0 , done  False , info  {}\n",
      "[0.00845399 0.02663034 0.0480839  0.09627425]\n",
      "Action  0\n",
      "Observation  [ 0.0089866  -0.16914661  0.05000938  0.40373148] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0089866  -0.16914661  0.05000938  0.40373148]\n",
      "Action  0\n",
      "Observation  [ 0.00560366 -0.36494082  0.05808401  0.71175225] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00560366 -0.36494082  0.05808401  0.71175225]\n",
      "Action  0\n",
      "Observation  [-0.00169515 -0.56081692  0.07231906  1.02213801] , reward  1.0 , done  False , info  {}\n",
      "[-0.00169515 -0.56081692  0.07231906  1.02213801]\n",
      "Action  0\n",
      "Observation  [-0.01291149 -0.75682389  0.09276182  1.33662324] , reward  1.0 , done  False , info  {}\n",
      "[-0.01291149 -0.75682389  0.09276182  1.33662324]\n",
      "Action  1\n",
      "Observation  [-0.02804797 -0.56298491  0.11949428  1.07434769] , reward  1.0 , done  False , info  {}\n",
      "[-0.02804797 -0.56298491  0.11949428  1.07434769]\n",
      "Action  0\n",
      "Observation  [-0.03930767 -0.75946577  0.14098124  1.40201511] , reward  1.0 , done  False , info  {}\n",
      "[-0.03930767 -0.75946577  0.14098124  1.40201511]\n",
      "Action  0\n",
      "Observation  [-0.05449698 -0.95602947  0.16902154  1.7352467 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.05449698 -0.95602947  0.16902154  1.7352467 ]\n",
      "Action  0\n",
      "Observation  [-0.07361757 -1.15262831  0.20372647  2.07539867] , reward  1.0 , done  False , info  {}\n",
      "[-0.07361757 -1.15262831  0.20372647  2.07539867]\n",
      "Action  1\n",
      "Observation  [-0.09667014 -0.96007444  0.24523445  1.85202318] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 17 timesteps\n",
      "[-0.00103567  0.0318056  -0.0189267   0.00391523]\n",
      "Action  0\n",
      "Observation  [-0.00039956 -0.16303987 -0.0188484   0.29056697] , reward  1.0 , done  False , info  {}\n",
      "[-0.00039956 -0.16303987 -0.0188484   0.29056697]\n",
      "Action  1\n",
      "Observation  [-0.00366036  0.0323457  -0.01303706 -0.00800042] , reward  1.0 , done  False , info  {}\n",
      "[-0.00366036  0.0323457  -0.01303706 -0.00800042]\n",
      "Action  0\n",
      "Observation  [-0.00301345 -0.16258688 -0.01319706  0.28054081] , reward  1.0 , done  False , info  {}\n",
      "[-0.00301345 -0.16258688 -0.01319706  0.28054081]\n",
      "Action  0\n",
      "Observation  [-0.00626518 -0.35751812 -0.00758625  0.56903238] , reward  1.0 , done  False , info  {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00626518 -0.35751812 -0.00758625  0.56903238]\n",
      "Action  1\n",
      "Observation  [-0.01341555 -0.16229059  0.0037944   0.27396918] , reward  1.0 , done  False , info  {}\n",
      "[-0.01341555 -0.16229059  0.0037944   0.27396918]\n",
      "Action  0\n",
      "Observation  [-0.01666136 -0.35746647  0.00927378  0.56784645] , reward  1.0 , done  False , info  {}\n",
      "[-0.01666136 -0.35746647  0.00927378  0.56784645]\n",
      "Action  0\n",
      "Observation  [-0.02381069 -0.55271727  0.02063071  0.86343651] , reward  1.0 , done  False , info  {}\n",
      "[-0.02381069 -0.55271727  0.02063071  0.86343651]\n",
      "Action  1\n",
      "Observation  [-0.03486503 -0.35788217  0.03789944  0.57731105] , reward  1.0 , done  False , info  {}\n",
      "[-0.03486503 -0.35788217  0.03789944  0.57731105]\n",
      "Action  1\n",
      "Observation  [-0.04202268 -0.16331135  0.04944566  0.29680417] , reward  1.0 , done  False , info  {}\n",
      "[-0.04202268 -0.16331135  0.04944566  0.29680417]\n",
      "Action  0\n",
      "Observation  [-0.0452889  -0.35910203  0.05538175  0.60466235] , reward  1.0 , done  False , info  {}\n",
      "[-0.0452889  -0.35910203  0.05538175  0.60466235]\n",
      "Action  0\n",
      "Observation  [-0.05247094 -0.55495296  0.06747499  0.91426224] , reward  1.0 , done  False , info  {}\n",
      "[-0.05247094 -0.55495296  0.06747499  0.91426224]\n",
      "Action  1\n",
      "Observation  [-0.06357    -0.36080536  0.08576024  0.64352614] , reward  1.0 , done  False , info  {}\n",
      "[-0.06357    -0.36080536  0.08576024  0.64352614]\n",
      "Action  1\n",
      "Observation  [-0.07078611 -0.16697676  0.09863076  0.37903438] , reward  1.0 , done  False , info  {}\n",
      "[-0.07078611 -0.16697676  0.09863076  0.37903438]\n",
      "Action  1\n",
      "Observation  [-0.07412564  0.02661628  0.10621145  0.11900658] , reward  1.0 , done  False , info  {}\n",
      "[-0.07412564  0.02661628  0.10621145  0.11900658]\n",
      "Action  1\n",
      "Observation  [-0.07359332  0.22006882  0.10859158 -0.13836954] , reward  1.0 , done  False , info  {}\n",
      "[-0.07359332  0.22006882  0.10859158 -0.13836954]\n",
      "Action  0\n",
      "Observation  [-0.06919194  0.02357262  0.10582419  0.18650185] , reward  1.0 , done  False , info  {}\n",
      "[-0.06919194  0.02357262  0.10582419  0.18650185]\n",
      "Action  0\n",
      "Observation  [-0.06872049 -0.17289172  0.10955423  0.51060406] , reward  1.0 , done  False , info  {}\n",
      "[-0.06872049 -0.17289172  0.10955423  0.51060406]\n",
      "Action  1\n",
      "Observation  [-0.07217832  0.02053023  0.11976631  0.25435506] , reward  1.0 , done  False , info  {}\n",
      "[-0.07217832  0.02053023  0.11976631  0.25435506]\n",
      "Action  0\n",
      "Observation  [-0.07176772 -0.17608006  0.12485341  0.58228506] , reward  1.0 , done  False , info  {}\n",
      "[-0.07176772 -0.17608006  0.12485341  0.58228506]\n",
      "Action  1\n",
      "Observation  [-0.07528932  0.01709186  0.13649911  0.33139428] , reward  1.0 , done  False , info  {}\n",
      "[-0.07528932  0.01709186  0.13649911  0.33139428]\n",
      "Action  1\n",
      "Observation  [-0.07494748  0.21003361  0.143127    0.08467988] , reward  1.0 , done  False , info  {}\n",
      "[-0.07494748  0.21003361  0.143127    0.08467988]\n",
      "Action  0\n",
      "Observation  [-0.07074681  0.01318098  0.14482059  0.41887535] , reward  1.0 , done  False , info  {}\n",
      "[-0.07074681  0.01318098  0.14482059  0.41887535]\n",
      "Action  1\n",
      "Observation  [-0.07048319  0.2059858   0.1531981   0.17512419] , reward  1.0 , done  False , info  {}\n",
      "[-0.07048319  0.2059858   0.1531981   0.17512419]\n",
      "Action  1\n",
      "Observation  [-0.06636348  0.39862121  0.15670058 -0.06558048] , reward  1.0 , done  False , info  {}\n",
      "[-0.06636348  0.39862121  0.15670058 -0.06558048]\n",
      "Action  0\n",
      "Observation  [-0.05839105  0.20164024  0.15538897  0.27215239] , reward  1.0 , done  False , info  {}\n",
      "[-0.05839105  0.20164024  0.15538897  0.27215239]\n",
      "Action  0\n",
      "Observation  [-0.05435825  0.00468185  0.16083202  0.6095311 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.05435825  0.00468185  0.16083202  0.6095311 ]\n",
      "Action  1\n",
      "Observation  [-0.05426461  0.19723354  0.17302264  0.37151209] , reward  1.0 , done  False , info  {}\n",
      "[-0.05426461  0.19723354  0.17302264  0.37151209]\n",
      "Action  0\n",
      "Observation  [-5.03199395e-02  1.30067675e-04  1.80452886e-01  7.13368071e-01] , reward  1.0 , done  False , info  {}\n",
      "[-5.03199395e-02  1.30067675e-04  1.80452886e-01  7.13368071e-01]\n",
      "Action  0\n",
      "Observation  [-0.05031734 -0.19697037  0.19472025  1.05698379] , reward  1.0 , done  False , info  {}\n",
      "[-0.05031734 -0.19697037  0.19472025  1.05698379]\n",
      "Action  0\n",
      "Observation  [-0.05425675 -0.39406401  0.21585992  1.40392386] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 30 timesteps\n",
      "[-0.04052862 -0.02997247  0.03185125  0.04062383]\n",
      "Action  0\n",
      "Observation  [-0.04112807 -0.22553634  0.03266372  0.34318353] , reward  1.0 , done  False , info  {}\n",
      "[-0.04112807 -0.22553634  0.03266372  0.34318353]\n",
      "Action  0\n",
      "Observation  [-0.0456388  -0.42110739  0.03952739  0.64598505] , reward  1.0 , done  False , info  {}\n",
      "[-0.0456388  -0.42110739  0.03952739  0.64598505]\n",
      "Action  0\n",
      "Observation  [-0.05406094 -0.61675719  0.0524471   0.95084854] , reward  1.0 , done  False , info  {}\n",
      "[-0.05406094 -0.61675719  0.0524471   0.95084854]\n",
      "Action  1\n",
      "Observation  [-0.06639609 -0.42237887  0.07146407  0.67509436] , reward  1.0 , done  False , info  {}\n",
      "[-0.06639609 -0.42237887  0.07146407  0.67509436]\n",
      "Action  0\n",
      "Observation  [-0.07484367 -0.61841736  0.08496595  0.98939407] , reward  1.0 , done  False , info  {}\n",
      "[-0.07484367 -0.61841736  0.08496595  0.98939407]\n",
      "Action  1\n",
      "Observation  [-0.08721201 -0.42452921  0.10475384  0.72456096] , reward  1.0 , done  False , info  {}\n",
      "[-0.08721201 -0.42452921  0.10475384  0.72456096]\n",
      "Action  1\n",
      "Observation  [-0.0957026  -0.23099989  0.11924505  0.46659959] , reward  1.0 , done  False , info  {}\n",
      "[-0.0957026  -0.23099989  0.11924505  0.46659959]\n",
      "Action  1\n",
      "Observation  [-0.1003226  -0.03774677  0.12857705  0.21375345] , reward  1.0 , done  False , info  {}\n",
      "[-0.1003226  -0.03774677  0.12857705  0.21375345]\n",
      "Action  0\n",
      "Observation  [-0.10107753 -0.23445009  0.13285212  0.54407045] , reward  1.0 , done  False , info  {}\n",
      "[-0.10107753 -0.23445009  0.13285212  0.54407045]\n",
      "Action  0\n",
      "Observation  [-0.10576653 -0.43116419  0.14373352  0.87548519] , reward  1.0 , done  False , info  {}\n",
      "[-0.10576653 -0.43116419  0.14373352  0.87548519]\n",
      "Action  1\n",
      "Observation  [-0.11438982 -0.23825781  0.16124323  0.63122177] , reward  1.0 , done  False , info  {}\n",
      "[-0.11438982 -0.23825781  0.16124323  0.63122177]\n",
      "Action  1\n",
      "Observation  [-0.11915497 -0.04570918  0.17386766  0.39334566] , reward  1.0 , done  False , info  {}\n",
      "[-0.11915497 -0.04570918  0.17386766  0.39334566]\n",
      "Action  1\n",
      "Observation  [-0.12006916  0.14657426  0.18173458  0.160129  ] , reward  1.0 , done  False , info  {}\n",
      "[-0.12006916  0.14657426  0.18173458  0.160129  ]\n",
      "Action  1\n",
      "Observation  [-0.11713767  0.33869221  0.18493716 -0.07016579] , reward  1.0 , done  False , info  {}\n",
      "[-0.11713767  0.33869221  0.18493716 -0.07016579]\n",
      "Action  0\n",
      "Observation  [-0.11036383  0.14146691  0.18353384  0.27468961] , reward  1.0 , done  False , info  {}\n",
      "[-0.11036383  0.14146691  0.18353384  0.27468961]\n",
      "Action  1\n",
      "Observation  [-0.10753449  0.33356069  0.18902763  0.04504481] , reward  1.0 , done  False , info  {}\n",
      "[-0.10753449  0.33356069  0.18902763  0.04504481]\n",
      "Action  0\n",
      "Observation  [-0.10086327  0.13630192  0.18992853  0.3909062 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.10086327  0.13630192  0.18992853  0.3909062 ]\n",
      "Action  1\n",
      "Observation  [-0.09813724  0.3282925   0.19774665  0.16360284] , reward  1.0 , done  False , info  {}\n",
      "[-0.09813724  0.3282925   0.19774665  0.16360284]\n",
      "Action  0\n",
      "Observation  [-0.09157139  0.13097033  0.20101871  0.51157726] , reward  1.0 , done  False , info  {}\n",
      "[-0.09157139  0.13097033  0.20101871  0.51157726]\n",
      "Action  0\n",
      "Observation  [-0.08895198 -0.06633096  0.21125026  0.8602721 ] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 20 timesteps\n",
      "[ 0.0195608   0.00708341 -0.02874904 -0.0451542 ]\n",
      "Action  1\n",
      "Observation  [ 0.01970246  0.20260556 -0.02965213 -0.34676728] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01970246  0.20260556 -0.02965213 -0.34676728]\n",
      "Action  1\n",
      "Observation  [ 0.02375457  0.39813646 -0.03658747 -0.64865115] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02375457  0.39813646 -0.03658747 -0.64865115]\n",
      "Action  1\n",
      "Observation  [ 0.0317173   0.59374849 -0.0495605  -0.95262715] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0317173   0.59374849 -0.0495605  -0.95262715]\n",
      "Action  1\n",
      "Observation  [ 0.04359227  0.78950104 -0.06861304 -1.26046025] , reward  1.0 , done  False , info  {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04359227  0.78950104 -0.06861304 -1.26046025]\n",
      "Action  0\n",
      "Observation  [ 0.05938229  0.59532053 -0.09382225 -0.99003125] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05938229  0.59532053 -0.09382225 -0.99003125]\n",
      "Action  0\n",
      "Observation  [ 0.07128871  0.40157108 -0.11362287 -0.72822856] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07128871  0.40157108 -0.11362287 -0.72822856]\n",
      "Action  0\n",
      "Observation  [ 0.07932013  0.2081879  -0.12818744 -0.47335751] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07932013  0.2081879  -0.12818744 -0.47335751]\n",
      "Action  0\n",
      "Observation  [ 0.08348388  0.01508723 -0.13765459 -0.22366701] , reward  1.0 , done  False , info  {}\n",
      "[ 0.08348388  0.01508723 -0.13765459 -0.22366701]\n",
      "Action  0\n",
      "Observation  [ 0.08378563 -0.17782631 -0.14212793  0.02262326] , reward  1.0 , done  False , info  {}\n",
      "[ 0.08378563 -0.17782631 -0.14212793  0.02262326]\n",
      "Action  1\n",
      "Observation  [ 0.0802291   0.01901756 -0.14167547 -0.3113104 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0802291   0.01901756 -0.14167547 -0.3113104 ]\n",
      "Action  0\n",
      "Observation  [ 0.08060945 -0.17383165 -0.14790167 -0.06644827] , reward  1.0 , done  False , info  {}\n",
      "[ 0.08060945 -0.17383165 -0.14790167 -0.06644827]\n",
      "Action  1\n",
      "Observation  [ 0.07713282  0.02306713 -0.14923064 -0.4018967 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07713282  0.02306713 -0.14923064 -0.4018967 ]\n",
      "Action  0\n",
      "Observation  [ 0.07759416 -0.16965797 -0.15726857 -0.15973318] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07759416 -0.16965797 -0.15726857 -0.15973318]\n",
      "Action  0\n",
      "Observation  [ 0.074201   -0.36221974 -0.16046324  0.0794982 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.074201   -0.36221974 -0.16046324  0.0794982 ]\n",
      "Action  1\n",
      "Observation  [ 0.06695661 -0.1652048  -0.15887327 -0.25920173] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06695661 -0.1652048  -0.15887327 -0.25920173]\n",
      "Action  0\n",
      "Observation  [ 0.06365251 -0.35774418 -0.16405731 -0.02054238] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06365251 -0.35774418 -0.16405731 -0.02054238]\n",
      "Action  0\n",
      "Observation  [ 0.05649763 -0.55017978 -0.16446816  0.21621842] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05649763 -0.55017978 -0.16446816  0.21621842]\n",
      "Action  1\n",
      "Observation  [ 0.04549403 -0.35313538 -0.16014379 -0.12349561] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04549403 -0.35313538 -0.16014379 -0.12349561]\n",
      "Action  0\n",
      "Observation  [ 0.03843133 -0.54564391 -0.1626137   0.11469098] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03843133 -0.54564391 -0.1626137   0.11469098]\n",
      "Action  1\n",
      "Observation  [ 0.02751845 -0.34861069 -0.16031988 -0.2245578 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02751845 -0.34861069 -0.16031988 -0.2245578 ]\n",
      "Action  0\n",
      "Observation  [ 0.02054623 -0.54112151 -0.16481104  0.01357298] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02054623 -0.54112151 -0.16481104  0.01357298]\n",
      "Action  1\n",
      "Observation  [ 0.0097238  -0.34406665 -0.16453958 -0.32623938] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0097238  -0.34406665 -0.16453958 -0.32623938]\n",
      "Action  0\n",
      "Observation  [ 0.00284247 -0.53651043 -0.17106436 -0.08962911] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00284247 -0.53651043 -0.17106436 -0.08962911]\n",
      "Action  0\n",
      "Observation  [-0.00788774 -0.72882021 -0.17285695  0.14457717] , reward  1.0 , done  False , info  {}\n",
      "[-0.00788774 -0.72882021 -0.17285695  0.14457717]\n",
      "Action  1\n",
      "Observation  [-0.02246414 -0.53169853 -0.1699654  -0.19726616] , reward  1.0 , done  False , info  {}\n",
      "[-0.02246414 -0.53169853 -0.1699654  -0.19726616]\n",
      "Action  1\n",
      "Observation  [-0.03309811 -0.33460473 -0.17391073 -0.53837643] , reward  1.0 , done  False , info  {}\n",
      "[-0.03309811 -0.33460473 -0.17391073 -0.53837643]\n",
      "Action  0\n",
      "Observation  [-0.03979021 -0.52691016 -0.18467825 -0.30514192] , reward  1.0 , done  False , info  {}\n",
      "[-0.03979021 -0.52691016 -0.18467825 -0.30514192]\n",
      "Action  0\n",
      "Observation  [-0.05032841 -0.71898614 -0.19078109 -0.07591452] , reward  1.0 , done  False , info  {}\n",
      "[-0.05032841 -0.71898614 -0.19078109 -0.07591452]\n",
      "Action  0\n",
      "Observation  [-0.06470813 -0.91093395 -0.19229938  0.15103327] , reward  1.0 , done  False , info  {}\n",
      "[-0.06470813 -0.91093395 -0.19229938  0.15103327]\n",
      "Action  0\n",
      "Observation  [-0.08292681 -1.10285686 -0.18927872  0.37742294] , reward  1.0 , done  False , info  {}\n",
      "[-0.08292681 -1.10285686 -0.18927872  0.37742294]\n",
      "Action  0\n",
      "Observation  [-0.10498395 -1.29485746 -0.18173026  0.60496394] , reward  1.0 , done  False , info  {}\n",
      "[-0.10498395 -1.29485746 -0.18173026  0.60496394]\n",
      "Action  1\n",
      "Observation  [-0.1308811  -1.09772193 -0.16963098  0.26099506] , reward  1.0 , done  False , info  {}\n",
      "[-0.1308811  -1.09772193 -0.16963098  0.26099506]\n",
      "Action  1\n",
      "Observation  [-0.15283554 -0.90063583 -0.16441108 -0.08002363] , reward  1.0 , done  False , info  {}\n",
      "[-0.15283554 -0.90063583 -0.16441108 -0.08002363]\n",
      "Action  0\n",
      "Observation  [-0.17084825 -1.09306599 -0.16601155  0.15660981] , reward  1.0 , done  False , info  {}\n",
      "[-0.17084825 -1.09306599 -0.16601155  0.15660981]\n",
      "Action  1\n",
      "Observation  [-0.19270957 -0.8960044  -0.16287936 -0.18350221] , reward  1.0 , done  False , info  {}\n",
      "[-0.19270957 -0.8960044  -0.16287936 -0.18350221]\n",
      "Action  1\n",
      "Observation  [-0.21062966 -0.69897205 -0.1665494  -0.52281406] , reward  1.0 , done  False , info  {}\n",
      "[-0.21062966 -0.69897205 -0.1665494  -0.52281406]\n",
      "Action  0\n",
      "Observation  [-0.2246091  -0.89140659 -0.17700568 -0.28689587] , reward  1.0 , done  False , info  {}\n",
      "[-0.2246091  -0.89140659 -0.17700568 -0.28689587]\n",
      "Action  0\n",
      "Observation  [-0.24243723 -1.08362078 -0.1827436  -0.05484786] , reward  1.0 , done  False , info  {}\n",
      "[-0.24243723 -1.08362078 -0.1827436  -0.05484786]\n",
      "Action  0\n",
      "Observation  [-0.26410965 -1.27571622 -0.18384056  0.17506929] , reward  1.0 , done  False , info  {}\n",
      "[-0.26410965 -1.27571622 -0.18384056  0.17506929]\n",
      "Action  1\n",
      "Observation  [-0.28962397 -1.07850443 -0.18033917 -0.16950872] , reward  1.0 , done  False , info  {}\n",
      "[-0.28962397 -1.07850443 -0.18033917 -0.16950872]\n",
      "Action  0\n",
      "Observation  [-0.31119406 -1.27064849 -0.18372934  0.06130055] , reward  1.0 , done  False , info  {}\n",
      "[-0.31119406 -1.27064849 -0.18372934  0.06130055]\n",
      "Action  1\n",
      "Observation  [-0.33660703 -1.07343282 -0.18250333 -0.28325704] , reward  1.0 , done  False , info  {}\n",
      "[-0.33660703 -1.07343282 -0.18250333 -0.28325704]\n",
      "Action  1\n",
      "Observation  [-0.35807569 -0.87624072 -0.18816847 -0.6274915 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.35807569 -0.87624072 -0.18816847 -0.6274915 ]\n",
      "Action  1\n",
      "Observation  [-0.3756005  -0.67906012 -0.2007183  -0.97303724] , reward  1.0 , done  False , info  {}\n",
      "[-0.3756005  -0.67906012 -0.2007183  -0.97303724]\n",
      "Action  1\n",
      "Observation  [-0.38918171 -0.48189389 -0.22017905 -1.32146475] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 45 timesteps\n",
      "[-0.04178295  0.04324643 -0.0221574  -0.01006935]\n",
      "Action  1\n",
      "Observation  [-0.04091802  0.23867902 -0.02235878 -0.30966002] , reward  1.0 , done  False , info  {}\n",
      "[-0.04091802  0.23867902 -0.02235878 -0.30966002]\n",
      "Action  1\n",
      "Observation  [-0.03614444  0.43411229 -0.02855198 -0.60930958] , reward  1.0 , done  False , info  {}\n",
      "[-0.03614444  0.43411229 -0.02855198 -0.60930958]\n",
      "Action  1\n",
      "Observation  [-0.02746219  0.62962149 -0.04073818 -0.91084701] , reward  1.0 , done  False , info  {}\n",
      "[-0.02746219  0.62962149 -0.04073818 -0.91084701]\n",
      "Action  1\n",
      "Observation  [-0.01486976  0.82527033 -0.05895512 -1.21605049] , reward  1.0 , done  False , info  {}\n",
      "[-0.01486976  0.82527033 -0.05895512 -1.21605049]\n",
      "Action  1\n",
      "Observation  [ 0.00163565  1.02110108 -0.08327613 -1.52660903] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00163565  1.02110108 -0.08327613 -1.52660903]\n",
      "Action  0\n",
      "Observation  [ 0.02205767  0.82707701 -0.11380831 -1.2610364 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02205767  0.82707701 -0.11380831 -1.2610364 ]\n",
      "Action  1\n",
      "Observation  [ 0.03859921  1.02345559 -0.13902903 -1.58708611] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03859921  1.02345559 -0.13902903 -1.58708611]\n",
      "Action  1\n",
      "Observation  [ 0.05906832  1.21992918 -0.17077076 -1.91969583] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05906832  1.21992918 -0.17077076 -1.91969583]\n",
      "Action  1\n",
      "Observation  [ 0.0834669   1.41642666 -0.20916467 -2.26011763] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0834669   1.41642666 -0.20916467 -2.26011763]\n",
      "Action  1\n",
      "Observation  [ 0.11179544  1.61280727 -0.25436703 -2.60931528] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 10 timesteps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02538204 -0.03276309 -0.02769695  0.04865145]\n",
      "Action  0\n",
      "Observation  [ 0.02472678 -0.22747717 -0.02672392  0.33246869] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02472678 -0.22747717 -0.02672392  0.33246869]\n",
      "Action  0\n",
      "Observation  [ 0.02017724 -0.42220875 -0.02007454  0.61660587] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02017724 -0.42220875 -0.02007454  0.61660587]\n",
      "Action  0\n",
      "Observation  [ 0.01173306 -0.61704458 -0.00774243  0.90289921] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01173306 -0.61704458 -0.00774243  0.90289921]\n",
      "Action  0\n",
      "Observation  [-6.07827806e-04 -8.12060799e-01  1.03155582e-02  1.19313852e+00] , reward  1.0 , done  False , info  {}\n",
      "[-6.07827806e-04 -8.12060799e-01  1.03155582e-02  1.19313852e+00]\n",
      "Action  1\n",
      "Observation  [-0.01684904 -0.61707397  0.03417833  0.90370656] , reward  1.0 , done  False , info  {}\n",
      "[-0.01684904 -0.61707397  0.03417833  0.90370656]\n",
      "Action  1\n",
      "Observation  [-0.02919052 -0.42243121  0.05225246  0.6219594 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.02919052 -0.42243121  0.05225246  0.6219594 ]\n",
      "Action  1\n",
      "Observation  [-0.03763915 -0.22807635  0.06469165  0.34618025] , reward  1.0 , done  False , info  {}\n",
      "[-0.03763915 -0.22807635  0.06469165  0.34618025]\n",
      "Action  0\n",
      "Observation  [-0.04220067 -0.424056    0.07161525  0.65854089] , reward  1.0 , done  False , info  {}\n",
      "[-0.04220067 -0.424056    0.07161525  0.65854089]\n",
      "Action  1\n",
      "Observation  [-0.05068179 -0.23000004  0.08478607  0.38923996] , reward  1.0 , done  False , info  {}\n",
      "[-0.05068179 -0.23000004  0.08478607  0.38923996]\n",
      "Action  0\n",
      "Observation  [-0.0552818  -0.42621664  0.09257087  0.70740485] , reward  1.0 , done  False , info  {}\n",
      "[-0.0552818  -0.42621664  0.09257087  0.70740485]\n",
      "Action  0\n",
      "Observation  [-0.06380613 -0.62249078  0.10671897  1.02773247] , reward  1.0 , done  False , info  {}\n",
      "[-0.06380613 -0.62249078  0.10671897  1.02773247]\n",
      "Action  1\n",
      "Observation  [-0.07625594 -0.42893866  0.12727362  0.77037185] , reward  1.0 , done  False , info  {}\n",
      "[-0.07625594 -0.42893866  0.12727362  0.77037185]\n",
      "Action  0\n",
      "Observation  [-0.08483472 -0.62556097  0.14268105  1.10023728] , reward  1.0 , done  False , info  {}\n",
      "[-0.08483472 -0.62556097  0.14268105  1.10023728]\n",
      "Action  0\n",
      "Observation  [-0.09734594 -0.82224258  0.1646858   1.43406783] , reward  1.0 , done  False , info  {}\n",
      "[-0.09734594 -0.82224258  0.1646858   1.43406783]\n",
      "Action  1\n",
      "Observation  [-0.11379079 -0.62949013  0.19336716  1.19705015] , reward  1.0 , done  False , info  {}\n",
      "[-0.11379079 -0.62949013  0.19336716  1.19705015]\n",
      "Action  1\n",
      "Observation  [-0.12638059 -0.43732333  0.21730816  0.97066846] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 16 timesteps\n",
      "[ 0.00268777  0.00811407 -0.02499095  0.03985174]\n",
      "Action  0\n",
      "Observation  [ 0.00285005 -0.18664076 -0.02419392  0.3245462 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00285005 -0.18664076 -0.02419392  0.3245462 ]\n",
      "Action  0\n",
      "Observation  [-0.00088276 -0.38141002 -0.01770299  0.60950226] , reward  1.0 , done  False , info  {}\n",
      "[-0.00088276 -0.38141002 -0.01770299  0.60950226]\n",
      "Action  1\n",
      "Observation  [-0.00851096 -0.18604513 -0.00551295  0.31129644] , reward  1.0 , done  False , info  {}\n",
      "[-0.00851096 -0.18604513 -0.00551295  0.31129644]\n",
      "Action  0\n",
      "Observation  [-0.01223187 -0.3810881   0.00071298  0.60223566] , reward  1.0 , done  False , info  {}\n",
      "[-0.01223187 -0.3810881   0.00071298  0.60223566]\n",
      "Action  0\n",
      "Observation  [-0.01985363 -0.57622002  0.01275769  0.89514307] , reward  1.0 , done  False , info  {}\n",
      "[-0.01985363 -0.57622002  0.01275769  0.89514307]\n",
      "Action  0\n",
      "Observation  [-0.03137803 -0.77151262  0.03066056  1.19180879] , reward  1.0 , done  False , info  {}\n",
      "[-0.03137803 -0.77151262  0.03066056  1.19180879]\n",
      "Action  1\n",
      "Observation  [-0.04680828 -0.57680101  0.05449673  0.90889145] , reward  1.0 , done  False , info  {}\n",
      "[-0.04680828 -0.57680101  0.05449673  0.90889145]\n",
      "Action  0\n",
      "Observation  [-0.0583443  -0.77261657  0.07267456  1.21819284] , reward  1.0 , done  False , info  {}\n",
      "[-0.0583443  -0.77261657  0.07267456  1.21819284]\n",
      "Action  0\n",
      "Observation  [-0.07379663 -0.96859639  0.09703842  1.53273411] , reward  1.0 , done  False , info  {}\n",
      "[-0.07379663 -0.96859639  0.09703842  1.53273411]\n",
      "Action  0\n",
      "Observation  [-0.09316856 -1.16474451  0.1276931   1.85405665] , reward  1.0 , done  False , info  {}\n",
      "[-0.09316856 -1.16474451  0.1276931   1.85405665]\n",
      "Action  1\n",
      "Observation  [-0.11646345 -0.97123656  0.16477423  1.60359779] , reward  1.0 , done  False , info  {}\n",
      "[-0.11646345 -0.97123656  0.16477423  1.60359779]\n",
      "Action  0\n",
      "Observation  [-0.13588818 -1.16788042  0.19684619  1.9427931 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.13588818 -1.16788042  0.19684619  1.9427931 ]\n",
      "Action  1\n",
      "Observation  [-0.15924579 -0.9753279   0.23570205  1.71704185] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 13 timesteps\n",
      "[-0.04962348  0.0016931   0.04063171 -0.03226047]\n",
      "Action  0\n",
      "Observation  [-0.04958962 -0.19398726  0.0399865   0.27296025] , reward  1.0 , done  False , info  {}\n",
      "[-0.04958962 -0.19398726  0.0399865   0.27296025]\n",
      "Action  1\n",
      "Observation  [-0.05346937  0.000542    0.04544571 -0.00684749] , reward  1.0 , done  False , info  {}\n",
      "[-0.05346937  0.000542    0.04544571 -0.00684749]\n",
      "Action  1\n",
      "Observation  [-0.05345853  0.19498373  0.04530876 -0.2848525 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.05345853  0.19498373  0.04530876 -0.2848525 ]\n",
      "Action  1\n",
      "Observation  [-0.04955885  0.38943117  0.03961171 -0.56290812] , reward  1.0 , done  False , info  {}\n",
      "[-0.04955885  0.38943117  0.03961171 -0.56290812]\n",
      "Action  1\n",
      "Observation  [-0.04177023  0.58397554  0.02835355 -0.84285297] , reward  1.0 , done  False , info  {}\n",
      "[-0.04177023  0.58397554  0.02835355 -0.84285297]\n",
      "Action  0\n",
      "Observation  [-0.03009072  0.38847833  0.01149649 -0.54139018] , reward  1.0 , done  False , info  {}\n",
      "[-0.03009072  0.38847833  0.01149649 -0.54139018]\n",
      "Action  1\n",
      "Observation  [-2.23211509e-02  5.83436817e-01  6.68683596e-04 -8.30428698e-01] , reward  1.0 , done  False , info  {}\n",
      "[-2.23211509e-02  5.83436817e-01  6.68683596e-04 -8.30428698e-01]\n",
      "Action  0\n",
      "Observation  [-0.01065241  0.38830573 -0.01593989 -0.53753554] , reward  1.0 , done  False , info  {}\n",
      "[-0.01065241  0.38830573 -0.01593989 -0.53753554]\n",
      "Action  1\n",
      "Observation  [-0.0028863   0.58364812 -0.0266906  -0.83519803] , reward  1.0 , done  False , info  {}\n",
      "[-0.0028863   0.58364812 -0.0266906  -0.83519803]\n",
      "Action  1\n",
      "Observation  [ 0.00878666  0.77912432 -0.04339456 -1.13615401] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00878666  0.77912432 -0.04339456 -1.13615401]\n",
      "Action  0\n",
      "Observation  [ 0.02436915  0.58459609 -0.06611764 -0.85739034] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02436915  0.58459609 -0.06611764 -0.85739034]\n",
      "Action  0\n",
      "Observation  [ 0.03606107  0.39043423 -0.08326545 -0.58620834] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03606107  0.39043423 -0.08326545 -0.58620834]\n",
      "Action  0\n",
      "Observation  [ 0.04386976  0.19657115 -0.09498962 -0.32087297] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04386976  0.19657115 -0.09498962 -0.32087297]\n",
      "Action  1\n",
      "Observation  [ 0.04780118  0.39290845 -0.10140708 -0.64193621] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04780118  0.39290845 -0.10140708 -0.64193621]\n",
      "Action  1\n",
      "Observation  [ 0.05565935  0.58928682 -0.1142458  -0.9647531 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05565935  0.58928682 -0.1142458  -0.9647531 ]\n",
      "Action  0\n",
      "Observation  [ 0.06744508  0.3958695  -0.13354086 -0.71003369] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06744508  0.3958695  -0.13354086 -0.71003369]\n",
      "Action  1\n",
      "Observation  [ 0.07536247  0.59256331 -0.14774154 -1.04159198] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07536247  0.59256331 -0.14774154 -1.04159198]\n",
      "Action  1\n",
      "Observation  [ 0.08721374  0.78930564 -0.16857337 -1.37676869] , reward  1.0 , done  False , info  {}\n",
      "[ 0.08721374  0.78930564 -0.16857337 -1.37676869]\n",
      "Action  0\n",
      "Observation  [ 0.10299985  0.5966423  -0.19610875 -1.14119633] , reward  1.0 , done  False , info  {}\n",
      "[ 0.10299985  0.5966423  -0.19610875 -1.14119633]\n",
      "Action  0\n",
      "Observation  [ 0.1149327   0.40454737 -0.21893268 -0.9158641 ] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 20 timesteps\n",
      "[ 0.03548638  0.00145555 -0.02158727 -0.01223247]\n",
      "Action  0\n",
      "Observation  [ 0.03551549 -0.19335026 -0.02183192  0.273562  ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03551549 -0.19335026 -0.02183192  0.273562  ]\n",
      "Action  0\n",
      "Observation  [ 0.03164849 -0.38815401 -0.01636068  0.55927992] , reward  1.0 , done  False , info  {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03164849 -0.38815401 -0.01636068  0.55927992]\n",
      "Action  1\n",
      "Observation  [ 0.02388541 -0.19280629 -0.00517509  0.26148772] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02388541 -0.19280629 -0.00517509  0.26148772]\n",
      "Action  1\n",
      "Observation  [ 2.00292795e-02  2.38915349e-03  5.46692548e-05 -3.28229912e-02] , reward  1.0 , done  False , info  {}\n",
      "[ 2.00292795e-02  2.38915349e-03  5.46692548e-05 -3.28229912e-02]\n",
      "Action  0\n",
      "Observation  [ 0.02007706 -0.19273358 -0.00060179  0.25987718] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02007706 -0.19273358 -0.00060179  0.25987718]\n",
      "Action  1\n",
      "Observation  [ 0.01622239  0.00239696  0.00459575 -0.0329955 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01622239  0.00239696  0.00459575 -0.0329955 ]\n",
      "Action  1\n",
      "Observation  [ 0.01627033  0.1974527   0.00393584 -0.32422488] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01627033  0.1974527   0.00393584 -0.32422488]\n",
      "Action  1\n",
      "Observation  [ 0.02021938  0.39251839 -0.00254865 -0.61566401] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02021938  0.39251839 -0.00254865 -0.61566401]\n",
      "Action  1\n",
      "Observation  [ 0.02806975  0.58767586 -0.01486193 -0.90914856] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02806975  0.58767586 -0.01486193 -0.90914856]\n",
      "Action  1\n",
      "Observation  [ 0.03982327  0.78299578 -0.03304491 -1.20646534] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03982327  0.78299578 -0.03304491 -1.20646534]\n",
      "Action  1\n",
      "Observation  [ 0.05548318  0.97852876 -0.05717421 -1.50931813] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05548318  0.97852876 -0.05717421 -1.50931813]\n",
      "Action  0\n",
      "Observation  [ 0.07505376  0.78414443 -0.08736057 -1.23501813] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07505376  0.78414443 -0.08736057 -1.23501813]\n",
      "Action  1\n",
      "Observation  [ 0.09073665  0.98027382 -0.11206094 -1.55374165] , reward  1.0 , done  False , info  {}\n",
      "[ 0.09073665  0.98027382 -0.11206094 -1.55374165]\n",
      "Action  1\n",
      "Observation  [ 0.11034212  1.1765465  -0.14313577 -1.87918106] , reward  1.0 , done  False , info  {}\n",
      "[ 0.11034212  1.1765465  -0.14313577 -1.87918106]\n",
      "Action  1\n",
      "Observation  [ 0.13387305  1.37290949 -0.18071939 -2.21265177] , reward  1.0 , done  False , info  {}\n",
      "[ 0.13387305  1.37290949 -0.18071939 -2.21265177]\n",
      "Action  0\n",
      "Observation  [ 0.16133124  1.17992094 -0.22497243 -1.98072605] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 16 timesteps\n",
      "[ 0.03801715  0.02516524 -0.00532398  0.00775957]\n",
      "Action  1\n",
      "Observation  [ 0.03852045  0.22036314 -0.00516878 -0.28659837] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03852045  0.22036314 -0.00516878 -0.28659837]\n",
      "Action  1\n",
      "Observation  [ 0.04292772  0.41555842 -0.01090075 -0.580907  ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04292772  0.41555842 -0.01090075 -0.580907  ]\n",
      "Action  1\n",
      "Observation  [ 0.05123888  0.61083141 -0.02251889 -0.87700383] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05123888  0.61083141 -0.02251889 -0.87700383]\n",
      "Action  1\n",
      "Observation  [ 0.06345551  0.80625206 -0.04005897 -1.17668048] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06345551  0.80625206 -0.04005897 -1.17668048]\n",
      "Action  1\n",
      "Observation  [ 0.07958055  1.00187084 -0.06359258 -1.48164744] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07958055  1.00187084 -0.06359258 -1.48164744]\n",
      "Action  0\n",
      "Observation  [ 0.09961797  0.80757975 -0.09322553 -1.20948351] , reward  1.0 , done  False , info  {}\n",
      "[ 0.09961797  0.80757975 -0.09322553 -1.20948351]\n",
      "Action  1\n",
      "Observation  [ 0.11576956  1.0037737  -0.1174152  -1.52986514] , reward  1.0 , done  False , info  {}\n",
      "[ 0.11576956  1.0037737  -0.1174152  -1.52986514]\n",
      "Action  1\n",
      "Observation  [ 0.13584504  1.2000995  -0.1480125  -1.85676701] , reward  1.0 , done  False , info  {}\n",
      "[ 0.13584504  1.2000995  -0.1480125  -1.85676701]\n",
      "Action  0\n",
      "Observation  [ 0.15984703  1.00688085 -0.18514784 -1.61346494] , reward  1.0 , done  False , info  {}\n",
      "[ 0.15984703  1.00688085 -0.18514784 -1.61346494]\n",
      "Action  0\n",
      "Observation  [ 0.17998465  0.81436376 -0.21741714 -1.38374776] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 10 timesteps\n",
      "[ 0.01906859 -0.00436457 -0.02568656  0.04466401]\n",
      "Action  1\n",
      "Observation  [ 0.0189813   0.19111611 -0.02479328 -0.25601131] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0189813   0.19111611 -0.02479328 -0.25601131]\n",
      "Action  1\n",
      "Observation  [ 0.02280362  0.38658312 -0.02991351 -0.55641018] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02280362  0.38658312 -0.02991351 -0.55641018]\n",
      "Action  1\n",
      "Observation  [ 0.03053529  0.58211198 -0.04104171 -0.85836553] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03053529  0.58211198 -0.04104171 -0.85836553]\n",
      "Action  0\n",
      "Observation  [ 0.04217753  0.38757244 -0.05820902 -0.57886482] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04217753  0.38757244 -0.05820902 -0.57886482]\n",
      "Action  1\n",
      "Observation  [ 0.04992897  0.58345978 -0.06978632 -0.88930197] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04992897  0.58345978 -0.06978632 -0.88930197]\n",
      "Action  0\n",
      "Observation  [ 0.06159817  0.38935068 -0.08757236 -0.61934756] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06159817  0.38935068 -0.08757236 -0.61934756]\n",
      "Action  1\n",
      "Observation  [ 0.06938518  0.58557957 -0.09995931 -0.93827635] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06938518  0.58557957 -0.09995931 -0.93827635]\n",
      "Action  0\n",
      "Observation  [ 0.08109678  0.39193705 -0.11872484 -0.67860161] , reward  1.0 , done  False , info  {}\n",
      "[ 0.08109678  0.39193705 -0.11872484 -0.67860161]\n",
      "Action  0\n",
      "Observation  [ 0.08893552  0.19864704 -0.13229687 -0.42553076] , reward  1.0 , done  False , info  {}\n",
      "[ 0.08893552  0.19864704 -0.13229687 -0.42553076]\n",
      "Action  0\n",
      "Observation  [ 0.09290846  0.00562281 -0.14080748 -0.17730644] , reward  1.0 , done  False , info  {}\n",
      "[ 0.09290846  0.00562281 -0.14080748 -0.17730644]\n",
      "Action  1\n",
      "Observation  [ 0.09302091  0.20244959 -0.14435361 -0.51088537] , reward  1.0 , done  False , info  {}\n",
      "[ 0.09302091  0.20244959 -0.14435361 -0.51088537]\n",
      "Action  1\n",
      "Observation  [ 0.0970699   0.39927849 -0.15457132 -0.84535063] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0970699   0.39927849 -0.15457132 -0.84535063]\n",
      "Action  1\n",
      "Observation  [ 0.10505547  0.59613333 -0.17147833 -1.18237564] , reward  1.0 , done  False , info  {}\n",
      "[ 0.10505547  0.59613333 -0.17147833 -1.18237564]\n",
      "Action  1\n",
      "Observation  [ 0.11697814  0.7930144  -0.19512585 -1.52353386] , reward  1.0 , done  False , info  {}\n",
      "[ 0.11697814  0.7930144  -0.19512585 -1.52353386]\n",
      "Action  1\n",
      "Observation  [ 0.13283843  0.98988364 -0.22559652 -1.87023748] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 15 timesteps\n",
      "[ 0.04841743 -0.01833749 -0.02726226  0.02543157]\n",
      "Action  0\n",
      "Observation  [ 0.04805068 -0.21305808 -0.02675363  0.30938981] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04805068 -0.21305808 -0.02675363  0.30938981]\n",
      "Action  1\n",
      "Observation  [ 0.04378952 -0.01756536 -0.02056583  0.00839104] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04378952 -0.01756536 -0.02056583  0.00839104]\n",
      "Action  1\n",
      "Observation  [ 0.04343821  0.1778454  -0.02039801 -0.29070905] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04343821  0.1778454  -0.02039801 -0.29070905]\n",
      "Action  1\n",
      "Observation  [ 0.04699512  0.37325218 -0.02621219 -0.58975483] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04699512  0.37325218 -0.02621219 -0.58975483]\n",
      "Action  0\n",
      "Observation  [ 0.05446016  0.17850687 -0.03800729 -0.30544272] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05446016  0.17850687 -0.03800729 -0.30544272]\n",
      "Action  1\n",
      "Observation  [ 0.0580303   0.37414925 -0.04411614 -0.60986581] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0580303   0.37414925 -0.04411614 -0.60986581]\n",
      "Action  1\n",
      "Observation  [ 0.06551329  0.56985922 -0.05631346 -0.91611107] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06551329  0.56985922 -0.05631346 -0.91611107]\n",
      "Action  1\n",
      "Observation  [ 0.07691047  0.7656956  -0.07463568 -1.22594739] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07691047  0.7656956  -0.07463568 -1.22594739]\n",
      "Action  0\n",
      "Observation  [ 0.09222438  0.57160969 -0.09915463 -0.95755154] , reward  1.0 , done  False , info  {}\n",
      "[ 0.09222438  0.57160969 -0.09915463 -0.95755154]\n",
      "Action  1\n",
      "Observation  [ 0.10365658  0.76791505 -0.11830566 -1.27966698] , reward  1.0 , done  False , info  {}\n",
      "[ 0.10365658  0.76791505 -0.11830566 -1.27966698]\n",
      "Action  1\n",
      "Observation  [ 0.11901488  0.96432911 -0.143899   -1.60692947] , reward  1.0 , done  False , info  {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11901488  0.96432911 -0.143899   -1.60692947]\n",
      "Action  0\n",
      "Observation  [ 0.13830146  0.77117189 -0.17603759 -1.36234868] , reward  1.0 , done  False , info  {}\n",
      "[ 0.13830146  0.77117189 -0.17603759 -1.36234868]\n",
      "Action  1\n",
      "Observation  [ 0.1537249   0.9680078  -0.20328456 -1.70452765] , reward  1.0 , done  False , info  {}\n",
      "[ 0.1537249   0.9680078  -0.20328456 -1.70452765]\n",
      "Action  0\n",
      "Observation  [ 0.17308505  0.77572278 -0.23737511 -1.48139409] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 14 timesteps\n",
      "[-0.03534474 -0.03801097 -0.00149005  0.04285437]\n",
      "Action  1\n",
      "Observation  [-0.03610496  0.15713232 -0.00063297 -0.25029831] , reward  1.0 , done  False , info  {}\n",
      "[-0.03610496  0.15713232 -0.00063297 -0.25029831]\n",
      "Action  1\n",
      "Observation  [-0.03296231  0.3522633  -0.00563893 -0.54318082] , reward  1.0 , done  False , info  {}\n",
      "[-0.03296231  0.3522633  -0.00563893 -0.54318082]\n",
      "Action  0\n",
      "Observation  [-0.02591705  0.15722105 -0.01650255 -0.25227994] , reward  1.0 , done  False , info  {}\n",
      "[-0.02591705  0.15722105 -0.01650255 -0.25227994]\n",
      "Action  0\n",
      "Observation  [-0.02277263 -0.03766141 -0.02154815  0.03515243] , reward  1.0 , done  False , info  {}\n",
      "[-0.02277263 -0.03766141 -0.02154815  0.03515243]\n",
      "Action  1\n",
      "Observation  [-0.02352585  0.15776281 -0.0208451  -0.26425052] , reward  1.0 , done  False , info  {}\n",
      "[-0.02352585  0.15776281 -0.0208451  -0.26425052]\n",
      "Action  1\n",
      "Observation  [-0.0203706   0.35317599 -0.02613011 -0.56343463] , reward  1.0 , done  False , info  {}\n",
      "[-0.0203706   0.35317599 -0.02613011 -0.56343463]\n",
      "Action  1\n",
      "Observation  [-0.01330708  0.54865466 -0.0373988  -0.86423391] , reward  1.0 , done  False , info  {}\n",
      "[-0.01330708  0.54865466 -0.0373988  -0.86423391]\n",
      "Action  1\n",
      "Observation  [-0.00233399  0.74426521 -0.05468348 -1.16843725] , reward  1.0 , done  False , info  {}\n",
      "[-0.00233399  0.74426521 -0.05468348 -1.16843725]\n",
      "Action  1\n",
      "Observation  [ 0.01255132  0.94005425 -0.07805223 -1.47775074] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01255132  0.94005425 -0.07805223 -1.47775074]\n",
      "Action  0\n",
      "Observation  [ 0.0313524   0.74596736 -0.10760724 -1.21043083] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0313524   0.74596736 -0.10760724 -1.21043083]\n",
      "Action  1\n",
      "Observation  [ 0.04627175  0.94230144 -0.13181586 -1.53480405] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04627175  0.94230144 -0.13181586 -1.53480405]\n",
      "Action  0\n",
      "Observation  [ 0.06511778  0.7489904  -0.16251194 -1.28599471] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06511778  0.7489904  -0.16251194 -1.28599471]\n",
      "Action  1\n",
      "Observation  [ 0.08009759  0.94576422 -0.18823183 -1.62483487] , reward  1.0 , done  False , info  {}\n",
      "[ 0.08009759  0.94576422 -0.18823183 -1.62483487]\n",
      "Action  1\n",
      "Observation  [ 0.09901287  1.14253626 -0.22072853 -1.9697934 ] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 14 timesteps\n",
      "[-0.02196376 -0.00210672  0.04871006 -0.04814991]\n",
      "Action  1\n",
      "Observation  [-0.0220059   0.19228415  0.04774707 -0.32507527] , reward  1.0 , done  False , info  {}\n",
      "[-0.0220059   0.19228415  0.04774707 -0.32507527]\n",
      "Action  0\n",
      "Observation  [-0.01816021 -0.00348397  0.04124556 -0.01772546] , reward  1.0 , done  False , info  {}\n",
      "[-0.01816021 -0.00348397  0.04124556 -0.01772546]\n",
      "Action  1\n",
      "Observation  [-0.01822989  0.19102294  0.04089105 -0.29711494] , reward  1.0 , done  False , info  {}\n",
      "[-0.01822989  0.19102294  0.04089105 -0.29711494]\n",
      "Action  1\n",
      "Observation  [-0.01440943  0.38553885  0.03494875 -0.57662627] , reward  1.0 , done  False , info  {}\n",
      "[-0.01440943  0.38553885  0.03494875 -0.57662627]\n",
      "Action  0\n",
      "Observation  [-0.00669866  0.1899449   0.02341623 -0.27314166] , reward  1.0 , done  False , info  {}\n",
      "[-0.00669866  0.1899449   0.02341623 -0.27314166]\n",
      "Action  0\n",
      "Observation  [-0.00289976 -0.00550321  0.01795339  0.02683387] , reward  1.0 , done  False , info  {}\n",
      "[-0.00289976 -0.00550321  0.01795339  0.02683387]\n",
      "Action  1\n",
      "Observation  [-0.00300982  0.18935674  0.01849007 -0.26013093] , reward  1.0 , done  False , info  {}\n",
      "[-0.00300982  0.18935674  0.01849007 -0.26013093]\n",
      "Action  1\n",
      "Observation  [ 0.00077731  0.38420992  0.01328745 -0.54692497] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00077731  0.38420992  0.01328745 -0.54692497]\n",
      "Action  1\n",
      "Observation  [ 0.00846151  0.57914269  0.00234895 -0.83539192] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00846151  0.57914269  0.00234895 -0.83539192]\n",
      "Action  1\n",
      "Observation  [ 0.02004436  0.77423248 -0.01435889 -1.1273352 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02004436  0.77423248 -0.01435889 -1.1273352 ]\n",
      "Action  0\n",
      "Observation  [ 0.03552901  0.57930156 -0.03690559 -0.83919034] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03552901  0.57930156 -0.03690559 -0.83919034]\n",
      "Action  0\n",
      "Observation  [ 0.04711504  0.38470244 -0.0536894  -0.55833821] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04711504  0.38470244 -0.0536894  -0.55833821]\n",
      "Action  1\n",
      "Observation  [ 0.05480909  0.58053531 -0.06485616 -0.86744134] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05480909  0.58053531 -0.06485616 -0.86744134]\n",
      "Action  0\n",
      "Observation  [ 0.0664198   0.38635299 -0.08220499 -0.59583459] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0664198   0.38635299 -0.08220499 -0.59583459]\n",
      "Action  0\n",
      "Observation  [ 0.07414686  0.19247189 -0.09412168 -0.33013607] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07414686  0.19247189 -0.09412168 -0.33013607]\n",
      "Action  0\n",
      "Observation  [ 0.0779963  -0.00119301 -0.1007244  -0.06855545] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0779963  -0.00119301 -0.1007244  -0.06855545]\n",
      "Action  0\n",
      "Observation  [ 0.07797244 -0.19473744 -0.10209551  0.19072683] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07797244 -0.19473744 -0.10209551  0.19072683]\n",
      "Action  0\n",
      "Observation  [ 0.07407769 -0.38826188 -0.09828097  0.44953793] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07407769 -0.38826188 -0.09828097  0.44953793]\n",
      "Action  1\n",
      "Observation  [ 0.06631245 -0.19189717 -0.08929021  0.12756416] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06631245 -0.19189717 -0.08929021  0.12756416]\n",
      "Action  1\n",
      "Observation  [ 0.06247451  0.00438292 -0.08673893 -0.19189955] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06247451  0.00438292 -0.08673893 -0.19189955]\n",
      "Action  0\n",
      "Observation  [ 0.06256216 -0.189398   -0.09057692  0.07220978] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06256216 -0.189398   -0.09057692  0.07220978]\n",
      "Action  1\n",
      "Observation  [ 0.0587742   0.00689789 -0.08913273 -0.24762026] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0587742   0.00689789 -0.08913273 -0.24762026]\n",
      "Action  0\n",
      "Observation  [ 0.05891216 -0.18684553 -0.09408513  0.01567087] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05891216 -0.18684553 -0.09408513  0.01567087]\n",
      "Action  0\n",
      "Observation  [ 0.05517525 -0.38050107 -0.09377171  0.27724922] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05517525 -0.38050107 -0.09377171  0.27724922]\n",
      "Action  0\n",
      "Observation  [ 0.04756523 -0.57416885 -0.08822673  0.53894611] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04756523 -0.57416885 -0.08822673  0.53894611]\n",
      "Action  0\n",
      "Observation  [ 0.03608185 -0.76794694 -0.07744781  0.80257769] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03608185 -0.76794694 -0.07744781  0.80257769]\n",
      "Action  0\n",
      "Observation  [ 0.02072291 -0.96192627 -0.06139625  1.06992758] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02072291 -0.96192627 -0.06139625  1.06992758]\n",
      "Action  1\n",
      "Observation  [ 0.00148439 -0.7660485  -0.0399977   0.75862537] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00148439 -0.7660485  -0.0399977   0.75862537]\n",
      "Action  1\n",
      "Observation  [-0.01383658 -0.57039888 -0.0248252   0.45362946] , reward  1.0 , done  False , info  {}\n",
      "[-0.01383658 -0.57039888 -0.0248252   0.45362946]\n",
      "Action  1\n",
      "Observation  [-0.02524456 -0.37493484 -0.01575261  0.15322589] , reward  1.0 , done  False , info  {}\n",
      "[-0.02524456 -0.37493484 -0.01575261  0.15322589]\n",
      "Action  1\n",
      "Observation  [-0.03274326 -0.17959092 -0.01268809 -0.14438471] , reward  1.0 , done  False , info  {}\n",
      "[-0.03274326 -0.17959092 -0.01268809 -0.14438471]\n",
      "Action  1\n",
      "Observation  [-0.03633507  0.01571042 -0.01557578 -0.44104334] , reward  1.0 , done  False , info  {}\n",
      "[-0.03633507  0.01571042 -0.01557578 -0.44104334]\n",
      "Action  1\n",
      "Observation  [-0.03602086  0.21104929 -0.02439665 -0.7385952 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.03602086  0.21104929 -0.02439665 -0.7385952 ]\n",
      "Action  1\n",
      "Observation  [-0.03179988  0.4064995  -0.03916855 -1.03885516] , reward  1.0 , done  False , info  {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03179988  0.4064995  -0.03916855 -1.03885516]\n",
      "Action  0\n",
      "Observation  [-0.02366989  0.21191931 -0.05994566 -0.75872135] , reward  1.0 , done  False , info  {}\n",
      "[-0.02366989  0.21191931 -0.05994566 -0.75872135]\n",
      "Action  1\n",
      "Observation  [-0.0194315   0.40781382 -0.07512008 -1.06964878] , reward  1.0 , done  False , info  {}\n",
      "[-0.0194315   0.40781382 -0.07512008 -1.06964878]\n",
      "Action  1\n",
      "Observation  [-0.01127523  0.60384449 -0.09651306 -1.38493007] , reward  1.0 , done  False , info  {}\n",
      "[-0.01127523  0.60384449 -0.09651306 -1.38493007]\n",
      "Action  0\n",
      "Observation  [ 8.01662943e-04  4.10049450e-01 -1.24211661e-01 -1.12392114e+00] , reward  1.0 , done  False , info  {}\n",
      "[ 8.01662943e-04  4.10049450e-01 -1.24211661e-01 -1.12392114e+00]\n",
      "Action  1\n",
      "Observation  [ 0.00900265  0.60656112 -0.14669008 -1.45284205] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00900265  0.60656112 -0.14669008 -1.45284205]\n",
      "Action  0\n",
      "Observation  [ 0.02113387  0.41351366 -0.17574692 -1.20935314] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02113387  0.41351366 -0.17574692 -1.20935314]\n",
      "Action  1\n",
      "Observation  [ 0.02940415  0.61041458 -0.19993399 -1.55155902] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02940415  0.61041458 -0.19993399 -1.55155902]\n",
      "Action  0\n",
      "Observation  [ 0.04161244  0.41817256 -0.23096517 -1.32733003] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 42 timesteps\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        print(\"Action \", action)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        print(\"Observation \", observation, \", reward \", reward, \", done \", done, \", info \" , info)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Frozen Lake scenario\n",
    "We are going to play to the [Frozen Lake](http://gym.openai.com/envs/FrozenLake-v0/) game.\n",
    "\n",
    "The problem is a grid where you should go from the 'start' (S) position to the 'goal position (G) (the pizza!). You can only walk through the 'frozen tiles' (F). Unfortunately, you can fall in a  'hole' (H).\n",
    "![](images/frozenlake-problem.png \"Frozen lake problem\")\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise. The possible actions are going left, right, up or down. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "![](images/frozenlake-world.png \"Frozen lake world\")\n",
    "\n",
    "\n",
    "Here you can see several episodes. A full recording is available at  [Frozen World](http://gym.openai.com/envs/FrozenLake-v0/).\n",
    "\n",
    "![](images/recording.gif \"Example running\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning with the Frozen Lake scenario\n",
    "We are now going to apply Q-Learning for the Frozen Lake scenario. This part of the notebook is taken from [here](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/Q%20Learning%20with%20FrozenLake.ipynb).\n",
    "\n",
    "First we create the environment and a Q-table inizializated with zeros to store the value of each action in a given state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning hyperparameters\n",
    "total_episodes = 10000        # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration hyperparameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.01             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we implement the Q-Learning algorithm.\n",
    "\n",
    "![](images/qlearning-algo.png \"Q-Learning algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.4826\n",
      "[[1.60518451e-01 8.91387956e-02 2.96601641e-01 9.01854463e-02]\n",
      " [1.13175005e-03 1.41005657e-03 3.62002511e-03 1.59960656e-01]\n",
      " [5.00399653e-03 1.36336032e-02 1.13519882e-02 1.23768268e-01]\n",
      " [8.74284025e-03 1.23643338e-03 7.20367554e-04 3.40941171e-02]\n",
      " [2.46871727e-01 4.60946645e-04 1.03481616e-01 7.45689133e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.08654555e-04 8.28346456e-10 6.92777320e-03 5.43643846e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.56917014e-02 1.46775173e-02 9.21011717e-02 1.26453025e-01]\n",
      " [8.03435752e-03 4.00005248e-01 1.88008887e-02 1.44495931e-02]\n",
      " [3.80255929e-02 3.62925397e-02 1.02173788e-01 6.43993394e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [8.26085057e-02 3.07854324e-02 1.06771521e-01 8.39566542e-02]\n",
      " [6.41377434e-02 3.67876191e-01 2.56672518e-01 1.70758755e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    episode += 1\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the learnt Q-table for playing the Frozen World game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        env.render()\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## Taxi\n",
    "Analyze the [Taxi problem](http://gym.openai.com/envs/Taxi-v2/) and solve it applying Q-Learning. You can find a solution as the one previously presented  [here](https://www.oreilly.com/learning/introduction-to-reinforcement-learning-and-openai-gym).\n",
    "\n",
    "Analyze the impact of not changing the learning rate (alfa or epsilon, depending on the book) or changing it in a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Create environment and initialize Qtable\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable)\n",
    "\n",
    "# Q-Learning hyperparameters\n",
    "total_episodes = 10000        # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration hyperparameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.01             # Exponential decay rate for exploration prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: -387.1101\n",
      "[[  0.           0.           0.           0.           0.\n",
      "    0.        ]\n",
      " [204.74486227 216.81449713 204.96239363 216.81287998 229.49140548\n",
      "  207.77400707]\n",
      " [196.01089049 207.72247246 195.91790223 207.81781805 219.98977698\n",
      "  198.75950646]\n",
      " ...\n",
      " [256.86632681 271.81779241 257.20212665 243.37904779 248.14730043\n",
      "  248.22009062]\n",
      " [171.67480096 181.88887893 171.59345337 181.84552996 162.71989687\n",
      "  162.77521002]\n",
      " [215.49649028 224.36265441 222.30866408 257.16550459 212.19436592\n",
      "  213.33934571]]\n",
      "****************************************************\n",
      "EPISODE  0\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m:\u001b[43m \u001b[0m| : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[42mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[42mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[42m_\u001b[0m|\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    }
   ],
   "source": [
    "#Algorithm\n",
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    episode += 1\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    #epsilon = 1.0\n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)\n",
    "\n",
    "\n",
    "\n",
    "#Game\n",
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        env.render()\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificando la tasa de aprendizaje minima 1 para que siempre intente explorar en vez de explotar el entorno me da el mismo resultado - No cambia el resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional exercises\n",
    "\n",
    "## Doom\n",
    "Read this [article](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8) and execute the companion [notebook](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/DQN%20Doom). Analyze the results and provide conclusions about DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [Diving deeper into Reinforcement Learning with Q-Learning, Thomas Simonini](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe).\n",
    "* Illustrations by [Thomas Simonini](https://github.com/simoninithomas/Deep_reinforcement_learning_Course) and [Sung Kim](https://www.youtube.com/watch?v=xgoO54qN4lY).\n",
    "* [Frozen Lake solution with TensorFlow](https://analyticsindiamag.com/openai-gym-frozen-lake-beginners-guide-reinforcement-learning/)\n",
    "* [Deep Q-Learning for Doom](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)\n",
    "* [Intro OpenAI Gym with Random Search and the Cart Pole scenario](http://www.pinchofintelligence.com/getting-started-openai-gym/)\n",
    "* [Q-Learning for the Taxi scenario](https://www.oreilly.com/learning/introduction-to-reinforcement-learning-and-openai-gym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is freely licensed under under the [Creative Commons Attribution Share-Alike license](https://creativecommons.org/licenses/by/2.0/).  \n",
    "\n",
    "© 2018 Carlos A. Iglesias, Universidad Politécnica de Madrid."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
