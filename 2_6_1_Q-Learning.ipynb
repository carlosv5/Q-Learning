{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/EscUpmPolit_p.gif \"UPM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Notes for Learning Intelligent Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Department of Telematic Engineering Systems, Universidad Politécnica de Madrid, © 2018 Carlos A. Iglesias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Introduction to Machine Learning V](2_6_0_Intro_RL.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Introduction](#Introduction)\n",
    "* [Getting started with OpenAI Gym](#Getting-started-with-OpenAI-Gym)\n",
    "* [The Frozen Lake scenario](#The-Frozen-Lake-scenario)\n",
    "* [Q-Learning with the Frozen Lake scenario](#Q-Learning-with-the-Frozen-Lake-scenario)\n",
    "* [Exercises](#Exercises)\n",
    "* [Optional exercises](#Optional-exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The purpose of this practice is to understand better Reinforcement Learning (RL) and, in particular, Q-Learning.\n",
    "\n",
    "We are going to use [OpenAI Gym](https://gym.openai.com/). OpenAI is a toolkit for developing and comparing RL algorithms.Take a loot at ther [website](https://gym.openai.com/).\n",
    "\n",
    "It implements [algorithm imitation](http://gym.openai.com/envs/#algorithmic), [classic control problems](http://gym.openai.com/envs/#classic_control), [Atari games](http://gym.openai.com/envs/#atari), [Box2D continuous control](http://gym.openai.com/envs/#box2d), [robotics with MuJoCo, Multi-Joint dynamics with Contact](http://gym.openai.com/envs/#mujoco),  and [simple text based environments](http://gym.openai.com/envs/#toy_text).\n",
    "\n",
    "This notebook is based on * [Diving deeper into Reinforcement Learning with Q-Learning](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe).\n",
    "\n",
    "First of all, install the OpenAI Gym  library:\n",
    "\n",
    "```console\n",
    "foo@bar:~$ pip install gym\n",
    "```\n",
    "\n",
    "\n",
    "If you get the error message 'NotImplementedError: abstract', [execute](https://github.com/openai/gym/issues/775) \n",
    "```console\n",
    "foo@bar:~$ pip install pyglet==1.2.4\n",
    "```\n",
    "\n",
    "If you want to try the Atari environment, it is better that you opt for the full installation from the source. Follow the instructions at [https://github.com/openai/gym#id15](OpenGym).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with OpenAI Gym\n",
    "\n",
    "First of all, read the [introduction](http://gym.openai.com/docs/#getting-started-with-gym) of OpenAI Gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "OpenGym provides a number of problems called *environments*. \n",
    "\n",
    "Try the 'CartPole-v0' (or 'MountainCar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "#env = gym.make('MountainCar-v0')\n",
    "#env = gym.make('Taxi-v2')\n",
    "\n",
    "#env = gym.make('Jamesbond-ram-v0')\n",
    "\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will launch an external window with the game. If you cannot close that window, just execute in a code cell:\n",
    "\n",
    "```python\n",
    "env.close()\n",
    "```\n",
    "\n",
    "The full list of available environments can be found printing the environment registry as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([EnvSpec(Copy-v0), EnvSpec(RepeatCopy-v0), EnvSpec(ReversedAddition-v0), EnvSpec(ReversedAddition3-v0), EnvSpec(DuplicatedInput-v0), EnvSpec(Reverse-v0), EnvSpec(CartPole-v0), EnvSpec(CartPole-v1), EnvSpec(MountainCar-v0), EnvSpec(MountainCarContinuous-v0), EnvSpec(Pendulum-v0), EnvSpec(Acrobot-v1), EnvSpec(LunarLander-v2), EnvSpec(LunarLanderContinuous-v2), EnvSpec(BipedalWalker-v2), EnvSpec(BipedalWalkerHardcore-v2), EnvSpec(CarRacing-v0), EnvSpec(Blackjack-v0), EnvSpec(KellyCoinflip-v0), EnvSpec(KellyCoinflipGeneralized-v0), EnvSpec(FrozenLake-v0), EnvSpec(FrozenLake8x8-v0), EnvSpec(CliffWalking-v0), EnvSpec(NChain-v0), EnvSpec(Roulette-v0), EnvSpec(Taxi-v2), EnvSpec(GuessingGame-v0), EnvSpec(HotterColder-v0), EnvSpec(Reacher-v2), EnvSpec(Pusher-v2), EnvSpec(Thrower-v2), EnvSpec(Striker-v2), EnvSpec(InvertedPendulum-v2), EnvSpec(InvertedDoublePendulum-v2), EnvSpec(HalfCheetah-v2), EnvSpec(Hopper-v2), EnvSpec(Swimmer-v2), EnvSpec(Walker2d-v2), EnvSpec(Ant-v2), EnvSpec(Humanoid-v2), EnvSpec(HumanoidStandup-v2), EnvSpec(FetchSlide-v1), EnvSpec(FetchPickAndPlace-v1), EnvSpec(FetchReach-v1), EnvSpec(FetchPush-v1), EnvSpec(HandReach-v0), EnvSpec(HandManipulateBlockRotateZ-v0), EnvSpec(HandManipulateBlockRotateParallel-v0), EnvSpec(HandManipulateBlockRotateXYZ-v0), EnvSpec(HandManipulateBlockFull-v0), EnvSpec(HandManipulateBlock-v0), EnvSpec(HandManipulateEggRotate-v0), EnvSpec(HandManipulateEggFull-v0), EnvSpec(HandManipulateEgg-v0), EnvSpec(HandManipulatePenRotate-v0), EnvSpec(HandManipulatePenFull-v0), EnvSpec(HandManipulatePen-v0), EnvSpec(FetchSlideDense-v1), EnvSpec(FetchPickAndPlaceDense-v1), EnvSpec(FetchReachDense-v1), EnvSpec(FetchPushDense-v1), EnvSpec(HandReachDense-v0), EnvSpec(HandManipulateBlockRotateZDense-v0), EnvSpec(HandManipulateBlockRotateParallelDense-v0), EnvSpec(HandManipulateBlockRotateXYZDense-v0), EnvSpec(HandManipulateBlockFullDense-v0), EnvSpec(HandManipulateBlockDense-v0), EnvSpec(HandManipulateEggRotateDense-v0), EnvSpec(HandManipulateEggFullDense-v0), EnvSpec(HandManipulateEggDense-v0), EnvSpec(HandManipulatePenRotateDense-v0), EnvSpec(HandManipulatePenFullDense-v0), EnvSpec(HandManipulatePenDense-v0), EnvSpec(AirRaid-v0), EnvSpec(AirRaid-v4), EnvSpec(AirRaidDeterministic-v0), EnvSpec(AirRaidDeterministic-v4), EnvSpec(AirRaidNoFrameskip-v0), EnvSpec(AirRaidNoFrameskip-v4), EnvSpec(AirRaid-ram-v0), EnvSpec(AirRaid-ram-v4), EnvSpec(AirRaid-ramDeterministic-v0), EnvSpec(AirRaid-ramDeterministic-v4), EnvSpec(AirRaid-ramNoFrameskip-v0), EnvSpec(AirRaid-ramNoFrameskip-v4), EnvSpec(Alien-v0), EnvSpec(Alien-v4), EnvSpec(AlienDeterministic-v0), EnvSpec(AlienDeterministic-v4), EnvSpec(AlienNoFrameskip-v0), EnvSpec(AlienNoFrameskip-v4), EnvSpec(Alien-ram-v0), EnvSpec(Alien-ram-v4), EnvSpec(Alien-ramDeterministic-v0), EnvSpec(Alien-ramDeterministic-v4), EnvSpec(Alien-ramNoFrameskip-v0), EnvSpec(Alien-ramNoFrameskip-v4), EnvSpec(Amidar-v0), EnvSpec(Amidar-v4), EnvSpec(AmidarDeterministic-v0), EnvSpec(AmidarDeterministic-v4), EnvSpec(AmidarNoFrameskip-v0), EnvSpec(AmidarNoFrameskip-v4), EnvSpec(Amidar-ram-v0), EnvSpec(Amidar-ram-v4), EnvSpec(Amidar-ramDeterministic-v0), EnvSpec(Amidar-ramDeterministic-v4), EnvSpec(Amidar-ramNoFrameskip-v0), EnvSpec(Amidar-ramNoFrameskip-v4), EnvSpec(Assault-v0), EnvSpec(Assault-v4), EnvSpec(AssaultDeterministic-v0), EnvSpec(AssaultDeterministic-v4), EnvSpec(AssaultNoFrameskip-v0), EnvSpec(AssaultNoFrameskip-v4), EnvSpec(Assault-ram-v0), EnvSpec(Assault-ram-v4), EnvSpec(Assault-ramDeterministic-v0), EnvSpec(Assault-ramDeterministic-v4), EnvSpec(Assault-ramNoFrameskip-v0), EnvSpec(Assault-ramNoFrameskip-v4), EnvSpec(Asterix-v0), EnvSpec(Asterix-v4), EnvSpec(AsterixDeterministic-v0), EnvSpec(AsterixDeterministic-v4), EnvSpec(AsterixNoFrameskip-v0), EnvSpec(AsterixNoFrameskip-v4), EnvSpec(Asterix-ram-v0), EnvSpec(Asterix-ram-v4), EnvSpec(Asterix-ramDeterministic-v0), EnvSpec(Asterix-ramDeterministic-v4), EnvSpec(Asterix-ramNoFrameskip-v0), EnvSpec(Asterix-ramNoFrameskip-v4), EnvSpec(Asteroids-v0), EnvSpec(Asteroids-v4), EnvSpec(AsteroidsDeterministic-v0), EnvSpec(AsteroidsDeterministic-v4), EnvSpec(AsteroidsNoFrameskip-v0), EnvSpec(AsteroidsNoFrameskip-v4), EnvSpec(Asteroids-ram-v0), EnvSpec(Asteroids-ram-v4), EnvSpec(Asteroids-ramDeterministic-v0), EnvSpec(Asteroids-ramDeterministic-v4), EnvSpec(Asteroids-ramNoFrameskip-v0), EnvSpec(Asteroids-ramNoFrameskip-v4), EnvSpec(Atlantis-v0), EnvSpec(Atlantis-v4), EnvSpec(AtlantisDeterministic-v0), EnvSpec(AtlantisDeterministic-v4), EnvSpec(AtlantisNoFrameskip-v0), EnvSpec(AtlantisNoFrameskip-v4), EnvSpec(Atlantis-ram-v0), EnvSpec(Atlantis-ram-v4), EnvSpec(Atlantis-ramDeterministic-v0), EnvSpec(Atlantis-ramDeterministic-v4), EnvSpec(Atlantis-ramNoFrameskip-v0), EnvSpec(Atlantis-ramNoFrameskip-v4), EnvSpec(BankHeist-v0), EnvSpec(BankHeist-v4), EnvSpec(BankHeistDeterministic-v0), EnvSpec(BankHeistDeterministic-v4), EnvSpec(BankHeistNoFrameskip-v0), EnvSpec(BankHeistNoFrameskip-v4), EnvSpec(BankHeist-ram-v0), EnvSpec(BankHeist-ram-v4), EnvSpec(BankHeist-ramDeterministic-v0), EnvSpec(BankHeist-ramDeterministic-v4), EnvSpec(BankHeist-ramNoFrameskip-v0), EnvSpec(BankHeist-ramNoFrameskip-v4), EnvSpec(BattleZone-v0), EnvSpec(BattleZone-v4), EnvSpec(BattleZoneDeterministic-v0), EnvSpec(BattleZoneDeterministic-v4), EnvSpec(BattleZoneNoFrameskip-v0), EnvSpec(BattleZoneNoFrameskip-v4), EnvSpec(BattleZone-ram-v0), EnvSpec(BattleZone-ram-v4), EnvSpec(BattleZone-ramDeterministic-v0), EnvSpec(BattleZone-ramDeterministic-v4), EnvSpec(BattleZone-ramNoFrameskip-v0), EnvSpec(BattleZone-ramNoFrameskip-v4), EnvSpec(BeamRider-v0), EnvSpec(BeamRider-v4), EnvSpec(BeamRiderDeterministic-v0), EnvSpec(BeamRiderDeterministic-v4), EnvSpec(BeamRiderNoFrameskip-v0), EnvSpec(BeamRiderNoFrameskip-v4), EnvSpec(BeamRider-ram-v0), EnvSpec(BeamRider-ram-v4), EnvSpec(BeamRider-ramDeterministic-v0), EnvSpec(BeamRider-ramDeterministic-v4), EnvSpec(BeamRider-ramNoFrameskip-v0), EnvSpec(BeamRider-ramNoFrameskip-v4), EnvSpec(Berzerk-v0), EnvSpec(Berzerk-v4), EnvSpec(BerzerkDeterministic-v0), EnvSpec(BerzerkDeterministic-v4), EnvSpec(BerzerkNoFrameskip-v0), EnvSpec(BerzerkNoFrameskip-v4), EnvSpec(Berzerk-ram-v0), EnvSpec(Berzerk-ram-v4), EnvSpec(Berzerk-ramDeterministic-v0), EnvSpec(Berzerk-ramDeterministic-v4), EnvSpec(Berzerk-ramNoFrameskip-v0), EnvSpec(Berzerk-ramNoFrameskip-v4), EnvSpec(Bowling-v0), EnvSpec(Bowling-v4), EnvSpec(BowlingDeterministic-v0), EnvSpec(BowlingDeterministic-v4), EnvSpec(BowlingNoFrameskip-v0), EnvSpec(BowlingNoFrameskip-v4), EnvSpec(Bowling-ram-v0), EnvSpec(Bowling-ram-v4), EnvSpec(Bowling-ramDeterministic-v0), EnvSpec(Bowling-ramDeterministic-v4), EnvSpec(Bowling-ramNoFrameskip-v0), EnvSpec(Bowling-ramNoFrameskip-v4), EnvSpec(Boxing-v0), EnvSpec(Boxing-v4), EnvSpec(BoxingDeterministic-v0), EnvSpec(BoxingDeterministic-v4), EnvSpec(BoxingNoFrameskip-v0), EnvSpec(BoxingNoFrameskip-v4), EnvSpec(Boxing-ram-v0), EnvSpec(Boxing-ram-v4), EnvSpec(Boxing-ramDeterministic-v0), EnvSpec(Boxing-ramDeterministic-v4), EnvSpec(Boxing-ramNoFrameskip-v0), EnvSpec(Boxing-ramNoFrameskip-v4), EnvSpec(Breakout-v0), EnvSpec(Breakout-v4), EnvSpec(BreakoutDeterministic-v0), EnvSpec(BreakoutDeterministic-v4), EnvSpec(BreakoutNoFrameskip-v0), EnvSpec(BreakoutNoFrameskip-v4), EnvSpec(Breakout-ram-v0), EnvSpec(Breakout-ram-v4), EnvSpec(Breakout-ramDeterministic-v0), EnvSpec(Breakout-ramDeterministic-v4), EnvSpec(Breakout-ramNoFrameskip-v0), EnvSpec(Breakout-ramNoFrameskip-v4), EnvSpec(Carnival-v0), EnvSpec(Carnival-v4), EnvSpec(CarnivalDeterministic-v0), EnvSpec(CarnivalDeterministic-v4), EnvSpec(CarnivalNoFrameskip-v0), EnvSpec(CarnivalNoFrameskip-v4), EnvSpec(Carnival-ram-v0), EnvSpec(Carnival-ram-v4), EnvSpec(Carnival-ramDeterministic-v0), EnvSpec(Carnival-ramDeterministic-v4), EnvSpec(Carnival-ramNoFrameskip-v0), EnvSpec(Carnival-ramNoFrameskip-v4), EnvSpec(Centipede-v0), EnvSpec(Centipede-v4), EnvSpec(CentipedeDeterministic-v0), EnvSpec(CentipedeDeterministic-v4), EnvSpec(CentipedeNoFrameskip-v0), EnvSpec(CentipedeNoFrameskip-v4), EnvSpec(Centipede-ram-v0), EnvSpec(Centipede-ram-v4), EnvSpec(Centipede-ramDeterministic-v0), EnvSpec(Centipede-ramDeterministic-v4), EnvSpec(Centipede-ramNoFrameskip-v0), EnvSpec(Centipede-ramNoFrameskip-v4), EnvSpec(ChopperCommand-v0), EnvSpec(ChopperCommand-v4), EnvSpec(ChopperCommandDeterministic-v0), EnvSpec(ChopperCommandDeterministic-v4), EnvSpec(ChopperCommandNoFrameskip-v0), EnvSpec(ChopperCommandNoFrameskip-v4), EnvSpec(ChopperCommand-ram-v0), EnvSpec(ChopperCommand-ram-v4), EnvSpec(ChopperCommand-ramDeterministic-v0), EnvSpec(ChopperCommand-ramDeterministic-v4), EnvSpec(ChopperCommand-ramNoFrameskip-v0), EnvSpec(ChopperCommand-ramNoFrameskip-v4), EnvSpec(CrazyClimber-v0), EnvSpec(CrazyClimber-v4), EnvSpec(CrazyClimberDeterministic-v0), EnvSpec(CrazyClimberDeterministic-v4), EnvSpec(CrazyClimberNoFrameskip-v0), EnvSpec(CrazyClimberNoFrameskip-v4), EnvSpec(CrazyClimber-ram-v0), EnvSpec(CrazyClimber-ram-v4), EnvSpec(CrazyClimber-ramDeterministic-v0), EnvSpec(CrazyClimber-ramDeterministic-v4), EnvSpec(CrazyClimber-ramNoFrameskip-v0), EnvSpec(CrazyClimber-ramNoFrameskip-v4), EnvSpec(DemonAttack-v0), EnvSpec(DemonAttack-v4), EnvSpec(DemonAttackDeterministic-v0), EnvSpec(DemonAttackDeterministic-v4), EnvSpec(DemonAttackNoFrameskip-v0), EnvSpec(DemonAttackNoFrameskip-v4), EnvSpec(DemonAttack-ram-v0), EnvSpec(DemonAttack-ram-v4), EnvSpec(DemonAttack-ramDeterministic-v0), EnvSpec(DemonAttack-ramDeterministic-v4), EnvSpec(DemonAttack-ramNoFrameskip-v0), EnvSpec(DemonAttack-ramNoFrameskip-v4), EnvSpec(DoubleDunk-v0), EnvSpec(DoubleDunk-v4), EnvSpec(DoubleDunkDeterministic-v0), EnvSpec(DoubleDunkDeterministic-v4), EnvSpec(DoubleDunkNoFrameskip-v0), EnvSpec(DoubleDunkNoFrameskip-v4), EnvSpec(DoubleDunk-ram-v0), EnvSpec(DoubleDunk-ram-v4), EnvSpec(DoubleDunk-ramDeterministic-v0), EnvSpec(DoubleDunk-ramDeterministic-v4), EnvSpec(DoubleDunk-ramNoFrameskip-v0), EnvSpec(DoubleDunk-ramNoFrameskip-v4), EnvSpec(ElevatorAction-v0), EnvSpec(ElevatorAction-v4), EnvSpec(ElevatorActionDeterministic-v0), EnvSpec(ElevatorActionDeterministic-v4), EnvSpec(ElevatorActionNoFrameskip-v0), EnvSpec(ElevatorActionNoFrameskip-v4), EnvSpec(ElevatorAction-ram-v0), EnvSpec(ElevatorAction-ram-v4), EnvSpec(ElevatorAction-ramDeterministic-v0), EnvSpec(ElevatorAction-ramDeterministic-v4), EnvSpec(ElevatorAction-ramNoFrameskip-v0), EnvSpec(ElevatorAction-ramNoFrameskip-v4), EnvSpec(Enduro-v0), EnvSpec(Enduro-v4), EnvSpec(EnduroDeterministic-v0), EnvSpec(EnduroDeterministic-v4), EnvSpec(EnduroNoFrameskip-v0), EnvSpec(EnduroNoFrameskip-v4), EnvSpec(Enduro-ram-v0), EnvSpec(Enduro-ram-v4), EnvSpec(Enduro-ramDeterministic-v0), EnvSpec(Enduro-ramDeterministic-v4), EnvSpec(Enduro-ramNoFrameskip-v0), EnvSpec(Enduro-ramNoFrameskip-v4), EnvSpec(FishingDerby-v0), EnvSpec(FishingDerby-v4), EnvSpec(FishingDerbyDeterministic-v0), EnvSpec(FishingDerbyDeterministic-v4), EnvSpec(FishingDerbyNoFrameskip-v0), EnvSpec(FishingDerbyNoFrameskip-v4), EnvSpec(FishingDerby-ram-v0), EnvSpec(FishingDerby-ram-v4), EnvSpec(FishingDerby-ramDeterministic-v0), EnvSpec(FishingDerby-ramDeterministic-v4), EnvSpec(FishingDerby-ramNoFrameskip-v0), EnvSpec(FishingDerby-ramNoFrameskip-v4), EnvSpec(Freeway-v0), EnvSpec(Freeway-v4), EnvSpec(FreewayDeterministic-v0), EnvSpec(FreewayDeterministic-v4), EnvSpec(FreewayNoFrameskip-v0), EnvSpec(FreewayNoFrameskip-v4), EnvSpec(Freeway-ram-v0), EnvSpec(Freeway-ram-v4), EnvSpec(Freeway-ramDeterministic-v0), EnvSpec(Freeway-ramDeterministic-v4), EnvSpec(Freeway-ramNoFrameskip-v0), EnvSpec(Freeway-ramNoFrameskip-v4), EnvSpec(Frostbite-v0), EnvSpec(Frostbite-v4), EnvSpec(FrostbiteDeterministic-v0), EnvSpec(FrostbiteDeterministic-v4), EnvSpec(FrostbiteNoFrameskip-v0), EnvSpec(FrostbiteNoFrameskip-v4), EnvSpec(Frostbite-ram-v0), EnvSpec(Frostbite-ram-v4), EnvSpec(Frostbite-ramDeterministic-v0), EnvSpec(Frostbite-ramDeterministic-v4), EnvSpec(Frostbite-ramNoFrameskip-v0), EnvSpec(Frostbite-ramNoFrameskip-v4), EnvSpec(Gopher-v0), EnvSpec(Gopher-v4), EnvSpec(GopherDeterministic-v0), EnvSpec(GopherDeterministic-v4), EnvSpec(GopherNoFrameskip-v0), EnvSpec(GopherNoFrameskip-v4), EnvSpec(Gopher-ram-v0), EnvSpec(Gopher-ram-v4), EnvSpec(Gopher-ramDeterministic-v0), EnvSpec(Gopher-ramDeterministic-v4), EnvSpec(Gopher-ramNoFrameskip-v0), EnvSpec(Gopher-ramNoFrameskip-v4), EnvSpec(Gravitar-v0), EnvSpec(Gravitar-v4), EnvSpec(GravitarDeterministic-v0), EnvSpec(GravitarDeterministic-v4), EnvSpec(GravitarNoFrameskip-v0), EnvSpec(GravitarNoFrameskip-v4), EnvSpec(Gravitar-ram-v0), EnvSpec(Gravitar-ram-v4), EnvSpec(Gravitar-ramDeterministic-v0), EnvSpec(Gravitar-ramDeterministic-v4), EnvSpec(Gravitar-ramNoFrameskip-v0), EnvSpec(Gravitar-ramNoFrameskip-v4), EnvSpec(Hero-v0), EnvSpec(Hero-v4), EnvSpec(HeroDeterministic-v0), EnvSpec(HeroDeterministic-v4), EnvSpec(HeroNoFrameskip-v0), EnvSpec(HeroNoFrameskip-v4), EnvSpec(Hero-ram-v0), EnvSpec(Hero-ram-v4), EnvSpec(Hero-ramDeterministic-v0), EnvSpec(Hero-ramDeterministic-v4), EnvSpec(Hero-ramNoFrameskip-v0), EnvSpec(Hero-ramNoFrameskip-v4), EnvSpec(IceHockey-v0), EnvSpec(IceHockey-v4), EnvSpec(IceHockeyDeterministic-v0), EnvSpec(IceHockeyDeterministic-v4), EnvSpec(IceHockeyNoFrameskip-v0), EnvSpec(IceHockeyNoFrameskip-v4), EnvSpec(IceHockey-ram-v0), EnvSpec(IceHockey-ram-v4), EnvSpec(IceHockey-ramDeterministic-v0), EnvSpec(IceHockey-ramDeterministic-v4), EnvSpec(IceHockey-ramNoFrameskip-v0), EnvSpec(IceHockey-ramNoFrameskip-v4), EnvSpec(Jamesbond-v0), EnvSpec(Jamesbond-v4), EnvSpec(JamesbondDeterministic-v0), EnvSpec(JamesbondDeterministic-v4), EnvSpec(JamesbondNoFrameskip-v0), EnvSpec(JamesbondNoFrameskip-v4), EnvSpec(Jamesbond-ram-v0), EnvSpec(Jamesbond-ram-v4), EnvSpec(Jamesbond-ramDeterministic-v0), EnvSpec(Jamesbond-ramDeterministic-v4), EnvSpec(Jamesbond-ramNoFrameskip-v0), EnvSpec(Jamesbond-ramNoFrameskip-v4), EnvSpec(JourneyEscape-v0), EnvSpec(JourneyEscape-v4), EnvSpec(JourneyEscapeDeterministic-v0), EnvSpec(JourneyEscapeDeterministic-v4), EnvSpec(JourneyEscapeNoFrameskip-v0), EnvSpec(JourneyEscapeNoFrameskip-v4), EnvSpec(JourneyEscape-ram-v0), EnvSpec(JourneyEscape-ram-v4), EnvSpec(JourneyEscape-ramDeterministic-v0), EnvSpec(JourneyEscape-ramDeterministic-v4), EnvSpec(JourneyEscape-ramNoFrameskip-v0), EnvSpec(JourneyEscape-ramNoFrameskip-v4), EnvSpec(Kangaroo-v0), EnvSpec(Kangaroo-v4), EnvSpec(KangarooDeterministic-v0), EnvSpec(KangarooDeterministic-v4), EnvSpec(KangarooNoFrameskip-v0), EnvSpec(KangarooNoFrameskip-v4), EnvSpec(Kangaroo-ram-v0), EnvSpec(Kangaroo-ram-v4), EnvSpec(Kangaroo-ramDeterministic-v0), EnvSpec(Kangaroo-ramDeterministic-v4), EnvSpec(Kangaroo-ramNoFrameskip-v0), EnvSpec(Kangaroo-ramNoFrameskip-v4), EnvSpec(Krull-v0), EnvSpec(Krull-v4), EnvSpec(KrullDeterministic-v0), EnvSpec(KrullDeterministic-v4), EnvSpec(KrullNoFrameskip-v0), EnvSpec(KrullNoFrameskip-v4), EnvSpec(Krull-ram-v0), EnvSpec(Krull-ram-v4), EnvSpec(Krull-ramDeterministic-v0), EnvSpec(Krull-ramDeterministic-v4), EnvSpec(Krull-ramNoFrameskip-v0), EnvSpec(Krull-ramNoFrameskip-v4), EnvSpec(KungFuMaster-v0), EnvSpec(KungFuMaster-v4), EnvSpec(KungFuMasterDeterministic-v0), EnvSpec(KungFuMasterDeterministic-v4), EnvSpec(KungFuMasterNoFrameskip-v0), EnvSpec(KungFuMasterNoFrameskip-v4), EnvSpec(KungFuMaster-ram-v0), EnvSpec(KungFuMaster-ram-v4), EnvSpec(KungFuMaster-ramDeterministic-v0), EnvSpec(KungFuMaster-ramDeterministic-v4), EnvSpec(KungFuMaster-ramNoFrameskip-v0), EnvSpec(KungFuMaster-ramNoFrameskip-v4), EnvSpec(MontezumaRevenge-v0), EnvSpec(MontezumaRevenge-v4), EnvSpec(MontezumaRevengeDeterministic-v0), EnvSpec(MontezumaRevengeDeterministic-v4), EnvSpec(MontezumaRevengeNoFrameskip-v0), EnvSpec(MontezumaRevengeNoFrameskip-v4), EnvSpec(MontezumaRevenge-ram-v0), EnvSpec(MontezumaRevenge-ram-v4), EnvSpec(MontezumaRevenge-ramDeterministic-v0), EnvSpec(MontezumaRevenge-ramDeterministic-v4), EnvSpec(MontezumaRevenge-ramNoFrameskip-v0), EnvSpec(MontezumaRevenge-ramNoFrameskip-v4), EnvSpec(MsPacman-v0), EnvSpec(MsPacman-v4), EnvSpec(MsPacmanDeterministic-v0), EnvSpec(MsPacmanDeterministic-v4), EnvSpec(MsPacmanNoFrameskip-v0), EnvSpec(MsPacmanNoFrameskip-v4), EnvSpec(MsPacman-ram-v0), EnvSpec(MsPacman-ram-v4), EnvSpec(MsPacman-ramDeterministic-v0), EnvSpec(MsPacman-ramDeterministic-v4), EnvSpec(MsPacman-ramNoFrameskip-v0), EnvSpec(MsPacman-ramNoFrameskip-v4), EnvSpec(NameThisGame-v0), EnvSpec(NameThisGame-v4), EnvSpec(NameThisGameDeterministic-v0), EnvSpec(NameThisGameDeterministic-v4), EnvSpec(NameThisGameNoFrameskip-v0), EnvSpec(NameThisGameNoFrameskip-v4), EnvSpec(NameThisGame-ram-v0), EnvSpec(NameThisGame-ram-v4), EnvSpec(NameThisGame-ramDeterministic-v0), EnvSpec(NameThisGame-ramDeterministic-v4), EnvSpec(NameThisGame-ramNoFrameskip-v0), EnvSpec(NameThisGame-ramNoFrameskip-v4), EnvSpec(Phoenix-v0), EnvSpec(Phoenix-v4), EnvSpec(PhoenixDeterministic-v0), EnvSpec(PhoenixDeterministic-v4), EnvSpec(PhoenixNoFrameskip-v0), EnvSpec(PhoenixNoFrameskip-v4), EnvSpec(Phoenix-ram-v0), EnvSpec(Phoenix-ram-v4), EnvSpec(Phoenix-ramDeterministic-v0), EnvSpec(Phoenix-ramDeterministic-v4), EnvSpec(Phoenix-ramNoFrameskip-v0), EnvSpec(Phoenix-ramNoFrameskip-v4), EnvSpec(Pitfall-v0), EnvSpec(Pitfall-v4), EnvSpec(PitfallDeterministic-v0), EnvSpec(PitfallDeterministic-v4), EnvSpec(PitfallNoFrameskip-v0), EnvSpec(PitfallNoFrameskip-v4), EnvSpec(Pitfall-ram-v0), EnvSpec(Pitfall-ram-v4), EnvSpec(Pitfall-ramDeterministic-v0), EnvSpec(Pitfall-ramDeterministic-v4), EnvSpec(Pitfall-ramNoFrameskip-v0), EnvSpec(Pitfall-ramNoFrameskip-v4), EnvSpec(Pong-v0), EnvSpec(Pong-v4), EnvSpec(PongDeterministic-v0), EnvSpec(PongDeterministic-v4), EnvSpec(PongNoFrameskip-v0), EnvSpec(PongNoFrameskip-v4), EnvSpec(Pong-ram-v0), EnvSpec(Pong-ram-v4), EnvSpec(Pong-ramDeterministic-v0), EnvSpec(Pong-ramDeterministic-v4), EnvSpec(Pong-ramNoFrameskip-v0), EnvSpec(Pong-ramNoFrameskip-v4), EnvSpec(Pooyan-v0), EnvSpec(Pooyan-v4), EnvSpec(PooyanDeterministic-v0), EnvSpec(PooyanDeterministic-v4), EnvSpec(PooyanNoFrameskip-v0), EnvSpec(PooyanNoFrameskip-v4), EnvSpec(Pooyan-ram-v0), EnvSpec(Pooyan-ram-v4), EnvSpec(Pooyan-ramDeterministic-v0), EnvSpec(Pooyan-ramDeterministic-v4), EnvSpec(Pooyan-ramNoFrameskip-v0), EnvSpec(Pooyan-ramNoFrameskip-v4), EnvSpec(PrivateEye-v0), EnvSpec(PrivateEye-v4), EnvSpec(PrivateEyeDeterministic-v0), EnvSpec(PrivateEyeDeterministic-v4), EnvSpec(PrivateEyeNoFrameskip-v0), EnvSpec(PrivateEyeNoFrameskip-v4), EnvSpec(PrivateEye-ram-v0), EnvSpec(PrivateEye-ram-v4), EnvSpec(PrivateEye-ramDeterministic-v0), EnvSpec(PrivateEye-ramDeterministic-v4), EnvSpec(PrivateEye-ramNoFrameskip-v0), EnvSpec(PrivateEye-ramNoFrameskip-v4), EnvSpec(Qbert-v0), EnvSpec(Qbert-v4), EnvSpec(QbertDeterministic-v0), EnvSpec(QbertDeterministic-v4), EnvSpec(QbertNoFrameskip-v0), EnvSpec(QbertNoFrameskip-v4), EnvSpec(Qbert-ram-v0), EnvSpec(Qbert-ram-v4), EnvSpec(Qbert-ramDeterministic-v0), EnvSpec(Qbert-ramDeterministic-v4), EnvSpec(Qbert-ramNoFrameskip-v0), EnvSpec(Qbert-ramNoFrameskip-v4), EnvSpec(Riverraid-v0), EnvSpec(Riverraid-v4), EnvSpec(RiverraidDeterministic-v0), EnvSpec(RiverraidDeterministic-v4), EnvSpec(RiverraidNoFrameskip-v0), EnvSpec(RiverraidNoFrameskip-v4), EnvSpec(Riverraid-ram-v0), EnvSpec(Riverraid-ram-v4), EnvSpec(Riverraid-ramDeterministic-v0), EnvSpec(Riverraid-ramDeterministic-v4), EnvSpec(Riverraid-ramNoFrameskip-v0), EnvSpec(Riverraid-ramNoFrameskip-v4), EnvSpec(RoadRunner-v0), EnvSpec(RoadRunner-v4), EnvSpec(RoadRunnerDeterministic-v0), EnvSpec(RoadRunnerDeterministic-v4), EnvSpec(RoadRunnerNoFrameskip-v0), EnvSpec(RoadRunnerNoFrameskip-v4), EnvSpec(RoadRunner-ram-v0), EnvSpec(RoadRunner-ram-v4), EnvSpec(RoadRunner-ramDeterministic-v0), EnvSpec(RoadRunner-ramDeterministic-v4), EnvSpec(RoadRunner-ramNoFrameskip-v0), EnvSpec(RoadRunner-ramNoFrameskip-v4), EnvSpec(Robotank-v0), EnvSpec(Robotank-v4), EnvSpec(RobotankDeterministic-v0), EnvSpec(RobotankDeterministic-v4), EnvSpec(RobotankNoFrameskip-v0), EnvSpec(RobotankNoFrameskip-v4), EnvSpec(Robotank-ram-v0), EnvSpec(Robotank-ram-v4), EnvSpec(Robotank-ramDeterministic-v0), EnvSpec(Robotank-ramDeterministic-v4), EnvSpec(Robotank-ramNoFrameskip-v0), EnvSpec(Robotank-ramNoFrameskip-v4), EnvSpec(Seaquest-v0), EnvSpec(Seaquest-v4), EnvSpec(SeaquestDeterministic-v0), EnvSpec(SeaquestDeterministic-v4), EnvSpec(SeaquestNoFrameskip-v0), EnvSpec(SeaquestNoFrameskip-v4), EnvSpec(Seaquest-ram-v0), EnvSpec(Seaquest-ram-v4), EnvSpec(Seaquest-ramDeterministic-v0), EnvSpec(Seaquest-ramDeterministic-v4), EnvSpec(Seaquest-ramNoFrameskip-v0), EnvSpec(Seaquest-ramNoFrameskip-v4), EnvSpec(Skiing-v0), EnvSpec(Skiing-v4), EnvSpec(SkiingDeterministic-v0), EnvSpec(SkiingDeterministic-v4), EnvSpec(SkiingNoFrameskip-v0), EnvSpec(SkiingNoFrameskip-v4), EnvSpec(Skiing-ram-v0), EnvSpec(Skiing-ram-v4), EnvSpec(Skiing-ramDeterministic-v0), EnvSpec(Skiing-ramDeterministic-v4), EnvSpec(Skiing-ramNoFrameskip-v0), EnvSpec(Skiing-ramNoFrameskip-v4), EnvSpec(Solaris-v0), EnvSpec(Solaris-v4), EnvSpec(SolarisDeterministic-v0), EnvSpec(SolarisDeterministic-v4), EnvSpec(SolarisNoFrameskip-v0), EnvSpec(SolarisNoFrameskip-v4), EnvSpec(Solaris-ram-v0), EnvSpec(Solaris-ram-v4), EnvSpec(Solaris-ramDeterministic-v0), EnvSpec(Solaris-ramDeterministic-v4), EnvSpec(Solaris-ramNoFrameskip-v0), EnvSpec(Solaris-ramNoFrameskip-v4), EnvSpec(SpaceInvaders-v0), EnvSpec(SpaceInvaders-v4), EnvSpec(SpaceInvadersDeterministic-v0), EnvSpec(SpaceInvadersDeterministic-v4), EnvSpec(SpaceInvadersNoFrameskip-v0), EnvSpec(SpaceInvadersNoFrameskip-v4), EnvSpec(SpaceInvaders-ram-v0), EnvSpec(SpaceInvaders-ram-v4), EnvSpec(SpaceInvaders-ramDeterministic-v0), EnvSpec(SpaceInvaders-ramDeterministic-v4), EnvSpec(SpaceInvaders-ramNoFrameskip-v0), EnvSpec(SpaceInvaders-ramNoFrameskip-v4), EnvSpec(StarGunner-v0), EnvSpec(StarGunner-v4), EnvSpec(StarGunnerDeterministic-v0), EnvSpec(StarGunnerDeterministic-v4), EnvSpec(StarGunnerNoFrameskip-v0), EnvSpec(StarGunnerNoFrameskip-v4), EnvSpec(StarGunner-ram-v0), EnvSpec(StarGunner-ram-v4), EnvSpec(StarGunner-ramDeterministic-v0), EnvSpec(StarGunner-ramDeterministic-v4), EnvSpec(StarGunner-ramNoFrameskip-v0), EnvSpec(StarGunner-ramNoFrameskip-v4), EnvSpec(Tennis-v0), EnvSpec(Tennis-v4), EnvSpec(TennisDeterministic-v0), EnvSpec(TennisDeterministic-v4), EnvSpec(TennisNoFrameskip-v0), EnvSpec(TennisNoFrameskip-v4), EnvSpec(Tennis-ram-v0), EnvSpec(Tennis-ram-v4), EnvSpec(Tennis-ramDeterministic-v0), EnvSpec(Tennis-ramDeterministic-v4), EnvSpec(Tennis-ramNoFrameskip-v0), EnvSpec(Tennis-ramNoFrameskip-v4), EnvSpec(TimePilot-v0), EnvSpec(TimePilot-v4), EnvSpec(TimePilotDeterministic-v0), EnvSpec(TimePilotDeterministic-v4), EnvSpec(TimePilotNoFrameskip-v0), EnvSpec(TimePilotNoFrameskip-v4), EnvSpec(TimePilot-ram-v0), EnvSpec(TimePilot-ram-v4), EnvSpec(TimePilot-ramDeterministic-v0), EnvSpec(TimePilot-ramDeterministic-v4), EnvSpec(TimePilot-ramNoFrameskip-v0), EnvSpec(TimePilot-ramNoFrameskip-v4), EnvSpec(Tutankham-v0), EnvSpec(Tutankham-v4), EnvSpec(TutankhamDeterministic-v0), EnvSpec(TutankhamDeterministic-v4), EnvSpec(TutankhamNoFrameskip-v0), EnvSpec(TutankhamNoFrameskip-v4), EnvSpec(Tutankham-ram-v0), EnvSpec(Tutankham-ram-v4), EnvSpec(Tutankham-ramDeterministic-v0), EnvSpec(Tutankham-ramDeterministic-v4), EnvSpec(Tutankham-ramNoFrameskip-v0), EnvSpec(Tutankham-ramNoFrameskip-v4), EnvSpec(UpNDown-v0), EnvSpec(UpNDown-v4), EnvSpec(UpNDownDeterministic-v0), EnvSpec(UpNDownDeterministic-v4), EnvSpec(UpNDownNoFrameskip-v0), EnvSpec(UpNDownNoFrameskip-v4), EnvSpec(UpNDown-ram-v0), EnvSpec(UpNDown-ram-v4), EnvSpec(UpNDown-ramDeterministic-v0), EnvSpec(UpNDown-ramDeterministic-v4), EnvSpec(UpNDown-ramNoFrameskip-v0), EnvSpec(UpNDown-ramNoFrameskip-v4), EnvSpec(Venture-v0), EnvSpec(Venture-v4), EnvSpec(VentureDeterministic-v0), EnvSpec(VentureDeterministic-v4), EnvSpec(VentureNoFrameskip-v0), EnvSpec(VentureNoFrameskip-v4), EnvSpec(Venture-ram-v0), EnvSpec(Venture-ram-v4), EnvSpec(Venture-ramDeterministic-v0), EnvSpec(Venture-ramDeterministic-v4), EnvSpec(Venture-ramNoFrameskip-v0), EnvSpec(Venture-ramNoFrameskip-v4), EnvSpec(VideoPinball-v0), EnvSpec(VideoPinball-v4), EnvSpec(VideoPinballDeterministic-v0), EnvSpec(VideoPinballDeterministic-v4), EnvSpec(VideoPinballNoFrameskip-v0), EnvSpec(VideoPinballNoFrameskip-v4), EnvSpec(VideoPinball-ram-v0), EnvSpec(VideoPinball-ram-v4), EnvSpec(VideoPinball-ramDeterministic-v0), EnvSpec(VideoPinball-ramDeterministic-v4), EnvSpec(VideoPinball-ramNoFrameskip-v0), EnvSpec(VideoPinball-ramNoFrameskip-v4), EnvSpec(WizardOfWor-v0), EnvSpec(WizardOfWor-v4), EnvSpec(WizardOfWorDeterministic-v0), EnvSpec(WizardOfWorDeterministic-v4), EnvSpec(WizardOfWorNoFrameskip-v0), EnvSpec(WizardOfWorNoFrameskip-v4), EnvSpec(WizardOfWor-ram-v0), EnvSpec(WizardOfWor-ram-v4), EnvSpec(WizardOfWor-ramDeterministic-v0), EnvSpec(WizardOfWor-ramDeterministic-v4), EnvSpec(WizardOfWor-ramNoFrameskip-v0), EnvSpec(WizardOfWor-ramNoFrameskip-v4), EnvSpec(YarsRevenge-v0), EnvSpec(YarsRevenge-v4), EnvSpec(YarsRevengeDeterministic-v0), EnvSpec(YarsRevengeDeterministic-v4), EnvSpec(YarsRevengeNoFrameskip-v0), EnvSpec(YarsRevengeNoFrameskip-v4), EnvSpec(YarsRevenge-ram-v0), EnvSpec(YarsRevenge-ram-v4), EnvSpec(YarsRevenge-ramDeterministic-v0), EnvSpec(YarsRevenge-ramDeterministic-v4), EnvSpec(YarsRevenge-ramNoFrameskip-v0), EnvSpec(YarsRevenge-ramNoFrameskip-v4), EnvSpec(Zaxxon-v0), EnvSpec(Zaxxon-v4), EnvSpec(ZaxxonDeterministic-v0), EnvSpec(ZaxxonDeterministic-v4), EnvSpec(ZaxxonNoFrameskip-v0), EnvSpec(ZaxxonNoFrameskip-v4), EnvSpec(Zaxxon-ram-v0), EnvSpec(Zaxxon-ram-v4), EnvSpec(Zaxxon-ramDeterministic-v0), EnvSpec(Zaxxon-ramDeterministic-v4), EnvSpec(Zaxxon-ramNoFrameskip-v0), EnvSpec(Zaxxon-ramNoFrameskip-v4), EnvSpec(CubeCrash-v0), EnvSpec(CubeCrashSparse-v0), EnvSpec(CubeCrashScreenBecomesBlack-v0), EnvSpec(MemorizeDigits-v0)])\n"
     ]
    }
   ],
   "source": [
    "from gym import envs\n",
    "print(envs.registry.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment’s **step** function returns  four values. These are:\n",
    "\n",
    "* **observation (object):** an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "* **reward (float):** amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "* **done (boolean):** whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.).\n",
    "* **info (dict):** diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.\n",
    "\n",
    "The typical agent loop consists in first calling the method *reset* which provides an initial observation. Then the agent executes an action, and receives the reward, the new observation, and if the episode has finished (done is true). \n",
    "\n",
    "For example, analyze this sample of agent loop for 100 ms. The details of the previous variables for this game as described [here](https://github.com/openai/gym/wiki/CartPole-v0) are:\n",
    "* **observation**: Cart Position, Cart Velocity, Pole Angle, Pole Velocity.\n",
    "* **action**: 0\t(Push cart to the left), 1\t(Push cart to the right).\n",
    "* **reward**: 1  for every step taken, including the termination step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[ 0.04940349  0.03291636  0.03694181 -0.01182461]\n",
      "Action  1\n",
      "Observation  [ 0.05006182  0.22748958  0.03670532 -0.29262689] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05006182  0.22748958  0.03670532 -0.29262689]\n",
      "Action  0\n",
      "Observation  [ 0.05461161  0.03186403  0.03085278  0.01140272] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05461161  0.03186403  0.03085278  0.01140272]\n",
      "Action  1\n",
      "Observation  [ 0.05524889  0.22653024  0.03108083 -0.27138835] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05524889  0.22653024  0.03108083 -0.27138835]\n",
      "Action  0\n",
      "Observation  [ 0.0597795   0.03097888  0.02565307  0.03093332] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0597795   0.03097888  0.02565307  0.03093332]\n",
      "Action  0\n",
      "Observation  [ 0.06039908 -0.16450137  0.02627173  0.3315984 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06039908 -0.16450137  0.02627173  0.3315984 ]\n",
      "Action  0\n",
      "Observation  [ 0.05710905 -0.35998724  0.0329037   0.63244901] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05710905 -0.35998724  0.0329037   0.63244901]\n",
      "Action  1\n",
      "Observation  [ 0.0499093  -0.16533941  0.04555268  0.35030725] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0499093  -0.16533941  0.04555268  0.35030725]\n",
      "Action  0\n",
      "Observation  [ 0.04660252 -0.3610786   0.05255883  0.65699932] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04660252 -0.3610786   0.05255883  0.65699932]\n",
      "Action  1\n",
      "Observation  [ 0.03938094 -0.16672616  0.06569881  0.3813184 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03938094 -0.16672616  0.06569881  0.3813184 ]\n",
      "Action  1\n",
      "Observation  [ 0.03604642  0.02740438  0.07332518  0.11005239] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03604642  0.02740438  0.07332518  0.11005239]\n",
      "Action  0\n",
      "Observation  [ 0.03659451 -0.16868753  0.07552623  0.42493816] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03659451 -0.16868753  0.07552623  0.42493816]\n",
      "Action  0\n",
      "Observation  [ 0.03322076 -0.36479353  0.08402499  0.7404422 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03322076 -0.36479353  0.08402499  0.7404422 ]\n",
      "Action  0\n",
      "Observation  [ 0.02592489 -0.56096887  0.09883384  1.05834132] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02592489 -0.56096887  0.09883384  1.05834132]\n",
      "Action  1\n",
      "Observation  [ 0.01470551 -0.36728528  0.12000066  0.7982436 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01470551 -0.36728528  0.12000066  0.7982436 ]\n",
      "Action  0\n",
      "Observation  [ 0.0073598  -0.56383122  0.13596553  1.12613792] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0073598  -0.56383122  0.13596553  1.12613792]\n",
      "Action  1\n",
      "Observation  [-0.00391682 -0.37072717  0.15848829  0.87900592] , reward  1.0 , done  False , info  {}\n",
      "[-0.00391682 -0.37072717  0.15848829  0.87900592]\n",
      "Action  1\n",
      "Observation  [-0.01133136 -0.17807246  0.17606841  0.6400464 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.01133136 -0.17807246  0.17606841  0.6400464 ]\n",
      "Action  1\n",
      "Observation  [-0.01489281  0.0142146   0.18886934  0.40757204] , reward  1.0 , done  False , info  {}\n",
      "[-0.01489281  0.0142146   0.18886934  0.40757204]\n",
      "Action  0\n",
      "Observation  [-0.01460852 -0.18301293  0.19702078  0.75335048] , reward  1.0 , done  False , info  {}\n",
      "[-0.01460852 -0.18301293  0.19702078  0.75335048]\n",
      "Action  1\n",
      "Observation  [-0.01826878  0.00892625  0.21208779  0.52856166] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 20 timesteps\n",
      "[-0.02812671  0.03827369  0.03121401 -0.00718929]\n",
      "Action  0\n",
      "Observation  [-0.02736124 -0.15728168  0.03107022  0.29517632] , reward  1.0 , done  False , info  {}\n",
      "[-0.02736124 -0.15728168  0.03107022  0.29517632]\n",
      "Action  0\n",
      "Observation  [-0.03050687 -0.35283249  0.03697375  0.59749413] , reward  1.0 , done  False , info  {}\n",
      "[-0.03050687 -0.35283249  0.03697375  0.59749413]\n",
      "Action  0\n",
      "Observation  [-0.03756352 -0.54845178  0.04892363  0.90159033] , reward  1.0 , done  False , info  {}\n",
      "[-0.03756352 -0.54845178  0.04892363  0.90159033]\n",
      "Action  0\n",
      "Observation  [-0.04853255 -0.74420121  0.06695544  1.20924096] , reward  1.0 , done  False , info  {}\n",
      "[-0.04853255 -0.74420121  0.06695544  1.20924096]\n",
      "Action  0\n",
      "Observation  [-0.06341658 -0.94012092  0.09114025  1.52213222] , reward  1.0 , done  False , info  {}\n",
      "[-0.06341658 -0.94012092  0.09114025  1.52213222]\n",
      "Action  1\n",
      "Observation  [-0.082219   -0.74621063  0.1215829   1.25923215] , reward  1.0 , done  False , info  {}\n",
      "[-0.082219   -0.74621063  0.1215829   1.25923215]\n",
      "Action  0\n",
      "Observation  [-0.09714321 -0.94266006  0.14676754  1.58738835] , reward  1.0 , done  False , info  {}\n",
      "[-0.09714321 -0.94266006  0.14676754  1.58738835]\n",
      "Action  0\n",
      "Observation  [-0.11599641 -1.13918972  0.17851531  1.92200841] , reward  1.0 , done  False , info  {}\n",
      "[-0.11599641 -1.13918972  0.17851531  1.92200841]\n",
      "Action  0\n",
      "Observation  [-0.13878021 -1.33572441  0.21695548  2.26433076] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 9 timesteps\n",
      "[-0.03985777 -0.03554137 -0.01980949 -0.00392773]\n",
      "Action  1\n",
      "Observation  [-0.0405686   0.15985899 -0.01988805 -0.30279438] , reward  1.0 , done  False , info  {}\n",
      "[-0.0405686   0.15985899 -0.01988805 -0.30279438]\n",
      "Action  1\n",
      "Observation  [-0.03737142  0.35525866 -0.02594393 -0.60168262] , reward  1.0 , done  False , info  {}\n",
      "[-0.03737142  0.35525866 -0.02594393 -0.60168262]\n",
      "Action  0\n",
      "Observation  [-0.03026624  0.16050904 -0.03797759 -0.31728316] , reward  1.0 , done  False , info  {}\n",
      "[-0.03026624  0.16050904 -0.03797759 -0.31728316]\n",
      "Action  1\n",
      "Observation  [-0.02705606  0.35615076 -0.04432325 -0.62169686] , reward  1.0 , done  False , info  {}\n",
      "[-0.02705606  0.35615076 -0.04432325 -0.62169686]\n",
      "Action  0\n",
      "Observation  [-0.01993305  0.16167486 -0.05675719 -0.34329628] , reward  1.0 , done  False , info  {}\n",
      "[-0.01993305  0.16167486 -0.05675719 -0.34329628]\n",
      "Action  1\n",
      "Observation  [-0.01669955  0.3575564  -0.06362311 -0.65332312] , reward  1.0 , done  False , info  {}\n",
      "[-0.01669955  0.3575564  -0.06362311 -0.65332312]\n",
      "Action  0\n",
      "Observation  [-0.00954842  0.1633754  -0.07668958 -0.38133351] , reward  1.0 , done  False , info  {}\n",
      "[-0.00954842  0.1633754  -0.07668958 -0.38133351]\n",
      "Action  1\n",
      "Observation  [-0.00628092  0.35949778 -0.08431625 -0.69717706] , reward  1.0 , done  False , info  {}\n",
      "[-0.00628092  0.35949778 -0.08431625 -0.69717706]\n",
      "Action  1\n",
      "Observation  [  9.09040357e-04   5.55681484e-01  -9.82597865e-02  -1.01516681e+00] , reward  1.0 , done  False , info  {}\n",
      "[  9.09040357e-04   5.55681484e-01  -9.82597865e-02  -1.01516681e+00]\n",
      "Action  0\n",
      "Observation  [ 0.01202267  0.36199743 -0.11856312 -0.75488402] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01202267  0.36199743 -0.11856312 -0.75488402]\n",
      "Action  1\n",
      "Observation  [ 0.01926262  0.55853688 -0.1336608  -1.08239948] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01926262  0.55853688 -0.1336608  -1.08239948]\n",
      "Action  0\n",
      "Observation  [ 0.03043336  0.36540771 -0.15530879 -0.83446897] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03043336  0.36540771 -0.15530879 -0.83446897]\n",
      "Action  1\n",
      "Observation  [ 0.03774151  0.56227162 -0.17199817 -1.17168804] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03774151  0.56227162 -0.17199817 -1.17168804]\n",
      "Action  0\n",
      "Observation  [ 0.04898694  0.36975142 -0.19543193 -0.93748728] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04898694  0.36975142 -0.19543193 -0.93748728]\n",
      "Action  0\n",
      "Observation  [ 0.05638197  0.17772477 -0.21418168 -0.7120224 ] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 15 timesteps\n",
      "[ 0.0352267  -0.00156879  0.00837339  0.04830306]\n",
      "Action  1\n",
      "Observation  [ 0.03519532  0.1934321   0.00933946 -0.24172627] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03519532  0.1934321   0.00933946 -0.24172627]\n",
      "Action  0\n",
      "Observation  [ 0.03906396 -0.001822    0.00450493  0.05388787] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03906396 -0.001822    0.00450493  0.05388787]\n",
      "Action  0\n",
      "Observation  [ 0.03902752 -0.19700826  0.00558269  0.34798873] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03902752 -0.19700826  0.00558269  0.34798873]\n",
      "Action  0\n",
      "Observation  [ 0.03508736 -0.39220917  0.01254246  0.64242683] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03508736 -0.39220917  0.01254246  0.64242683]\n",
      "Action  1\n",
      "Observation  [ 0.02724318 -0.19726427  0.025391    0.35371987] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02724318 -0.19726427  0.025391    0.35371987]\n",
      "Action  0\n",
      "Observation  [ 0.02329789 -0.39273789  0.0324654   0.65429994] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02329789 -0.39273789  0.0324654   0.65429994]\n",
      "Action  1\n",
      "Observation  [ 0.01544313 -0.19808266  0.0455514   0.37201411] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01544313 -0.19808266  0.0455514   0.37201411]\n",
      "Action  0\n",
      "Observation  [ 0.01148148 -0.39382114  0.05299168  0.67870475] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01148148 -0.39382114  0.05299168  0.67870475]\n",
      "Action  0\n",
      "Observation  [ 0.00360506 -0.58963764  0.06656577  0.98758946] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00360506 -0.58963764  0.06656577  0.98758946]\n",
      "Action  0\n",
      "Observation  [-0.0081877  -0.7855847   0.08631756  1.30041499] , reward  1.0 , done  False , info  {}\n",
      "[-0.0081877  -0.7855847   0.08631756  1.30041499]\n",
      "Action  1\n",
      "Observation  [-0.02389939 -0.59165776  0.11232586  1.03595344] , reward  1.0 , done  False , info  {}\n",
      "[-0.02389939 -0.59165776  0.11232586  1.03595344]\n",
      "Action  0\n",
      "Observation  [-0.03573255 -0.78807917  0.13304493  1.36168322] , reward  1.0 , done  False , info  {}\n",
      "[-0.03573255 -0.78807917  0.13304493  1.36168322]\n",
      "Action  0\n",
      "Observation  [-0.05149413 -0.98459376  0.16027859  1.69284999] , reward  1.0 , done  False , info  {}\n",
      "[-0.05149413 -0.98459376  0.16027859  1.69284999]\n",
      "Action  1\n",
      "Observation  [-0.071186   -0.79164466  0.19413559  1.45405634] , reward  1.0 , done  False , info  {}\n",
      "[-0.071186   -0.79164466  0.19413559  1.45405634]\n",
      "Action  1\n",
      "Observation  [-0.0870189  -0.59936316  0.22321672  1.22777018] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 15 timesteps\n",
      "[ 0.00913084  0.00165989  0.0492992  -0.04534199]\n",
      "Action  1\n",
      "Observation  [ 0.00916404  0.19604152  0.04839236 -0.3220721 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00916404  0.19604152  0.04839236 -0.3220721 ]\n",
      "Action  0\n",
      "Observation  [ 0.01308487  0.00026505  0.04195092 -0.01452938] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01308487  0.00026505  0.04195092 -0.01452938]\n",
      "Action  1\n",
      "Observation  [ 0.01309017  0.19476105  0.04166033 -0.29368674] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01309017  0.19476105  0.04166033 -0.29368674]\n",
      "Action  0\n",
      "Observation  [ 0.01698539 -0.00092934  0.0357866   0.01183874] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01698539 -0.00092934  0.0357866   0.01183874]\n",
      "Action  1\n",
      "Observation  [ 0.01696681  0.1936616   0.03602337 -0.26934176] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01696681  0.1936616   0.03602337 -0.26934176]\n",
      "Action  1\n",
      "Observation  [ 0.02084004  0.38825145  0.03063654 -0.55044859] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02084004  0.38825145  0.03063654 -0.55044859]\n",
      "Action  1\n",
      "Observation  [ 0.02860507  0.58292999  0.01962756 -0.83332363] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02860507  0.58292999  0.01962756 -0.83332363]\n",
      "Action  1\n",
      "Observation  [ 0.04026367  0.77777833  0.00296109 -1.11976972] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04026367  0.77777833  0.00296109 -1.11976972]\n",
      "Action  1\n",
      "Observation  [ 0.05581923  0.97286131 -0.0194343  -1.41152235] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05581923  0.97286131 -0.0194343  -1.41152235]\n",
      "Action  0\n",
      "Observation  [ 0.07527646  0.77798562 -0.04766475 -1.12497733] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07527646  0.77798562 -0.04766475 -1.12497733]\n",
      "Action  0\n",
      "Observation  [ 0.09083617  0.58351969 -0.0701643  -0.84761786] , reward  1.0 , done  False , info  {}\n",
      "[ 0.09083617  0.58351969 -0.0701643  -0.84761786]\n",
      "Action  1\n",
      "Observation  [ 0.10250657  0.77952497 -0.08711665 -1.16151376] , reward  1.0 , done  False , info  {}\n",
      "[ 0.10250657  0.77952497 -0.08711665 -1.16151376]\n",
      "Action  0\n",
      "Observation  [ 0.11809707  0.58563896 -0.11034693 -0.89736756] , reward  1.0 , done  False , info  {}\n",
      "[ 0.11809707  0.58563896 -0.11034693 -0.89736756]\n",
      "Action  1\n",
      "Observation  [ 0.12980985  0.78206982 -0.12829428 -1.22259799] , reward  1.0 , done  False , info  {}\n",
      "[ 0.12980985  0.78206982 -0.12829428 -1.22259799]\n",
      "Action  0\n",
      "Observation  [ 0.14545124  0.58881257 -0.15274624 -0.97270965] , reward  1.0 , done  False , info  {}\n",
      "[ 0.14545124  0.58881257 -0.15274624 -0.97270965]\n",
      "Action  0\n",
      "Observation  [ 0.15722749  0.39603341 -0.17220043 -0.7316407 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.15722749  0.39603341 -0.17220043 -0.7316407 ]\n",
      "Action  1\n",
      "Observation  [ 0.16514816  0.59306366 -0.18683325 -1.07319207] , reward  1.0 , done  False , info  {}\n",
      "[ 0.16514816  0.59306366 -0.18683325 -1.07319207]\n",
      "Action  1\n",
      "Observation  [ 0.17700943  0.79009701 -0.20829709 -1.41820873] , reward  1.0 , done  False , info  {}\n",
      "[ 0.17700943  0.79009701 -0.20829709 -1.41820873]\n",
      "Action  0\n",
      "Observation  [ 0.19281138  0.59807176 -0.23666126 -1.1971944 ] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 19 timesteps\n",
      "[-0.00862441 -0.04152339  0.00339814 -0.03820414]\n",
      "Action  0\n",
      "Observation  [-0.00945488 -0.23669391  0.00263406  0.255549  ] , reward  1.0 , done  False , info  {}\n",
      "[-0.00945488 -0.23669391  0.00263406  0.255549  ]\n",
      "Action  0\n",
      "Observation  [-0.01418876 -0.43185337  0.00774504  0.54906159] , reward  1.0 , done  False , info  {}\n",
      "[-0.01418876 -0.43185337  0.00774504  0.54906159]\n",
      "Action  1\n",
      "Observation  [-0.02282583 -0.23684106  0.01872627  0.25882893] , reward  1.0 , done  False , info  {}\n",
      "[-0.02282583 -0.23684106  0.01872627  0.25882893]\n",
      "Action  1\n",
      "Observation  [-0.02756265 -0.04199139  0.02390285 -0.02788914] , reward  1.0 , done  False , info  {}\n",
      "[-0.02756265 -0.04199139  0.02390285 -0.02788914]\n",
      "Action  0\n",
      "Observation  [-0.02840248 -0.23744782  0.02334507  0.27223853] , reward  1.0 , done  False , info  {}\n",
      "[-0.02840248 -0.23744782  0.02334507  0.27223853]\n",
      "Action  0\n",
      "Observation  [-0.03315143 -0.43289497  0.02878984  0.5721922 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.03315143 -0.43289497  0.02878984  0.5721922 ]\n",
      "Action  0\n",
      "Observation  [-0.04180933 -0.62840854  0.04023368  0.87380406] , reward  1.0 , done  False , info  {}\n",
      "[-0.04180933 -0.62840854  0.04023368  0.87380406]\n",
      "Action  0\n",
      "Observation  [-0.0543775  -0.82405375  0.05770977  1.1788599 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.0543775  -0.82405375  0.05770977  1.1788599 ]\n",
      "Action  0\n",
      "Observation  [-0.07085858 -1.01987564  0.08128696  1.489061  ] , reward  1.0 , done  False , info  {}\n",
      "[-0.07085858 -1.01987564  0.08128696  1.489061  ]\n",
      "Action  0\n",
      "Observation  [-0.09125609 -1.21588803  0.11106818  1.8059808 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.09125609 -1.21588803  0.11106818  1.8059808 ]\n",
      "Action  1\n",
      "Observation  [-0.11557385 -1.02216734  0.1471878   1.5497772 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.11557385 -1.02216734  0.1471878   1.5497772 ]\n",
      "Action  1\n",
      "Observation  [-0.1360172  -0.82908614  0.17818334  1.30640409] , reward  1.0 , done  False , info  {}\n",
      "[-0.1360172  -0.82908614  0.17818334  1.30640409]\n",
      "Action  1\n",
      "Observation  [-0.15259892 -0.63661316  0.20431143  1.07437481] , reward  1.0 , done  False , info  {}\n",
      "[-0.15259892 -0.63661316  0.20431143  1.07437481]\n",
      "Action  1\n",
      "Observation  [-0.16533118 -0.44469008  0.22579892  0.85212845] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 14 timesteps\n",
      "[ 0.03822744  0.01119697 -0.03390252 -0.04649902]\n",
      "Action  1\n",
      "Observation  [ 0.03845138  0.20678825 -0.0348325  -0.34968277] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03845138  0.20678825 -0.0348325  -0.34968277]\n",
      "Action  1\n",
      "Observation  [ 0.04258715  0.40238783 -0.04182616 -0.65314286] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04258715  0.40238783 -0.04182616 -0.65314286]\n",
      "Action  1\n",
      "Observation  [ 0.0506349   0.5980665  -0.05488902 -0.95869746] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0506349   0.5980665  -0.05488902 -0.95869746]\n",
      "Action  1\n",
      "Observation  [ 0.06259623  0.79388174 -0.07406297 -1.26810724] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06259623  0.79388174 -0.07406297 -1.26810724]\n",
      "Action  0\n",
      "Observation  [ 0.07847387  0.59977976 -0.09942511 -0.99950704] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07847387  0.59977976 -0.09942511 -0.99950704]\n",
      "Action  0\n",
      "Observation  [ 0.09046946  0.4061172  -0.11941525 -0.73963069] , reward  1.0 , done  False , info  {}\n",
      "[ 0.09046946  0.4061172  -0.11941525 -0.73963069]\n",
      "Action  0\n",
      "Observation  [ 0.09859181  0.21282875 -0.13420786 -0.48678747] , reward  1.0 , done  False , info  {}\n",
      "[ 0.09859181  0.21282875 -0.13420786 -0.48678747]\n",
      "Action  0\n",
      "Observation  [ 0.10284838  0.0198304  -0.14394361 -0.239232  ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.10284838  0.0198304  -0.14394361 -0.239232  ]\n",
      "Action  0\n",
      "Observation  [ 0.10324499 -0.1729733  -0.14872825  0.00480916] , reward  1.0 , done  False , info  {}\n",
      "[ 0.10324499 -0.1729733  -0.14872825  0.00480916]\n",
      "Action  0\n",
      "Observation  [ 0.09978552 -0.36568397 -0.14863207  0.24711889] , reward  1.0 , done  False , info  {}\n",
      "[ 0.09978552 -0.36568397 -0.14863207  0.24711889]\n",
      "Action  0\n",
      "Observation  [ 0.09247184 -0.55840516 -0.14368969  0.48947631] , reward  1.0 , done  False , info  {}\n",
      "[ 0.09247184 -0.55840516 -0.14368969  0.48947631]\n",
      "Action  1\n",
      "Observation  [ 0.08130374 -0.36157952 -0.13390017  0.15518091] , reward  1.0 , done  False , info  {}\n",
      "[ 0.08130374 -0.36157952 -0.13390017  0.15518091]\n",
      "Action  0\n",
      "Observation  [ 0.07407215 -0.5545555  -0.13079655  0.4028047 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07407215 -0.5545555  -0.13079655  0.4028047 ]\n",
      "Action  1\n",
      "Observation  [ 0.06298104 -0.35784454 -0.12274045  0.07191399] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06298104 -0.35784454 -0.12274045  0.07191399]\n",
      "Action  0\n",
      "Observation  [ 0.05582415 -0.55101259 -0.12130217  0.32349106] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05582415 -0.55101259 -0.12130217  0.32349106]\n",
      "Action  1\n",
      "Observation  [ 0.0448039  -0.35439101 -0.11483235 -0.00484956] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0448039  -0.35439101 -0.11483235 -0.00484956]\n",
      "Action  0\n",
      "Observation  [ 0.03771608 -0.54769485 -0.11492934  0.24950998] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03771608 -0.54769485 -0.11492934  0.24950998]\n",
      "Action  1\n",
      "Observation  [ 0.02676218 -0.35113522 -0.10993914 -0.07709926] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02676218 -0.35113522 -0.10993914 -0.07709926]\n",
      "Action  1\n",
      "Observation  [ 0.01973948 -0.15462298 -0.11148113 -0.40234507] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01973948 -0.15462298 -0.11148113 -0.40234507]\n",
      "Action  0\n",
      "Observation  [ 0.01664702 -0.34800179 -0.11952803 -0.14678509] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01664702 -0.34800179 -0.11952803 -0.14678509]\n",
      "Action  1\n",
      "Observation  [ 0.00968698 -0.15138898 -0.12246373 -0.47465769] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00968698 -0.15138898 -0.12246373 -0.47465769]\n",
      "Action  1\n",
      "Observation  [ 0.0066592   0.04523023 -0.13195689 -0.8032921 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0066592   0.04523023 -0.13195689 -0.8032921 ]\n",
      "Action  0\n",
      "Observation  [ 0.00756381 -0.1478594  -0.14802273 -0.55485849] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00756381 -0.1478594  -0.14802273 -0.55485849]\n",
      "Action  1\n",
      "Observation  [ 0.00460662  0.04899694 -0.1591199  -0.89027388] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00460662  0.04899694 -0.1591199  -0.89027388]\n",
      "Action  0\n",
      "Observation  [ 0.00558656 -0.14364994 -0.17692538 -0.65153817] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00558656 -0.14364994 -0.17692538 -0.65153817]\n",
      "Action  0\n",
      "Observation  [ 0.00271356 -0.33592422 -0.18995614 -0.41937411] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00271356 -0.33592422 -0.18995614 -0.41937411]\n",
      "Action  0\n",
      "Observation  [-0.00400493 -0.52791854 -0.19834362 -0.19207473] , reward  1.0 , done  False , info  {}\n",
      "[-0.00400493 -0.52791854 -0.19834362 -0.19207473]\n",
      "Action  1\n",
      "Observation  [-0.0145633  -0.33059376 -0.20218512 -0.5401903 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.0145633  -0.33059376 -0.20218512 -0.5401903 ]\n",
      "Action  1\n",
      "Observation  [-0.02117517 -0.13328989 -0.21298892 -0.88915578] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 29 timesteps\n",
      "[ 0.03441542 -0.00507124 -0.01598899  0.04747105]\n",
      "Action  0\n",
      "Observation  [ 0.034314   -0.19996031 -0.01503957  0.33506674] , reward  1.0 , done  False , info  {}\n",
      "[ 0.034314   -0.19996031 -0.01503957  0.33506674]\n",
      "Action  0\n",
      "Observation  [ 0.03031479 -0.39486503 -0.00833823  0.62296928] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03031479 -0.39486503 -0.00833823  0.62296928]\n",
      "Action  1\n",
      "Observation  [ 0.02241749 -0.19962765  0.00412115  0.32767198] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02241749 -0.19962765  0.00412115  0.32767198]\n",
      "Action  0\n",
      "Observation  [ 0.01842494 -0.39480803  0.01067459  0.62165168] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01842494 -0.39480803  0.01067459  0.62165168]\n",
      "Action  0\n",
      "Observation  [ 0.01052878 -0.5900774   0.02310763  0.91767733] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01052878 -0.5900774   0.02310763  0.91767733]\n",
      "Action  1\n",
      "Observation  [-0.00127277 -0.39527536  0.04146117  0.63234531] , reward  1.0 , done  False , info  {}\n",
      "[-0.00127277 -0.39527536  0.04146117  0.63234531]\n",
      "Action  0\n",
      "Observation  [-0.00917828 -0.59095047  0.05410808  0.93779183] , reward  1.0 , done  False , info  {}\n",
      "[-0.00917828 -0.59095047  0.05410808  0.93779183]\n",
      "Action  1\n",
      "Observation  [-0.02099729 -0.39659819  0.07286391  0.66259006] , reward  1.0 , done  False , info  {}\n",
      "[-0.02099729 -0.39659819  0.07286391  0.66259006]\n",
      "Action  1\n",
      "Observation  [-0.02892925 -0.20256158  0.08611572  0.39371048] , reward  1.0 , done  False , info  {}\n",
      "[-0.02892925 -0.20256158  0.08611572  0.39371048]\n",
      "Action  0\n",
      "Observation  [-0.03298049 -0.39879324  0.09398993  0.71225395] , reward  1.0 , done  False , info  {}\n",
      "[-0.03298049 -0.39879324  0.09398993  0.71225395]\n",
      "Action  0\n",
      "Observation  [-0.04095635 -0.59508226  0.108235    1.03298028] , reward  1.0 , done  False , info  {}\n",
      "[-0.04095635 -0.59508226  0.108235    1.03298028]\n",
      "Action  1\n",
      "Observation  [-0.052858   -0.40155325  0.12889461  0.77614447] , reward  1.0 , done  False , info  {}\n",
      "[-0.052858   -0.40155325  0.12889461  0.77614447]\n",
      "Action  1\n",
      "Observation  [-0.06088906 -0.20841737  0.1444175   0.52663404] , reward  1.0 , done  False , info  {}\n",
      "[-0.06088906 -0.20841737  0.1444175   0.52663404]\n",
      "Action  1\n",
      "Observation  [-0.06505741 -0.01559119  0.15495018  0.28271709] , reward  1.0 , done  False , info  {}\n",
      "[-0.06505741 -0.01559119  0.15495018  0.28271709]\n",
      "Action  0\n",
      "Observation  [-0.06536923 -0.21254467  0.16060452  0.6199811 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.06536923 -0.21254467  0.16060452  0.6199811 ]\n",
      "Action  0\n",
      "Observation  [-0.06962012 -0.40950201  0.17300414  0.95863009] , reward  1.0 , done  False , info  {}\n",
      "[-0.06962012 -0.40950201  0.17300414  0.95863009]\n",
      "Action  1\n",
      "Observation  [-0.07781017 -0.21707488  0.19217675  0.72490807] , reward  1.0 , done  False , info  {}\n",
      "[-0.07781017 -0.21707488  0.19217675  0.72490807]\n",
      "Action  0\n",
      "Observation  [-0.08215166 -0.41426131  0.20667491  1.07139549] , reward  1.0 , done  False , info  {}\n",
      "[-0.08215166 -0.41426131  0.20667491  1.07139549]\n",
      "Action  0\n",
      "Observation  [-0.09043689 -0.61142629  0.22810282  1.42117981] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 19 timesteps\n",
      "[ 0.02043427  0.0412406  -0.02926048 -0.02643335]\n",
      "Action  1\n",
      "Observation  [ 0.02125908  0.23676968 -0.02978915 -0.32820278] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02125908  0.23676968 -0.02978915 -0.32820278]\n",
      "Action  1\n",
      "Observation  [ 0.02599448  0.43230278 -0.0363532  -0.63012901] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02599448  0.43230278 -0.0363532  -0.63012901]\n",
      "Action  0\n",
      "Observation  [ 0.03464053  0.23770646 -0.04895578 -0.34911287] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03464053  0.23770646 -0.04895578 -0.34911287]\n",
      "Action  0\n",
      "Observation  [ 0.03939466  0.04331373 -0.05593804 -0.07226037] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03939466  0.04331373 -0.05593804 -0.07226037]\n",
      "Action  0\n",
      "Observation  [ 0.04026094 -0.15096351 -0.05738325  0.20226247] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04026094 -0.15096351 -0.05738325  0.20226247]\n",
      "Action  0\n",
      "Observation  [ 0.03724167 -0.34521985 -0.053338    0.47630594] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03724167 -0.34521985 -0.053338    0.47630594]\n",
      "Action  1\n",
      "Observation  [ 0.03033727 -0.14938693 -0.04381188  0.16730038] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03033727 -0.14938693 -0.04381188  0.16730038]\n",
      "Action  1\n",
      "Observation  [ 0.02734953  0.04633388 -0.04046587 -0.13887569] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02734953  0.04633388 -0.04046587 -0.13887569]\n",
      "Action  1\n",
      "Observation  [ 0.02827621  0.24201134 -0.04324338 -0.44404532] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02827621  0.24201134 -0.04324338 -0.44404532]\n",
      "Action  1\n",
      "Observation  [ 0.03311644  0.43771762 -0.05212429 -0.75003989] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03311644  0.43771762 -0.05212429 -0.75003989]\n",
      "Action  0\n",
      "Observation  [ 0.04187079  0.24335186 -0.06712509 -0.47420483] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04187079  0.24335186 -0.06712509 -0.47420483]\n",
      "Action  0\n",
      "Observation  [ 0.04673783  0.04923888 -0.07660918 -0.20341104] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04673783  0.04923888 -0.07660918 -0.20341104]\n",
      "Action  1\n",
      "Observation  [ 0.0477226   0.24536807 -0.08067741 -0.51924303] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0477226   0.24536807 -0.08067741 -0.51924303]\n",
      "Action  0\n",
      "Observation  [ 0.05262996  0.05146909 -0.09106227 -0.25303401] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05262996  0.05146909 -0.09106227 -0.25303401]\n",
      "Action  0\n",
      "Observation  [ 0.05365935 -0.14224257 -0.09612295  0.00959425] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05365935 -0.14224257 -0.09612295  0.00959425]\n",
      "Action  1\n",
      "Observation  [ 0.05081449  0.05411709 -0.09593106 -0.31180223] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05081449  0.05411709 -0.09593106 -0.31180223]\n",
      "Action  0\n",
      "Observation  [ 0.05189684 -0.13951659 -0.10216711 -0.05084765] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05189684 -0.13951659 -0.10216711 -0.05084765]\n",
      "Action  0\n",
      "Observation  [ 0.0491065  -0.33303646 -0.10318406  0.20793358] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0491065  -0.33303646 -0.10318406  0.20793358]\n",
      "Action  0\n",
      "Observation  [ 0.04244578 -0.52654321 -0.09902539  0.46636757] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04244578 -0.52654321 -0.09902539  0.46636757]\n",
      "Action  1\n",
      "Observation  [ 0.03191491 -0.33017177 -0.08969804  0.14418755] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03191491 -0.33017177 -0.08969804  0.14418755]\n",
      "Action  0\n",
      "Observation  [ 0.02531148 -0.52390228 -0.08681428  0.4072792 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02531148 -0.52390228 -0.08681428  0.4072792 ]\n",
      "Action  1\n",
      "Observation  [ 0.01483343 -0.32766351 -0.0786687   0.08853824] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01483343 -0.32766351 -0.0786687   0.08853824]\n",
      "Action  1\n",
      "Observation  [ 0.00828016 -0.13150723 -0.07689794 -0.22789093] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00828016 -0.13150723 -0.07689794 -0.22789093]\n",
      "Action  0\n",
      "Observation  [ 0.00565002 -0.32545084 -0.08145575  0.03957906] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00565002 -0.32545084 -0.08145575  0.03957906]\n",
      "Action  1\n",
      "Observation  [-0.000859   -0.12926101 -0.08066417 -0.27765144] , reward  1.0 , done  False , info  {}\n",
      "[-0.000859   -0.12926101 -0.08066417 -0.27765144]\n",
      "Action  1\n",
      "Observation  [-0.00344422  0.06691351 -0.0862172  -0.59464597] , reward  1.0 , done  False , info  {}\n",
      "[-0.00344422  0.06691351 -0.0862172  -0.59464597]\n",
      "Action  0\n",
      "Observation  [-0.00210595 -0.12690263 -0.09811012 -0.33031809] , reward  1.0 , done  False , info  {}\n",
      "[-0.00210595 -0.12690263 -0.09811012 -0.33031809]\n",
      "Action  0\n",
      "Observation  [-0.004644   -0.32050106 -0.10471648 -0.07011508] , reward  1.0 , done  False , info  {}\n",
      "[-0.004644   -0.32050106 -0.10471648 -0.07011508]\n",
      "Action  0\n",
      "Observation  [-0.01105402 -0.513978   -0.10611879  0.1877802 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.01105402 -0.513978   -0.10611879  0.1877802 ]\n",
      "Action  0\n",
      "Observation  [-0.02133358 -0.70743429 -0.10236318  0.44519186] , reward  1.0 , done  False , info  {}\n",
      "[-0.02133358 -0.70743429 -0.10236318  0.44519186]\n",
      "Action  0\n",
      "Observation  [-0.03548227 -0.90097033 -0.09345934  0.70393406] , reward  1.0 , done  False , info  {}\n",
      "[-0.03548227 -0.90097033 -0.09345934  0.70393406]\n",
      "Action  1\n",
      "Observation  [-0.05350168 -0.70468603 -0.07938066  0.38335546] , reward  1.0 , done  False , info  {}\n",
      "[-0.05350168 -0.70468603 -0.07938066  0.38335546]\n",
      "Action  1\n",
      "Observation  [-0.0675954  -0.50853204 -0.07171355  0.0667376 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.0675954  -0.50853204 -0.07171355  0.0667376 ]\n",
      "Action  1\n",
      "Observation  [-0.07776604 -0.3124591  -0.0703788  -0.24768157] , reward  1.0 , done  False , info  {}\n",
      "[-0.07776604 -0.3124591  -0.0703788  -0.24768157]\n",
      "Action  0\n",
      "Observation  [-0.08401522 -0.50650904 -0.07533243  0.02199847] , reward  1.0 , done  False , info  {}\n",
      "[-0.08401522 -0.50650904 -0.07533243  0.02199847]\n",
      "Action  1\n",
      "Observation  [-0.0941454  -0.31039211 -0.07489246 -0.29346939] , reward  1.0 , done  False , info  {}\n",
      "[-0.0941454  -0.31039211 -0.07489246 -0.29346939]\n",
      "Action  1\n",
      "Observation  [-0.10035324 -0.11428673 -0.08076185 -0.60880071] , reward  1.0 , done  False , info  {}\n",
      "[-0.10035324 -0.11428673 -0.08076185 -0.60880071]\n",
      "Action  0\n",
      "Observation  [-0.10263898 -0.30819229 -0.09293787 -0.34260858] , reward  1.0 , done  False , info  {}\n",
      "[-0.10263898 -0.30819229 -0.09293787 -0.34260858]\n",
      "Action  1\n",
      "Observation  [-0.10880282 -0.11187948 -0.09979004 -0.6630914 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.10880282 -0.11187948 -0.09979004 -0.6630914 ]\n",
      "Action  0\n",
      "Observation  [-0.11104041 -0.30548192 -0.11305187 -0.40342208] , reward  1.0 , done  False , info  {}\n",
      "[-0.11104041 -0.30548192 -0.11305187 -0.40342208]\n",
      "Action  0\n",
      "Observation  [-0.11715005 -0.49883418 -0.12112031 -0.14841159] , reward  1.0 , done  False , info  {}\n",
      "[-0.11715005 -0.49883418 -0.12112031 -0.14841159]\n",
      "Action  0\n",
      "Observation  [-0.12712674 -0.69203225 -0.12408854  0.10374007] , reward  1.0 , done  False , info  {}\n",
      "[-0.12712674 -0.69203225 -0.12408854  0.10374007]\n",
      "Action  0\n",
      "Observation  [-0.14096738 -0.88517761 -0.12201374  0.35484195] , reward  1.0 , done  False , info  {}\n",
      "[-0.14096738 -0.88517761 -0.12201374  0.35484195]\n",
      "Action  1\n",
      "Observation  [-0.15867093 -0.6885512  -0.1149169   0.02631194] , reward  1.0 , done  False , info  {}\n",
      "[-0.15867093 -0.6885512  -0.1149169   0.02631194]\n",
      "Action  1\n",
      "Observation  [-0.17244196 -0.49198482 -0.11439066 -0.30030414] , reward  1.0 , done  False , info  {}\n",
      "[-0.17244196 -0.49198482 -0.11439066 -0.30030414]\n",
      "Action  1\n",
      "Observation  [-0.18228165 -0.29543403 -0.12039674 -0.62676106] , reward  1.0 , done  False , info  {}\n",
      "[-0.18228165 -0.29543403 -0.12039674 -0.62676106]\n",
      "Action  1\n",
      "Observation  [-0.18819033 -0.09885557 -0.13293196 -0.95480541] , reward  1.0 , done  False , info  {}\n",
      "[-0.18819033 -0.09885557 -0.13293196 -0.95480541]\n",
      "Action  1\n",
      "Observation  [-0.19016745  0.0977798  -0.15202807 -1.28612325] , reward  1.0 , done  False , info  {}\n",
      "[-0.19016745  0.0977798  -0.15202807 -1.28612325]\n",
      "Action  0\n",
      "Observation  [-0.18821185 -0.09511597 -0.17775054 -1.04464117] , reward  1.0 , done  False , info  {}\n",
      "[-0.18821185 -0.09511597 -0.17775054 -1.04464117]\n",
      "Action  1\n",
      "Observation  [-0.19011417  0.10186297 -0.19864336 -1.38743807] , reward  1.0 , done  False , info  {}\n",
      "[-0.19011417  0.10186297 -0.19864336 -1.38743807]\n",
      "Action  0\n",
      "Observation  [-0.18807691 -0.09030752 -0.22639212 -1.16286865] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 51 timesteps\n",
      "[-0.03436449  0.00727433  0.04927777  0.0430936 ]\n",
      "Action  1\n",
      "Observation  [-0.034219    0.20165629  0.05013964 -0.2336436 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.034219    0.20165629  0.05013964 -0.2336436 ]\n",
      "Action  1\n",
      "Observation  [-0.03018588  0.39602731  0.04546677 -0.51009885] , reward  1.0 , done  False , info  {}\n",
      "[-0.03018588  0.39602731  0.04546677 -0.51009885]\n",
      "Action  1\n",
      "Observation  [-0.02226533  0.59048025  0.0352648  -0.78811419] , reward  1.0 , done  False , info  {}\n",
      "[-0.02226533  0.59048025  0.0352648  -0.78811419]\n",
      "Action  1\n",
      "Observation  [-0.01045573  0.78510053  0.01950251 -1.06949741] , reward  1.0 , done  False , info  {}\n",
      "[-0.01045573  0.78510053  0.01950251 -1.06949741]\n",
      "Action  1\n",
      "Observation  [ 0.00524628  0.9799592  -0.00188744 -1.35599645] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00524628  0.9799592  -0.00188744 -1.35599645]\n",
      "Action  0\n",
      "Observation  [ 0.02484547  0.78486098 -0.02900737 -1.06390455] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02484547  0.78486098 -0.02900737 -1.06390455]\n",
      "Action  0\n",
      "Observation  [ 0.04054269  0.59013476 -0.05028546 -0.78046507] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04054269  0.59013476 -0.05028546 -0.78046507]\n",
      "Action  1\n",
      "Observation  [ 0.05234538  0.7859106  -0.06589476 -1.08853532] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05234538  0.7859106  -0.06589476 -1.08853532]\n",
      "Action  1\n",
      "Observation  [ 0.06806359  0.98183657 -0.08766546 -1.4011455 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06806359  0.98183657 -0.08766546 -1.4011455 ]\n",
      "Action  0\n",
      "Observation  [ 0.08770033  0.78790652 -0.11568837 -1.13710815] , reward  1.0 , done  False , info  {}\n",
      "[ 0.08770033  0.78790652 -0.11568837 -1.13710815]\n",
      "Action  0\n",
      "Observation  [ 0.10345846  0.59447192 -0.13843054 -0.88283231] , reward  1.0 , done  False , info  {}\n",
      "[ 0.10345846  0.59447192 -0.13843054 -0.88283231]\n",
      "Action  1\n",
      "Observation  [ 0.11534789  0.79117492 -0.15608718 -1.21563298] , reward  1.0 , done  False , info  {}\n",
      "[ 0.11534789  0.79117492 -0.15608718 -1.21563298]\n",
      "Action  1\n",
      "Observation  [ 0.13117139  0.98792718 -0.18039984 -1.55287705] , reward  1.0 , done  False , info  {}\n",
      "[ 0.13117139  0.98792718 -0.18039984 -1.55287705]\n",
      "Action  1\n",
      "Observation  [ 0.15092994  1.18469495 -0.21145738 -1.89598934] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 14 timesteps\n",
      "[-0.00313094  0.03286864 -0.01470622  0.02429996]\n",
      "Action  1\n",
      "Observation  [-0.00247357  0.22819837 -0.01422022 -0.27298643] , reward  1.0 , done  False , info  {}\n",
      "[-0.00247357  0.22819837 -0.01422022 -0.27298643]\n",
      "Action  0\n",
      "Observation  [ 0.0020904   0.03328218 -0.01967995  0.01517769] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0020904   0.03328218 -0.01967995  0.01517769]\n",
      "Action  0\n",
      "Observation  [ 0.00275604 -0.16155209 -0.0193764   0.30158697] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00275604 -0.16155209 -0.0193764   0.30158697]\n",
      "Action  0\n",
      "Observation  [ -4.74998769e-04  -3.56392592e-01  -1.33446593e-02   5.88096556e-01] , reward  1.0 , done  False , info  {}\n",
      "[ -4.74998769e-04  -3.56392592e-01  -1.33446593e-02   5.88096556e-01]\n",
      "Action  0\n",
      "Observation  [-0.00760285 -0.55132515 -0.00158273  0.87654614] , reward  1.0 , done  False , info  {}\n",
      "[-0.00760285 -0.55132515 -0.00158273  0.87654614]\n",
      "Action  0\n",
      "Observation  [-0.01862935 -0.74642555  0.01594819  1.16873106] , reward  1.0 , done  False , info  {}\n",
      "[-0.01862935 -0.74642555  0.01594819  1.16873106]\n",
      "Action  0\n",
      "Observation  [-0.03355786 -0.9417513   0.03932282  1.46637099] , reward  1.0 , done  False , info  {}\n",
      "[-0.03355786 -0.9417513   0.03932282  1.46637099]\n",
      "Action  1\n",
      "Observation  [-0.05239289 -0.74713225  0.06865024  1.18622602] , reward  1.0 , done  False , info  {}\n",
      "[-0.05239289 -0.74713225  0.06865024  1.18622602]\n",
      "Action  1\n",
      "Observation  [-0.06733554 -0.55296443  0.09237476  0.91582766] , reward  1.0 , done  False , info  {}\n",
      "[-0.06733554 -0.55296443  0.09237476  0.91582766]\n",
      "Action  0\n",
      "Observation  [-0.07839482 -0.74920599  0.11069131  1.23605455] , reward  1.0 , done  False , info  {}\n",
      "[-0.07839482 -0.74920599  0.11069131  1.23605455]\n",
      "Action  0\n",
      "Observation  [-0.09337894 -0.9455625   0.1354124   1.56126358] , reward  1.0 , done  False , info  {}\n",
      "[-0.09337894 -0.9455625   0.1354124   1.56126358]\n",
      "Action  0\n",
      "Observation  [-0.11229019 -1.1420198   0.16663767  1.89294158] , reward  1.0 , done  False , info  {}\n",
      "[-0.11229019 -1.1420198   0.16663767  1.89294158]\n",
      "Action  0\n",
      "Observation  [-0.13513059 -1.33851218  0.2044965   2.23236251] , reward  1.0 , done  False , info  {}\n",
      "[-0.13513059 -1.33851218  0.2044965   2.23236251]\n",
      "Action  1\n",
      "Observation  [-0.16190083 -1.145836    0.24914375  2.00907413] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 14 timesteps\n",
      "[-0.03512879 -0.03250623  0.03724118  0.02948008]\n",
      "Action  1\n",
      "Observation  [-0.03577891  0.16206242  0.03783078 -0.25122416] , reward  1.0 , done  False , info  {}\n",
      "[-0.03577891  0.16206242  0.03783078 -0.25122416]\n",
      "Action  0\n",
      "Observation  [-0.03253766 -0.03357875  0.0328063   0.05314724] , reward  1.0 , done  False , info  {}\n",
      "[-0.03253766 -0.03357875  0.0328063   0.05314724]\n",
      "Action  0\n",
      "Observation  [-0.03320924 -0.22915537  0.03386925  0.35599763] , reward  1.0 , done  False , info  {}\n",
      "[-0.03320924 -0.22915537  0.03386925  0.35599763]\n",
      "Action  1\n",
      "Observation  [-0.03779235 -0.03453093  0.0409892   0.07418405] , reward  1.0 , done  False , info  {}\n",
      "[-0.03779235 -0.03453093  0.0409892   0.07418405]\n",
      "Action  0\n",
      "Observation  [-0.03848297 -0.2302158   0.04247288  0.37951226] , reward  1.0 , done  False , info  {}\n",
      "[-0.03848297 -0.2302158   0.04247288  0.37951226]\n",
      "Action  1\n",
      "Observation  [-0.04308728 -0.03572193  0.05006313  0.10051783] , reward  1.0 , done  False , info  {}\n",
      "[-0.04308728 -0.03572193  0.05006313  0.10051783]\n",
      "Action  1\n",
      "Observation  [-0.04380172  0.15864812  0.05207348 -0.17595954] , reward  1.0 , done  False , info  {}\n",
      "[-0.04380172  0.15864812  0.05207348 -0.17595954]\n",
      "Action  0\n",
      "Observation  [-0.04062876 -0.0371789   0.04855429  0.13268551] , reward  1.0 , done  False , info  {}\n",
      "[-0.04062876 -0.0371789   0.04855429  0.13268551]\n",
      "Action  0\n",
      "Observation  [-0.04137234 -0.23296152  0.051208    0.44028268] , reward  1.0 , done  False , info  {}\n",
      "[-0.04137234 -0.23296152  0.051208    0.44028268]\n",
      "Action  1\n",
      "Observation  [-0.04603157 -0.03860026  0.06001365  0.16417153] , reward  1.0 , done  False , info  {}\n",
      "[-0.04603157 -0.03860026  0.06001365  0.16417153]\n",
      "Action  1\n",
      "Observation  [-0.04680357  0.15561353  0.06329709 -0.10899127] , reward  1.0 , done  False , info  {}\n",
      "[-0.04680357  0.15561353  0.06329709 -0.10899127]\n",
      "Action  0\n",
      "Observation  [-0.0436913  -0.04035567  0.06111726  0.20297078] , reward  1.0 , done  False , info  {}\n",
      "[-0.0436913  -0.04035567  0.06111726  0.20297078]\n",
      "Action  1\n",
      "Observation  [-0.04449841  0.1538414   0.06517668 -0.06982366] , reward  1.0 , done  False , info  {}\n",
      "[-0.04449841  0.1538414   0.06517668 -0.06982366]\n",
      "Action  0\n",
      "Observation  [-0.04142159 -0.04215149  0.0637802   0.24268984] , reward  1.0 , done  False , info  {}\n",
      "[-0.04142159 -0.04215149  0.0637802   0.24268984]\n",
      "Action  0\n",
      "Observation  [-0.04226462 -0.23812375  0.068634    0.55478921] , reward  1.0 , done  False , info  {}\n",
      "[-0.04226462 -0.23812375  0.068634    0.55478921]\n",
      "Action  1\n",
      "Observation  [-0.04702709 -0.04402923  0.07972978  0.28449544] , reward  1.0 , done  False , info  {}\n",
      "[-0.04702709 -0.04402923  0.07972978  0.28449544]\n",
      "Action  1\n",
      "Observation  [-0.04790768  0.14987041  0.08541969  0.01798566] , reward  1.0 , done  False , info  {}\n",
      "[-0.04790768  0.14987041  0.08541969  0.01798566]\n",
      "Action  0\n",
      "Observation  [-0.04491027 -0.04636609  0.08577941  0.33635004] , reward  1.0 , done  False , info  {}\n",
      "[-0.04491027 -0.04636609  0.08577941  0.33635004]\n",
      "Action  0\n",
      "Observation  [-0.04583759 -0.24259737  0.09250641  0.65480294] , reward  1.0 , done  False , info  {}\n",
      "[-0.04583759 -0.24259737  0.09250641  0.65480294]\n",
      "Action  1\n",
      "Observation  [-0.05068954 -0.04887685  0.10560246  0.3926227 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.05068954 -0.04887685  0.10560246  0.3926227 ]\n",
      "Action  0\n",
      "Observation  [-0.05166707 -0.24532651  0.11345492  0.71664507] , reward  1.0 , done  False , info  {}\n",
      "[-0.05166707 -0.24532651  0.11345492  0.71664507]\n",
      "Action  0\n",
      "Observation  [-0.0565736  -0.44182064  0.12778782  1.04277558] , reward  1.0 , done  False , info  {}\n",
      "[-0.0565736  -0.44182064  0.12778782  1.04277558]\n",
      "Action  0\n",
      "Observation  [-0.06541002 -0.63838658  0.14864333  1.37268781] , reward  1.0 , done  False , info  {}\n",
      "[-0.06541002 -0.63838658  0.14864333  1.37268781]\n",
      "Action  1\n",
      "Observation  [-0.07817775 -0.44540259  0.17609709  1.12994429] , reward  1.0 , done  False , info  {}\n",
      "[-0.07817775 -0.44540259  0.17609709  1.12994429]\n",
      "Action  1\n",
      "Observation  [-0.0870858  -0.25296816  0.19869597  0.89726205] , reward  1.0 , done  False , info  {}\n",
      "[-0.0870858  -0.25296816  0.19869597  0.89726205]\n",
      "Action  1\n",
      "Observation  [-0.09214516 -0.06101361  0.21664121  0.67302832] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 26 timesteps\n",
      "[-0.03805726 -0.03032505  0.03290421 -0.002969  ]\n",
      "Action  0\n",
      "Observation  [-0.03866376 -0.22590307  0.03284483  0.29991132] , reward  1.0 , done  False , info  {}\n",
      "[-0.03866376 -0.22590307  0.03284483  0.29991132]\n",
      "Action  1\n",
      "Observation  [-0.04318182 -0.0312643   0.03884305  0.01776527] , reward  1.0 , done  False , info  {}\n",
      "[-0.04318182 -0.0312643   0.03884305  0.01776527]\n",
      "Action  1\n",
      "Observation  [-0.04380711  0.16327969  0.03919836 -0.26241361] , reward  1.0 , done  False , info  {}\n",
      "[-0.04380711  0.16327969  0.03919836 -0.26241361]\n",
      "Action  0\n",
      "Observation  [-0.04054152 -0.03237922  0.03395009  0.04237069] , reward  1.0 , done  False , info  {}\n",
      "[-0.04054152 -0.03237922  0.03395009  0.04237069]\n",
      "Action  0\n",
      "Observation  [-0.0411891  -0.22797115  0.0347975   0.34556892] , reward  1.0 , done  False , info  {}\n",
      "[-0.0411891  -0.22797115  0.0347975   0.34556892]\n",
      "Action  1\n",
      "Observation  [-0.04574852 -0.03336102  0.04170888  0.06405885] , reward  1.0 , done  False , info  {}\n",
      "[-0.04574852 -0.03336102  0.04170888  0.06405885]\n",
      "Action  1\n",
      "Observation  [-0.04641574  0.16113888  0.04299006 -0.21517842] , reward  1.0 , done  False , info  {}\n",
      "[-0.04641574  0.16113888  0.04299006 -0.21517842]\n",
      "Action  1\n",
      "Observation  [-0.04319297  0.35562071  0.03868649 -0.49399645] , reward  1.0 , done  False , info  {}\n",
      "[-0.04319297  0.35562071  0.03868649 -0.49399645]\n",
      "Action  1\n",
      "Observation  [-0.03608055  0.55017631  0.02880656 -0.77424049] , reward  1.0 , done  False , info  {}\n",
      "[-0.03608055  0.55017631  0.02880656 -0.77424049]\n",
      "Action  0\n",
      "Observation  [-0.02507703  0.35467017  0.01332175 -0.47263499] , reward  1.0 , done  False , info  {}\n",
      "[-0.02507703  0.35467017  0.01332175 -0.47263499]\n",
      "Action  1\n",
      "Observation  [-0.01798362  0.54960146  0.00386905 -0.76108951] , reward  1.0 , done  False , info  {}\n",
      "[-0.01798362  0.54960146  0.00386905 -0.76108951]\n",
      "Action  1\n",
      "Observation  [-0.00699159  0.7446699  -0.01135274 -1.05255248] , reward  1.0 , done  False , info  {}\n",
      "[-0.00699159  0.7446699  -0.01135274 -1.05255248]\n",
      "Action  0\n",
      "Observation  [ 0.0079018   0.54970031 -0.03240379 -0.76345458] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0079018   0.54970031 -0.03240379 -0.76345458]\n",
      "Action  0\n",
      "Observation  [ 0.01889581  0.35503928 -0.04767288 -0.48114137] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01889581  0.35503928 -0.04767288 -0.48114137]\n",
      "Action  1\n",
      "Observation  [ 0.0259966   0.5508006  -0.05729571 -0.78846025] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0259966   0.5508006  -0.05729571 -0.78846025]\n",
      "Action  1\n",
      "Observation  [ 0.03701261  0.74666074 -0.07306491 -1.09860408] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03701261  0.74666074 -0.07306491 -1.09860408]\n",
      "Action  1\n",
      "Observation  [ 0.05194582  0.94266444 -0.095037   -1.41328718] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05194582  0.94266444 -0.095037   -1.41328718]\n",
      "Action  1\n",
      "Observation  [ 0.07079911  1.138827   -0.12330274 -1.73410205] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07079911  1.138827   -0.12330274 -1.73410205]\n",
      "Action  1\n",
      "Observation  [ 0.09357565  1.33512133 -0.15798478 -2.06246733] , reward  1.0 , done  False , info  {}\n",
      "[ 0.09357565  1.33512133 -0.15798478 -2.06246733]\n",
      "Action  0\n",
      "Observation  [ 0.12027808  1.14192459 -0.19923413 -1.82253578] , reward  1.0 , done  False , info  {}\n",
      "[ 0.12027808  1.14192459 -0.19923413 -1.82253578]\n",
      "Action  0\n",
      "Observation  [ 0.14311657  0.9494951  -0.23568484 -1.59778946] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 21 timesteps\n",
      "[ 0.04641251 -0.01022718  0.0123254  -0.00426438]\n",
      "Action  1\n",
      "Observation  [ 0.04620797  0.18471586  0.01224012 -0.29303315] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04620797  0.18471586  0.01224012 -0.29303315]\n",
      "Action  0\n",
      "Observation  [ 0.04990229 -0.01057845  0.00637945  0.00348488] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04990229 -0.01057845  0.00637945  0.00348488]\n",
      "Action  1\n",
      "Observation  [ 0.04969072  0.18445143  0.00644915 -0.28717845] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04969072  0.18445143  0.00644915 -0.28717845]\n",
      "Action  0\n",
      "Observation  [ 0.05337975 -0.01076189  0.00070558  0.00753149] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05337975 -0.01076189  0.00070558  0.00753149]\n",
      "Action  1\n",
      "Observation  [ 0.05316451  0.18434993  0.00085621 -0.28492873] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05316451  0.18434993  0.00085621 -0.28492873]\n",
      "Action  1\n",
      "Observation  [ 0.05685151  0.37945966 -0.00484236 -0.57734149] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05685151  0.37945966 -0.00484236 -0.57734149]\n",
      "Action  0\n",
      "Observation  [ 0.0644407   0.18440591 -0.01638919 -0.28618795] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0644407   0.18440591 -0.01638919 -0.28618795]\n",
      "Action  1\n",
      "Observation  [ 0.06812882  0.37975772 -0.02211295 -0.58399451] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06812882  0.37975772 -0.02211295 -0.58399451]\n",
      "Action  1\n",
      "Observation  [ 0.07572397  0.57518235 -0.03379284 -0.88356047] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07572397  0.57518235 -0.03379284 -0.88356047]\n",
      "Action  1\n",
      "Observation  [ 0.08722762  0.7707465  -0.05146405 -1.18667243] , reward  1.0 , done  False , info  {}\n",
      "[ 0.08722762  0.7707465  -0.05146405 -1.18667243]\n",
      "Action  1\n",
      "Observation  [ 0.10264255  0.96649665 -0.0751975  -1.49503264] , reward  1.0 , done  False , info  {}\n",
      "[ 0.10264255  0.96649665 -0.0751975  -1.49503264]\n",
      "Action  1\n",
      "Observation  [ 0.12197248  1.16244824 -0.10509815 -1.81021662] , reward  1.0 , done  False , info  {}\n",
      "[ 0.12197248  1.16244824 -0.10509815 -1.81021662]\n",
      "Action  1\n",
      "Observation  [ 0.14522145  1.35857307 -0.14130248 -2.13362263] , reward  1.0 , done  False , info  {}\n",
      "[ 0.14522145  1.35857307 -0.14130248 -2.13362263]\n",
      "Action  0\n",
      "Observation  [ 0.17239291  1.16510602 -0.18397494 -1.8877192 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.17239291  1.16510602 -0.18397494 -1.8877192 ]\n",
      "Action  0\n",
      "Observation  [ 0.19569503  0.9723993  -0.22172932 -1.65732124] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 15 timesteps\n",
      "[ 0.00670101  0.01692842  0.04845078  0.04695223]\n",
      "Action  1\n",
      "Observation  [ 0.00703958  0.21132335  0.04938983 -0.23005901] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00703958  0.21132335  0.04938983 -0.23005901]\n",
      "Action  1\n",
      "Observation  [ 0.01126604  0.40570601  0.04478865 -0.50676274] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01126604  0.40570601  0.04478865 -0.50676274]\n",
      "Action  0\n",
      "Observation  [ 0.01938016  0.20998251  0.03465339 -0.20030845] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01938016  0.20998251  0.03465339 -0.20030845]\n",
      "Action  0\n",
      "Observation  [ 0.02357981  0.01438251  0.03064723  0.10310145] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02357981  0.01438251  0.03064723  0.10310145]\n",
      "Action  0\n",
      "Observation  [ 0.02386746 -0.18116494  0.03270925  0.40529376] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02386746 -0.18116494  0.03270925  0.40529376]\n",
      "Action  0\n",
      "Observation  [ 0.02024416 -0.37673511  0.04081513  0.70810691] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02024416 -0.37673511  0.04081513  0.70810691]\n",
      "Action  1\n",
      "Observation  [ 0.01270946 -0.18220159  0.05497727  0.42854597] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01270946 -0.18220159  0.05497727  0.42854597]\n",
      "Action  1\n",
      "Observation  [ 0.00906543  0.01210041  0.06354819  0.15368849] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00906543  0.01210041  0.06354819  0.15368849]\n",
      "Action  0\n",
      "Observation  [ 0.00930744 -0.18387117  0.06662196  0.4657231 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00930744 -0.18387117  0.06662196  0.4657231 ]\n",
      "Action  0\n",
      "Observation  [ 0.00563001 -0.37986811  0.07593642  0.77863867] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00563001 -0.37986811  0.07593642  0.77863867]\n",
      "Action  0\n",
      "Observation  [-0.00196735 -0.57594747  0.09150919  1.09421398] , reward  1.0 , done  False , info  {}\n",
      "[-0.00196735 -0.57594747  0.09150919  1.09421398]\n",
      "Action  0\n",
      "Observation  [-0.0134863  -0.77214786  0.11339347  1.41414937] , reward  1.0 , done  False , info  {}\n",
      "[-0.0134863  -0.77214786  0.11339347  1.41414937]\n",
      "Action  1\n",
      "Observation  [-0.02892925 -0.57859879  0.14167646  1.15895655] , reward  1.0 , done  False , info  {}\n",
      "[-0.02892925 -0.57859879  0.14167646  1.15895655]\n",
      "Action  0\n",
      "Observation  [-0.04050123 -0.77525358  0.16485559  1.49249688] , reward  1.0 , done  False , info  {}\n",
      "[-0.04050123 -0.77525358  0.16485559  1.49249688]\n",
      "Action  0\n",
      "Observation  [-0.0560063  -0.97195292  0.19470553  1.83179395] , reward  1.0 , done  False , info  {}\n",
      "[-0.0560063  -0.97195292  0.19470553  1.83179395]\n",
      "Action  0\n",
      "Observation  [-0.07544536 -1.16862513  0.23134141  2.17811041] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 16 timesteps\n",
      "[-0.03931424  0.04402913 -0.01247696 -0.02108779]\n",
      "Action  1\n",
      "Observation  [-0.03843366  0.23932777 -0.01289871 -0.31768108] , reward  1.0 , done  False , info  {}\n",
      "[-0.03843366  0.23932777 -0.01289871 -0.31768108]\n",
      "Action  1\n",
      "Observation  [-0.0336471   0.43463104 -0.01925233 -0.61440373] , reward  1.0 , done  False , info  {}\n",
      "[-0.0336471   0.43463104 -0.01925233 -0.61440373]\n",
      "Action  1\n",
      "Observation  [-0.02495448  0.63001665 -0.03154041 -0.91308765] , reward  1.0 , done  False , info  {}\n",
      "[-0.02495448  0.63001665 -0.03154041 -0.91308765]\n",
      "Action  0\n",
      "Observation  [-0.01235415  0.43533525 -0.04980216 -0.63048213] , reward  1.0 , done  False , info  {}\n",
      "[-0.01235415  0.43533525 -0.04980216 -0.63048213]\n",
      "Action  1\n",
      "Observation  [-0.00364744  0.63111545 -0.0624118  -0.9384241 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.00364744  0.63111545 -0.0624118  -0.9384241 ]\n",
      "Action  1\n",
      "Observation  [ 0.00897487  0.8270208  -0.08118029 -1.25004716] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00897487  0.8270208  -0.08118029 -1.25004716]\n",
      "Action  0\n",
      "Observation  [ 0.02551528  0.6330277  -0.10618123 -0.98385663] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02551528  0.6330277  -0.10618123 -0.98385663]\n",
      "Action  1\n",
      "Observation  [ 0.03817584  0.82939955 -0.12585836 -1.30791413] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03817584  0.82939955 -0.12585836 -1.30791413]\n",
      "Action  0\n",
      "Observation  [ 0.05476383  0.63607699 -0.15201664 -1.05712872] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05476383  0.63607699 -0.15201664 -1.05712872]\n",
      "Action  1\n",
      "Observation  [ 0.06748537  0.83285038 -0.17315922 -1.39340589] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06748537  0.83285038 -0.17315922 -1.39340589]\n",
      "Action  1\n",
      "Observation  [ 0.08414238  1.0296525  -0.20102734 -1.73484919] , reward  1.0 , done  False , info  {}\n",
      "[ 0.08414238  1.0296525  -0.20102734 -1.73484919]\n",
      "Action  0\n",
      "Observation  [ 0.10473543  0.83731134 -0.23572432 -1.5108523 ] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 12 timesteps\n",
      "[-0.02392825  0.04455562 -0.00511314  0.01909541]\n",
      "Action  0\n",
      "Observation  [-0.02303714 -0.15049263 -0.00473123  0.31016071] , reward  1.0 , done  False , info  {}\n",
      "[-0.02303714 -0.15049263 -0.00473123  0.31016071]\n",
      "Action  1\n",
      "Observation  [-0.02604699  0.04469641  0.00147198  0.01598945] , reward  1.0 , done  False , info  {}\n",
      "[-0.02604699  0.04469641  0.00147198  0.01598945]\n",
      "Action  1\n",
      "Observation  [-0.02515306  0.23979722  0.00179177 -0.27622869] , reward  1.0 , done  False , info  {}\n",
      "[-0.02515306  0.23979722  0.00179177 -0.27622869]\n",
      "Action  0\n",
      "Observation  [-0.02035712  0.04464975 -0.0037328   0.01701882] , reward  1.0 , done  False , info  {}\n",
      "[-0.02035712  0.04464975 -0.0037328   0.01701882]\n",
      "Action  1\n",
      "Observation  [-0.01946412  0.23982504 -0.00339243 -0.2768395 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.01946412  0.23982504 -0.00339243 -0.2768395 ]\n",
      "Action  0\n",
      "Observation  [-0.01466762  0.04475165 -0.00892922  0.01477152] , reward  1.0 , done  False , info  {}\n",
      "[-0.01466762  0.04475165 -0.00892922  0.01477152]\n",
      "Action  0\n",
      "Observation  [-0.01377259 -0.15024112 -0.00863379  0.30462385] , reward  1.0 , done  False , info  {}\n",
      "[-0.01377259 -0.15024112 -0.00863379  0.30462385]\n",
      "Action  0\n",
      "Observation  [-0.01677741 -0.34523897 -0.00254131  0.59457143] , reward  1.0 , done  False , info  {}\n",
      "[-0.01677741 -0.34523897 -0.00254131  0.59457143]\n",
      "Action  1\n",
      "Observation  [-0.02368219 -0.15008154  0.00935012  0.30108909] , reward  1.0 , done  False , info  {}\n",
      "[-0.02368219 -0.15008154  0.00935012  0.30108909]\n",
      "Action  1\n",
      "Observation  [-0.02668382  0.0449059   0.0153719   0.0113696 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.02668382  0.0449059   0.0153719   0.0113696 ]\n",
      "Action  1\n",
      "Observation  [-0.0257857   0.23980406  0.01559929 -0.27642394] , reward  1.0 , done  False , info  {}\n",
      "[-0.0257857   0.23980406  0.01559929 -0.27642394]\n",
      "Action  1\n",
      "Observation  [-0.02098962  0.43470003  0.01007081 -0.56414631] , reward  1.0 , done  False , info  {}\n",
      "[-0.02098962  0.43470003  0.01007081 -0.56414631]\n",
      "Action  1\n",
      "Observation  [-0.01229562  0.62967924 -0.00121211 -0.85363953] , reward  1.0 , done  False , info  {}\n",
      "[-0.01229562  0.62967924 -0.00121211 -0.85363953]\n",
      "Action  0\n",
      "Observation  [  2.97965327e-04   4.34573830e-01  -1.82849022e-02  -5.61337989e-01] , reward  1.0 , done  False , info  {}\n",
      "[  2.97965327e-04   4.34573830e-01  -1.82849022e-02  -5.61337989e-01]\n",
      "Action  1\n",
      "Observation  [ 0.00898944  0.62994756 -0.02951166 -0.85972505] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00898944  0.62994756 -0.02951166 -0.85972505]\n",
      "Action  0\n",
      "Observation  [ 0.02158839  0.43523973 -0.04670616 -0.57646565] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02158839  0.43523973 -0.04670616 -0.57646565]\n",
      "Action  0\n",
      "Observation  [ 0.03029319  0.24080251 -0.05823548 -0.29885451] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03029319  0.24080251 -0.05823548 -0.29885451]\n",
      "Action  1\n",
      "Observation  [ 0.03510924  0.43670413 -0.06421257 -0.60932035] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03510924  0.43670413 -0.06421257 -0.60932035]\n",
      "Action  0\n",
      "Observation  [ 0.04384332  0.24253581 -0.07639897 -0.33753364] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04384332  0.24253581 -0.07639897 -0.33753364]\n",
      "Action  1\n",
      "Observation  [ 0.04869404  0.43865709 -0.08314965 -0.6532969 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04869404  0.43865709 -0.08314965 -0.6532969 ]\n",
      "Action  0\n",
      "Observation  [ 0.05746718  0.24478541 -0.09621558 -0.38791193] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05746718  0.24478541 -0.09621558 -0.38791193]\n",
      "Action  0\n",
      "Observation  [ 0.06236289  0.05115149 -0.10397382 -0.12704818] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06236289  0.05115149 -0.10397382 -0.12704818]\n",
      "Action  0\n",
      "Observation  [ 0.06338592 -0.14233918 -0.10651479  0.13110717] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06338592 -0.14233918 -0.10651479  0.13110717]\n",
      "Action  1\n",
      "Observation  [ 0.06053913  0.05413455 -0.10389264 -0.19318937] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06053913  0.05413455 -0.10389264 -0.19318937]\n",
      "Action  1\n",
      "Observation  [ 0.06162182  0.25057741 -0.10775643 -0.51675435] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06162182  0.25057741 -0.10775643 -0.51675435]\n",
      "Action  1\n",
      "Observation  [ 0.06663337  0.44703852 -0.11809152 -0.84135589] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06663337  0.44703852 -0.11809152 -0.84135589]\n",
      "Action  1\n",
      "Observation  [ 0.07557414  0.64355741 -0.13491863 -1.16871944] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07557414  0.64355741 -0.13491863 -1.16871944]\n",
      "Action  0\n",
      "Observation  [ 0.08844529  0.45042343 -0.15829302 -0.92119704] , reward  1.0 , done  False , info  {}\n",
      "[ 0.08844529  0.45042343 -0.15829302 -0.92119704]\n",
      "Action  1\n",
      "Observation  [ 0.09745376  0.64728936 -0.17671696 -1.2591481 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.09745376  0.64728936 -0.17671696 -1.2591481 ]\n",
      "Action  1\n",
      "Observation  [ 0.11039955  0.84417611 -0.20189993 -1.60156358] , reward  1.0 , done  False , info  {}\n",
      "[ 0.11039955  0.84417611 -0.20189993 -1.60156358]\n",
      "Action  0\n",
      "Observation  [ 0.12728307  0.65193548 -0.2339312  -1.37801613] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 31 timesteps\n",
      "[-0.01932926  0.00900041  0.01607654 -0.03426054]\n",
      "Action  0\n",
      "Observation  [-0.01914925 -0.18634835  0.01539133  0.26345103] , reward  1.0 , done  False , info  {}\n",
      "[-0.01914925 -0.18634835  0.01539133  0.26345103]\n",
      "Action  1\n",
      "Observation  [-0.02287622  0.00855057  0.02066035 -0.02433784] , reward  1.0 , done  False , info  {}\n",
      "[-0.02287622  0.00855057  0.02066035 -0.02433784]\n",
      "Action  0\n",
      "Observation  [-0.02270521 -0.18686148  0.02017359  0.27479139] , reward  1.0 , done  False , info  {}\n",
      "[-0.02270521 -0.18686148  0.02017359  0.27479139]\n",
      "Action  1\n",
      "Observation  [-0.02644244  0.00796691  0.02566942 -0.0114611 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.02644244  0.00796691  0.02566942 -0.0114611 ]\n",
      "Action  0\n",
      "Observation  [-0.0262831  -0.18751359  0.0254402   0.28920903] , reward  1.0 , done  False , info  {}\n",
      "[-0.0262831  -0.18751359  0.0254402   0.28920903]\n",
      "Action  1\n",
      "Observation  [-0.03003337  0.00723653  0.03122438  0.00465699] , reward  1.0 , done  False , info  {}\n",
      "[-0.03003337  0.00723653  0.03122438  0.00465699]\n",
      "Action  0\n",
      "Observation  [-0.02988864 -0.18831899  0.03131752  0.30702576] , reward  1.0 , done  False , info  {}\n",
      "[-0.02988864 -0.18831899  0.03131752  0.30702576]\n",
      "Action  1\n",
      "Observation  [-0.03365502  0.00634303  0.03745803  0.02438176] , reward  1.0 , done  False , info  {}\n",
      "[-0.03365502  0.00634303  0.03745803  0.02438176]\n",
      "Action  0\n",
      "Observation  [-0.03352816 -0.18929552  0.03794567  0.32864382] , reward  1.0 , done  False , info  {}\n",
      "[-0.03352816 -0.18929552  0.03794567  0.32864382]\n",
      "Action  1\n",
      "Observation  [-0.03731407  0.00526626  0.04451854  0.04816458] , reward  1.0 , done  False , info  {}\n",
      "[-0.03731407  0.00526626  0.04451854  0.04816458]\n",
      "Action  1\n",
      "Observation  [-0.03720875  0.19972252  0.04548184 -0.23014668] , reward  1.0 , done  False , info  {}\n",
      "[-0.03720875  0.19972252  0.04548184 -0.23014668]\n",
      "Action  1\n",
      "Observation  [-0.0332143   0.39416603  0.0408789  -0.50814328] , reward  1.0 , done  False , info  {}\n",
      "[-0.0332143   0.39416603  0.0408789  -0.50814328]\n",
      "Action  1\n",
      "Observation  [-0.02533098  0.58868889  0.03071604 -0.78766876] , reward  1.0 , done  False , info  {}\n",
      "[-0.02533098  0.58868889  0.03071604 -0.78766876]\n",
      "Action  1\n",
      "Observation  [-0.0135572   0.78337576  0.01496266 -1.07053222] , reward  1.0 , done  False , info  {}\n",
      "[-0.0135572   0.78337576  0.01496266 -1.07053222]\n",
      "Action  1\n",
      "Observation  [ 0.00211032  0.97829669 -0.00644798 -1.35848203] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00211032  0.97829669 -0.00644798 -1.35848203]\n",
      "Action  0\n",
      "Observation  [ 0.02167625  0.7832562  -0.03361762 -1.06782306] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02167625  0.7832562  -0.03361762 -1.06782306]\n",
      "Action  1\n",
      "Observation  [ 0.03734137  0.97880636 -0.05497409 -1.37086429] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03734137  0.97880636 -0.05497409 -1.37086429]\n",
      "Action  0\n",
      "Observation  [ 0.0569175   0.78441343 -0.08239137 -1.09586964] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0569175   0.78441343 -0.08239137 -1.09586964]\n",
      "Action  1\n",
      "Observation  [ 0.07260577  0.98051804 -0.10430876 -1.41322438] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07260577  0.98051804 -0.10430876 -1.41322438]\n",
      "Action  0\n",
      "Observation  [ 0.09221613  0.78683197 -0.13257325 -1.15488555] , reward  1.0 , done  False , info  {}\n",
      "[ 0.09221613  0.78683197 -0.13257325 -1.15488555]\n",
      "Action  1\n",
      "Observation  [ 0.10795277  0.98340968 -0.15567096 -1.48602714] , reward  1.0 , done  False , info  {}\n",
      "[ 0.10795277  0.98340968 -0.15567096 -1.48602714]\n",
      "Action  0\n",
      "Observation  [ 0.12762096  0.79048969 -0.18539151 -1.24572905] , reward  1.0 , done  False , info  {}\n",
      "[ 0.12762096  0.79048969 -0.18539151 -1.24572905]\n",
      "Action  1\n",
      "Observation  [ 0.14343076  0.98744119 -0.21030609 -1.59028732] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 23 timesteps\n",
      "[ 0.00189643  0.03542658  0.0349299  -0.02601883]\n",
      "Action  0\n",
      "Observation  [ 0.00260497 -0.16017843  0.03440952  0.27747701] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00260497 -0.16017843  0.03440952  0.27747701]\n",
      "Action  1\n",
      "Observation  [-0.0005986   0.03443615  0.03995906 -0.00415766] , reward  1.0 , done  False , info  {}\n",
      "[-0.0005986   0.03443615  0.03995906 -0.00415766]\n",
      "Action  0\n",
      "Observation  [  9.01207286e-05  -1.61235409e-01   3.98759071e-02   3.00860226e-01] , reward  1.0 , done  False , info  {}\n",
      "[  9.01207286e-05  -1.61235409e-01   3.98759071e-02   3.00860226e-01]\n",
      "Action  0\n",
      "Observation  [-0.00313459 -0.35690236  0.04589311  0.60584775] , reward  1.0 , done  False , info  {}\n",
      "[-0.00313459 -0.35690236  0.04589311  0.60584775]\n",
      "Action  0\n",
      "Observation  [-0.01027263 -0.55263499  0.05801007  0.9126254 ] , reward  1.0 , done  False , info  {}\n",
      "[-0.01027263 -0.55263499  0.05801007  0.9126254 ]\n",
      "Action  1\n",
      "Observation  [-0.02132533 -0.3583438   0.07626257  0.63872424] , reward  1.0 , done  False , info  {}\n",
      "[-0.02132533 -0.3583438   0.07626257  0.63872424]\n",
      "Action  0\n",
      "Observation  [-0.02849221 -0.55444162  0.08903706  0.95441547] , reward  1.0 , done  False , info  {}\n",
      "[-0.02849221 -0.55444162  0.08903706  0.95441547]\n",
      "Action  1\n",
      "Observation  [-0.03958104 -0.36062296  0.10812537  0.69098144] , reward  1.0 , done  False , info  {}\n",
      "[-0.03958104 -0.36062296  0.10812537  0.69098144]\n",
      "Action  1\n",
      "Observation  [-0.0467935  -0.16715422  0.121945    0.43420002] , reward  1.0 , done  False , info  {}\n",
      "[-0.0467935  -0.16715422  0.121945    0.43420002]\n",
      "Action  1\n",
      "Observation  [-0.05013659  0.02604934  0.130629    0.18230984] , reward  1.0 , done  False , info  {}\n",
      "[-0.05013659  0.02604934  0.130629    0.18230984]\n",
      "Action  1\n",
      "Observation  [-0.0496156   0.2190837   0.13427519 -0.06647896] , reward  1.0 , done  False , info  {}\n",
      "[-0.0496156   0.2190837   0.13427519 -0.06647896]\n",
      "Action  0\n",
      "Observation  [-0.04523393  0.02231771  0.13294562  0.26537166] , reward  1.0 , done  False , info  {}\n",
      "[-0.04523393  0.02231771  0.13294562  0.26537166]\n",
      "Action  1\n",
      "Observation  [-0.04478757  0.21531639  0.13825305  0.01739922] , reward  1.0 , done  False , info  {}\n",
      "[-0.04478757  0.21531639  0.13825305  0.01739922]\n",
      "Action  0\n",
      "Observation  [-0.04048124  0.0185104   0.13860103  0.35030843] , reward  1.0 , done  False , info  {}\n",
      "[-0.04048124  0.0185104   0.13860103  0.35030843]\n",
      "Action  1\n",
      "Observation  [-0.04011104  0.21141708  0.1456072   0.10434166] , reward  1.0 , done  False , info  {}\n",
      "[-0.04011104  0.21141708  0.1456072   0.10434166]\n",
      "Action  0\n",
      "Observation  [-0.03588269  0.01454111  0.14769403  0.43918801] , reward  1.0 , done  False , info  {}\n",
      "[-0.03588269  0.01454111  0.14769403  0.43918801]\n",
      "Action  1\n",
      "Observation  [-0.03559187  0.20729784  0.15647779  0.19646507] , reward  1.0 , done  False , info  {}\n",
      "[-0.03559187  0.20729784  0.15647779  0.19646507]\n",
      "Action  1\n",
      "Observation  [-0.03144591  0.39987578  0.1604071  -0.04305559] , reward  1.0 , done  False , info  {}\n",
      "[-0.03144591  0.39987578  0.1604071  -0.04305559]\n",
      "Action  0\n",
      "Observation  [-0.0234484   0.20286065  0.15954598  0.29563099] , reward  1.0 , done  False , info  {}\n",
      "[-0.0234484   0.20286065  0.15954598  0.29563099]\n",
      "Action  1\n",
      "Observation  [-0.01939119  0.3953911   0.1654586   0.05721094] , reward  1.0 , done  False , info  {}\n",
      "[-0.01939119  0.3953911   0.1654586   0.05721094]\n",
      "Action  1\n",
      "Observation  [-0.01148336  0.5878018   0.16660282 -0.17904028] , reward  1.0 , done  False , info  {}\n",
      "[-0.01148336  0.5878018   0.16660282 -0.17904028]\n",
      "Action  1\n",
      "Observation  [  2.72671779e-04   7.80196433e-01   1.63022018e-01  -4.14881384e-01] , reward  1.0 , done  False , info  {}\n",
      "[  2.72671779e-04   7.80196433e-01   1.63022018e-01  -4.14881384e-01]\n",
      "Action  1\n",
      "Observation  [ 0.0158766   0.97267787  0.15472439 -0.652059  ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0158766   0.97267787  0.15472439 -0.652059  ]\n",
      "Action  1\n",
      "Observation  [ 0.03533016  1.16534521  0.14168321 -0.89229993] , reward  1.0 , done  False , info  {}\n",
      "[ 0.03533016  1.16534521  0.14168321 -0.89229993]\n",
      "Action  1\n",
      "Observation  [ 0.05863706  1.35829048  0.12383721 -1.13730214] , reward  1.0 , done  False , info  {}\n",
      "[ 0.05863706  1.35829048  0.12383721 -1.13730214]\n",
      "Action  1\n",
      "Observation  [ 0.08580287  1.55159459  0.10109117 -1.38872266] , reward  1.0 , done  False , info  {}\n",
      "[ 0.08580287  1.55159459  0.10109117 -1.38872266]\n",
      "Action  1\n",
      "Observation  [ 0.11683476  1.74532212  0.07331672 -1.64816017] , reward  1.0 , done  False , info  {}\n",
      "[ 0.11683476  1.74532212  0.07331672 -1.64816017]\n",
      "Action  0\n",
      "Observation  [ 0.15174121  1.5494235   0.04035351 -1.33356585] , reward  1.0 , done  False , info  {}\n",
      "[ 0.15174121  1.5494235   0.04035351 -1.33356585]\n",
      "Action  1\n",
      "Observation  [ 0.18272968  1.74401417  0.01368219 -1.61335352] , reward  1.0 , done  False , info  {}\n",
      "[ 0.18272968  1.74401417  0.01368219 -1.61335352]\n",
      "Action  0\n",
      "Observation  [ 0.21760996  1.54873344 -0.01858488 -1.31643741] , reward  1.0 , done  False , info  {}\n",
      "[ 0.21760996  1.54873344 -0.01858488 -1.31643741]\n",
      "Action  1\n",
      "Observation  [ 0.24858463  1.74408551 -0.04491362 -1.61487854] , reward  1.0 , done  False , info  {}\n",
      "[ 0.24858463  1.74408551 -0.04491362 -1.61487854]\n",
      "Action  0\n",
      "Observation  [ 0.28346634  1.54952129 -0.07721119 -1.33652668] , reward  1.0 , done  False , info  {}\n",
      "[ 0.28346634  1.54952129 -0.07721119 -1.33652668]\n",
      "Action  0\n",
      "Observation  [ 0.31445676  1.35545232 -0.10394173 -1.06896807] , reward  1.0 , done  False , info  {}\n",
      "[ 0.31445676  1.35545232 -0.10394173 -1.06896807]\n",
      "Action  0\n",
      "Observation  [ 0.34156581  1.16184717 -0.12532109 -0.81063156] , reward  1.0 , done  False , info  {}\n",
      "[ 0.34156581  1.16184717 -0.12532109 -0.81063156]\n",
      "Action  1\n",
      "Observation  [ 0.36480275  1.35844277 -0.14153372 -1.13996032] , reward  1.0 , done  False , info  {}\n",
      "[ 0.36480275  1.35844277 -0.14153372 -1.13996032]\n",
      "Action  1\n",
      "Observation  [ 0.39197161  1.55510235 -0.16433293 -1.47347217] , reward  1.0 , done  False , info  {}\n",
      "[ 0.39197161  1.55510235 -0.16433293 -1.47347217]\n",
      "Action  0\n",
      "Observation  [ 0.42307366  1.36232597 -0.19380237 -1.23630002] , reward  1.0 , done  False , info  {}\n",
      "[ 0.42307366  1.36232597 -0.19380237 -1.23630002]\n",
      "Action  1\n",
      "Observation  [ 0.45032018  1.55933638 -0.21852837 -1.58290519] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 38 timesteps\n",
      "[ 0.00918847  0.03330602  0.0284116   0.03524886]\n",
      "Action  1\n",
      "Observation  [ 0.00985459  0.22800927  0.02911657 -0.24833625] , reward  1.0 , done  False , info  {}\n",
      "[ 0.00985459  0.22800927  0.02911657 -0.24833625]\n",
      "Action  0\n",
      "Observation  [ 0.01441477  0.03248385  0.02414985  0.05338662] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01441477  0.03248385  0.02414985  0.05338662]\n",
      "Action  1\n",
      "Observation  [ 0.01506445  0.22725135  0.02521758 -0.23158007] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01506445  0.22725135  0.02521758 -0.23158007]\n",
      "Action  0\n",
      "Observation  [ 0.01960947  0.03177831  0.02058598  0.06894945] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01960947  0.03177831  0.02058598  0.06894945]\n",
      "Action  0\n",
      "Observation  [ 0.02024504 -0.16363264  0.02196497  0.36805561] , reward  1.0 , done  False , info  {}\n",
      "[ 0.02024504 -0.16363264  0.02196497  0.36805561]\n",
      "Action  1\n",
      "Observation  [ 0.01697239  0.03117043  0.02932608  0.08237868] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01697239  0.03117043  0.02932608  0.08237868]\n",
      "Action  0\n",
      "Observation  [ 0.0175958  -0.16435936  0.03097365  0.38416789] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0175958  -0.16435936  0.03097365  0.38416789]\n",
      "Action  1\n",
      "Observation  [ 0.01430861  0.03030946  0.03865701  0.10140951] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01430861  0.03030946  0.03865701  0.10140951]\n",
      "Action  1\n",
      "Observation  [ 0.0149148   0.2248567   0.0406852  -0.17883099] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0149148   0.2248567   0.0406852  -0.17883099]\n",
      "Action  1\n",
      "Observation  [ 0.01941193  0.41937353  0.03710858 -0.45840664] , reward  1.0 , done  False , info  {}\n",
      "[ 0.01941193  0.41937353  0.03710858 -0.45840664]\n",
      "Action  1\n",
      "Observation  [ 0.0277994   0.61395179  0.02794045 -0.73916567] , reward  1.0 , done  False , info  {}\n",
      "[ 0.0277994   0.61395179  0.02794045 -0.73916567]\n",
      "Action  0\n",
      "Observation  [ 0.04007844  0.41845539  0.01315714 -0.43782211] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04007844  0.41845539  0.01315714 -0.43782211]\n",
      "Action  1\n",
      "Observation  [ 0.04844755  0.61338866  0.00440069 -0.72632863] , reward  1.0 , done  False , info  {}\n",
      "[ 0.04844755  0.61338866  0.00440069 -0.72632863]\n",
      "Action  1\n",
      "Observation  [ 0.06071532  0.80844949 -0.01012588 -1.01762324] , reward  1.0 , done  False , info  {}\n",
      "[ 0.06071532  0.80844949 -0.01012588 -1.01762324]\n",
      "Action  1\n",
      "Observation  [ 0.07688431  1.00370496 -0.03047834 -1.31346838] , reward  1.0 , done  False , info  {}\n",
      "[ 0.07688431  1.00370496 -0.03047834 -1.31346838]\n",
      "Action  0\n",
      "Observation  [ 0.09695841  0.80898178 -0.05674771 -1.03047851] , reward  1.0 , done  False , info  {}\n",
      "[ 0.09695841  0.80898178 -0.05674771 -1.03047851]\n",
      "Action  0\n",
      "Observation  [ 0.11313805  0.61465891 -0.07735728 -0.75613828] , reward  1.0 , done  False , info  {}\n",
      "[ 0.11313805  0.61465891 -0.07735728 -0.75613828]\n",
      "Action  1\n",
      "Observation  [ 0.12543122  0.81075706 -0.09248005 -1.0721262 ] , reward  1.0 , done  False , info  {}\n",
      "[ 0.12543122  0.81075706 -0.09248005 -1.0721262 ]\n",
      "Action  1\n",
      "Observation  [ 0.14164636  1.00697176 -0.11392257 -1.39234093] , reward  1.0 , done  False , info  {}\n",
      "[ 0.14164636  1.00697176 -0.11392257 -1.39234093]\n",
      "Action  1\n",
      "Observation  [ 0.1617858   1.20331278 -0.14176939 -1.71836423] , reward  1.0 , done  False , info  {}\n",
      "[ 0.1617858   1.20331278 -0.14176939 -1.71836423]\n",
      "Action  1\n",
      "Observation  [ 0.18585206  1.39974679 -0.17613667 -2.05159989] , reward  1.0 , done  False , info  {}\n",
      "[ 0.18585206  1.39974679 -0.17613667 -2.05159989]\n",
      "Action  1\n",
      "Observation  [ 0.21384699  1.59618199 -0.21716867 -2.39321067] , reward  1.0 , done  True , info  {}\n",
      "Episode finished after 22 timesteps\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        print(\"Action \", action)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        print(\"Observation \", observation, \", reward \", reward, \", done \", done, \", info \" , info)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Frozen Lake scenario\n",
    "We are going to play to the [Frozen Lake](http://gym.openai.com/envs/FrozenLake-v0/) game.\n",
    "\n",
    "The problem is a grid where you should go from the 'start' (S) position to the 'goal position (G) (the pizza!). You can only walk through the 'frozen tiles' (F). Unfortunately, you can fall in a  'hole' (H).\n",
    "![](images/frozenlake-problem.png \"Frozen lake problem\")\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise. The possible actions are going left, right, up or down. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "![](images/frozenlake-world.png \"Frozen lake world\")\n",
    "\n",
    "\n",
    "Here you can see several episodes. A full recording is available at  [Frozen World](http://gym.openai.com/envs/FrozenLake-v0/).\n",
    "\n",
    "![](images/recording.gif \"Example running\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning with the Frozen Lake scenario\n",
    "We are now going to apply Q-Learning for the Frozen Lake scenario. This part of the notebook is taken from [here](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/Q%20Learning%20with%20FrozenLake.ipynb).\n",
    "\n",
    "First we create the environment and a Q-table inizializated with zeros to store the value of each action in a given state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q-Learning hyperparameters\n",
    "total_episodes = 10000        # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration hyperparameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.01             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we implement the Q-Learning algorithm.\n",
    "\n",
    "![](images/qlearning-algo.png \"Q-Learning algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.0\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    episode += 1\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the learnt Q-table for playing the Frozen World game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        env.render()\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## Taxi\n",
    "Analyze the [Taxi problem](http://gym.openai.com/envs/Taxi-v2/) and solve it applying Q-Learning. You can find a solution as the one previously presented  [here](https://www.oreilly.com/learning/introduction-to-reinforcement-learning-and-openai-gym).\n",
    "\n",
    "Analyze the impact of not changing the learning rate (alfa or epsilon, depending on the book) or changing it in a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#Create environment and initialize Qtable\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable)\n",
    "\n",
    "# Q-Learning hyperparameters\n",
    "total_episodes = 10000        # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration hyperparameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.01             # Exponential decay rate for exploration prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 2.9497\n",
      "[[   0.            0.            0.            0.            0.            0.        ]\n",
      " [ 234.72887092  248.10339575  209.96885613  248.11985896  273.30166436\n",
      "   239.2427979 ]\n",
      " [ 217.78740699  286.39809513  217.82308363   89.37390941  304.98799375\n",
      "   255.27124007]\n",
      " ..., \n",
      " [  -3.0130688   249.73820591   -3.04256      -3.01676544  -10.208        -8.        ]\n",
      " [  -5.73753971   -5.45796808   -5.71792547   -5.51271183  -11.51583232\n",
      "   -10.7136    ]\n",
      " [  -1.7216       -1.7216       -1.728       378.99969362   -9.6         -10.3296    ]]\n",
      "****************************************************\n",
      "EPISODE  0\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[42mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[42mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[42mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n"
     ]
    }
   ],
   "source": [
    "#Algorithm\n",
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    episode += 1\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    #epsilon = 1.0\n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)\n",
    "\n",
    "\n",
    "\n",
    "#Game\n",
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        env.render()\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Con la tasa de aprendizaje inicial (la actual) obtiene una puntuación positiva, en uno de los casos es 6,295. Esto es resultado de calcular la qtable en cada episodio jugado (basándose en los resutados anteriores) y realizar las jugadas con la qtable final. Como podemos ver, desde el primer episodio lo consigue resolver satisfactoriamente, por ello los resultados son siempre buenos. \n",
    "Si con esto mismo reduzco el número de pasos maximo a 10 no lo va a conseguir la primera vez y va a se más dificil que luego lo resuelva (hay que tener en cuenta que muchas veces necesita más de 10 pasos para resolverlo). Aquí la putnuación es de -39.\n",
    "\n",
    "Volviendo a 99 pasos, si pongo el epsilon (learning_rate) siempre 1, es decir, siempre intenta explorar y nunca explotar, el score que obtenemos es muy negativo, -386 porque le cuesta mucho más al algoritmo rellenar una qtable que consiga completar el juego. En los dibujos se ve como lo ha conseuido porque los realiza con la qtable que tiene ya al final.\n",
    "\n",
    "Si ponemos que solo explote, tambien obtenemos resultado de -0,23. Los resultados son mejores explotando siempre que explorando.\n",
    "\n",
    "La puntuación es la suma de todas las recompensas recibidas durante la realización del algoritmo en referencia al numero total de episodios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional exercises\n",
    "\n",
    "## Doom\n",
    "Read this [article](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8) and execute the companion [notebook](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/DQN%20Doom). Analyze the results and provide conclusions about DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [Diving deeper into Reinforcement Learning with Q-Learning, Thomas Simonini](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe).\n",
    "* Illustrations by [Thomas Simonini](https://github.com/simoninithomas/Deep_reinforcement_learning_Course) and [Sung Kim](https://www.youtube.com/watch?v=xgoO54qN4lY).\n",
    "* [Frozen Lake solution with TensorFlow](https://analyticsindiamag.com/openai-gym-frozen-lake-beginners-guide-reinforcement-learning/)\n",
    "* [Deep Q-Learning for Doom](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)\n",
    "* [Intro OpenAI Gym with Random Search and the Cart Pole scenario](http://www.pinchofintelligence.com/getting-started-openai-gym/)\n",
    "* [Q-Learning for the Taxi scenario](https://www.oreilly.com/learning/introduction-to-reinforcement-learning-and-openai-gym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is freely licensed under under the [Creative Commons Attribution Share-Alike license](https://creativecommons.org/licenses/by/2.0/).  \n",
    "\n",
    "© 2018 Carlos A. Iglesias, Universidad Politécnica de Madrid."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
